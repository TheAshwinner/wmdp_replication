{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 161691.83 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 456093.79 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 345770.29 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure: DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "Available splits: dict_keys(['test', 'train', 'validation'])\n",
      "\n",
      "Sample from train split:\n",
      "{'text': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Wikitext dataset from Hugging Face\n",
    "# Need to specify a config name as shown in the error message\n",
    "wikitext_dataset = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset structure: {wikitext_dataset}\")\n",
    "print(f\"Available splits: {wikitext_dataset.keys()}\")\n",
    "\n",
    "# Display a sample from the train split\n",
    "print(\"\\nSample from train split:\")\n",
    "print(wikitext_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFaceH4/zephyr-7b-beta model on cuda\n",
      "Model has 7.24 billion parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the Zephyr model and tokenizer\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "print(f\"Loaded {model_name} model on {device}\")\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())/1e9:.2f} billion parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "\n",
    "class JsonlDataset():\n",
    "  def __init__(self, tokenizer, tokenizer_max_length, batch_size, min_len, dataset_name, dataset_folder, device):\n",
    "    self.tokenizer = tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "      tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    self.tokenizer_max_length = tokenizer_max_length\n",
    "    self.batch_size = batch_size\n",
    "    self.min_len = min_len\n",
    "    self.dataset_name = dataset_name\n",
    "    self.dataset_folder = dataset_folder\n",
    "    self.data = []\n",
    "    self.device = device\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.data[idx]\n",
    "    input_ids = self.tokenizer(item[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=self.tokenizer_max_length)\n",
    "    inputs = {key: value.to(self.device) for key, value in input_ids.items()}\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "\n",
    "  def _load_dataset(self):\n",
    "    dataset_path = os.path.join(self.dataset_folder, self.dataset_name)\n",
    "    if not os.path.exists(dataset_path):\n",
    "      raise FileNotFoundError(f\"Dataset file not found at {dataset_path}\")\n",
    "    \n",
    "    data_list = []\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "      for line in f:\n",
    "        data = json.loads(line)\n",
    "        if len(data[\"text\"]) > self.min_len:\n",
    "          data_list.append(data)\n",
    "\n",
    "    self.data = data_list\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JsonlDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_input = wikitext_dataset[\"train\"][5][\"text\"]\n",
    "input_ids = tokenizer(test_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024)[\"input_ids\"]\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
