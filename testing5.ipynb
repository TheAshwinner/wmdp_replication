{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/wmdp_replication/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 189.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import init_empty_weights\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from abc import ABC, abstractmethod\n",
    "from datasets import load_dataset\n",
    "from typing import List, Dict, Optional, Iterator, Union\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BaseDataset(Dataset, ABC):\n",
    "\n",
    "    def __init__(self, \n",
    "                 tokenizer: AutoTokenizer, \n",
    "                 tokenizer_max_length: int = 768,\n",
    "                 batch_size: int = 4,\n",
    "                 min_len: int = 50\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer_max_length = tokenizer_max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.min_len = min_len\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.data = self._load_data()\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def _load_data(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict:\n",
    "        item = self.data[idx]\n",
    "        inputs = self.tokenizer(item, \n",
    "                                return_tensors=\"pt\",\n",
    "                                padding=True,\n",
    "                                truncation=True,\n",
    "                                max_length=self.tokenizer_max_length)\n",
    "        inputs = {key: value.to(self.device) for key, value in inputs.items()}\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "class JSONLDataset(BaseDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading and processing JSONL format data.\n",
    "\n",
    "    Attributes:\n",
    "        dataset_name (str): Name of the dataset from the data folder\n",
    "        dataset_folder (str): Name of the folder where the dataset is located\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dataset_name: str, \n",
    "                 dataset_folder: str = 'data/',\n",
    "                 **kwargs\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the JSONL dataset.\"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_folder = dataset_folder\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def _load_data(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load and parse the JSONL file.\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing parsed data entries.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.dataset_folder, self.dataset_name)\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "        data = []\n",
    "        for line in open(f\"{file_path}\", \"r\"):\n",
    "            if \"bio-forget-corpus\" in self.dataset_name:\n",
    "                raw_text = json.loads(line)['text']\n",
    "            else:\n",
    "                raw_text = line\n",
    "            if len(raw_text) > self.min_len:\n",
    "                data.append(str(raw_text))\n",
    "        return [data[i:i + self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "\n",
    "\n",
    "class WikitextDataset(BaseDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for handling the Wikitext dataset from HuggingFace.\n",
    "\n",
    "    Attributes:\n",
    "        dataset_version (str): Name of the version of wikitext to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dataset_version: str = 'wikitext-2-raw-v1', \n",
    "                 **kwargs\n",
    "    ) -> None:\n",
    "        self.dataset_version = dataset_version\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "    def _load_data(self) -> List[str]:\n",
    "        \"\"\"Load data from Wikitext dataset.\"\"\"\n",
    "\n",
    "        dataset = load_dataset(\"wikitext\", self.dataset_version, split=\"test\")\n",
    "        if dataset is None:\n",
    "            raise DatasetError(\"Failed to load Wikitext dataset\")\n",
    "        data = [item['text'] for item in dataset if len(item['text']) > self.min_len]\n",
    "        return [data[i:i + self.batch_size] for i in range(0, len(data), self.batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a Dataset class that loads in a jsonl file, tokenizes the dataset as expected and returns the input ids and attention mask.\n",
    "import os\n",
    "import json\n",
    "\n",
    "class JsonlDataset():\n",
    "  def __init__(self, tokenizer, tokenizer_max_length, batch_size, min_len, dataset_name, dataset_folder, device):\n",
    "    self.tokenizer = tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "      tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    self.tokenizer_max_length = tokenizer_max_length\n",
    "    self.batch_size = batch_size\n",
    "    self.min_len = min_len\n",
    "    self.dataset_name = dataset_name\n",
    "    self.dataset_folder = dataset_folder\n",
    "    self.data = []\n",
    "    self.device = device\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.data[idx]\n",
    "    input_ids = self.tokenizer(item[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=self.tokenizer_max_length)\n",
    "    inputs = {key: value.to(self.device) for key, value in input_ids.items()}\n",
    "    return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "\n",
    "  def _load_dataset(self):\n",
    "    dataset_path = os.path.join(self.dataset_folder, self.dataset_name)\n",
    "    if not os.path.exists(dataset_path):\n",
    "      raise FileNotFoundError(f\"Dataset file not found at {dataset_path}\")\n",
    "    \n",
    "    data_list = []\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "      for line in f:\n",
    "        data = json.loads(line)\n",
    "        if len(data[\"text\"]) > self.min_len:\n",
    "          data_list.append(data)\n",
    "\n",
    "    self.data = data_list\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "diogo_forget = JSONLDataset(\"cyber-forget-corpus.jsonl\", tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n"
     ]
    }
   ],
   "source": [
    "print(diogo_forget[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ -5.8438,  -5.7812,  -0.2119,  ...,  -4.1250,  -3.6406,  -3.8750],\n",
       "         [ -8.7500,  -8.7500,   5.5625,  ...,  -5.1875,  -6.3438,  -5.8438],\n",
       "         [ -6.4688,  -6.4375,   2.3750,  ...,  -5.8750,  -5.6250,  -4.8438],\n",
       "         ...,\n",
       "         [ -7.8750,  -8.0000,   5.8438,  ...,  -5.1250,  -7.4688,  -4.1250],\n",
       "         [ -6.7188,  -6.3750,  11.1875,  ...,  -6.2812,  -8.0625,  -5.0312],\n",
       "         [ -6.9688,  -5.6562,  10.5625,  ...,  -3.1094,  -5.1875,  -3.9688]],\n",
       "\n",
       "        [[ -5.8438,  -5.7812,  -0.2119,  ...,  -4.1250,  -3.6406,  -3.8750],\n",
       "         [ -6.0000,  -5.4062,  -0.7070,  ...,  -1.1484,   0.0430,   1.7266],\n",
       "         [-10.1875, -10.3125,  -1.7578,  ...,  -9.3750,  -6.3750,  -5.8125],\n",
       "         ...,\n",
       "         [ -7.5938,  -6.5938,   8.0000,  ...,  -5.0000,  -2.0938,  -2.7500],\n",
       "         [ -5.1562,  -4.8750,   9.6250,  ...,  -5.9375,  -5.9375,  -4.1250],\n",
       "         [ -7.3438,  -6.0000,  10.4375,  ...,  -3.5625,  -4.6250,  -3.6094]],\n",
       "\n",
       "        [[ -5.8438,  -5.7812,  -0.2119,  ...,  -4.1250,  -3.6406,  -3.8750],\n",
       "         [ -8.6250,  -9.8125,  -1.6094,  ...,  -6.9062,  -7.1875,  -5.6562],\n",
       "         [ -5.6875,  -5.8438,   1.2891,  ...,  -4.0312,  -4.1250,  -4.1562],\n",
       "         ...,\n",
       "         [ -7.4375,  -7.3438,  10.1875,  ...,  -3.6250,  -6.1562,  -3.4844],\n",
       "         [ -6.0312,  -5.9062,  10.1250,  ...,  -6.3125,  -8.1250,  -5.8750],\n",
       "         [ -7.4375,  -6.6562,   8.3750,  ...,  -3.1406,  -5.8125,  -5.2188]],\n",
       "\n",
       "        [[  1.2734,  11.7500,   3.3438,  ...,  -3.6875,  -1.4375,   1.6250],\n",
       "         [  1.2734,  11.7500,   3.3438,  ...,  -3.6875,  -1.4375,   1.6250],\n",
       "         [  1.2734,  11.7500,   3.3438,  ...,  -3.6875,  -1.4375,   1.6250],\n",
       "         ...,\n",
       "         [ -5.7188,  -4.3125,   3.7656,  ...,  -5.8438,  -5.3125,  -3.5625],\n",
       "         [ -5.2188,  -3.5938,   7.1562,  ...,  -6.8750,  -6.1562,  -4.9062],\n",
       "         [ -3.2031,  -1.8594,   3.6094,  ...,  -1.6016,  -1.8594,  -1.2812]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x72ec287e7aa0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(diogo_forget[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyber_forget = JsonlDataset(\n",
    "      tokenizer=tokenizer, tokenizer_max_length=1024, batch_size=1,\n",
    "      min_len=30, dataset_name=\"cyber-forget-corpus.jsonl\", dataset_folder=\"data/\",\n",
    "      device=device\n",
    "    )\n",
    "cyber_forget._load_dataset()\n",
    "\n",
    "cyber_retain = JsonlDataset(\n",
    "      tokenizer=tokenizer, tokenizer_max_length=1024, batch_size=1,\n",
    "      min_len=30, dataset_name=\"cyber-retain-corpus.jsonl\", dataset_folder=\"data/\",\n",
    "      device=device\n",
    "    )\n",
    "cyber_retain._load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "4385\n"
     ]
    }
   ],
   "source": [
    "print(len(cyber_forget.data))\n",
    "print(len(cyber_retain.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "my_input = cyber_forget[0][\"input_ids\"]\n",
    "print(my_input.shape)\n",
    "\n",
    "# What they do in my fork:\n",
    "inputs = cyber_forget.data[0][\"text\"]\n",
    "\n",
    "tokenized_inputs = tokenizer(inputs, max_length=1024, return_tensors=\"pt\").to(device)\n",
    "print(tokenized_inputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-5.8438, -5.7812, -0.2109,  ..., -4.1250, -3.6406, -3.8750],\n",
       "         [-7.4688, -7.7500, -1.0547,  ..., -6.7500, -3.8438, -4.9375],\n",
       "         [-7.6875, -7.9375,  0.2891,  ..., -6.5938, -6.4375, -7.3125],\n",
       "         ...,\n",
       "         [-8.6875, -7.7500,  6.8125,  ..., -7.1562, -6.6562, -3.8906],\n",
       "         [-4.4375, -4.1875,  2.1094,  ..., -4.3125, -1.1172, -2.7344],\n",
       "         [-9.2500, -8.7500,  9.6875,  ..., -5.9688, -7.8750, -4.8750]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x72ec12b01f70>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(cyber_forget[0][\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
