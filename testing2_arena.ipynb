{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import webbrowser\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import gelu_new, tokenize_and_concatenate\n",
    "from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # This is commented out because in the `wmdp_replication` conda environment,\n",
    "# # Python 3.8 is used but plotly requires Python 3.10 or higher.\n",
    "# from plotly_utils import imshow\n",
    "\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerTrainingArgs:\n",
    "    batch_size = 16\n",
    "    epochs = 20\n",
    "    max_steps_per_epoch = 200\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    wandb_project: str | None = \"wmdp_replication_testing\"\n",
    "    wandb_name: str | None = None\n",
    "\n",
    "\n",
    "args = TransformerTrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "cyber_dataset = load_dataset(\"cais/wmdp-corpora\", \"cyber-retain-corpus\")\n",
    "# cyber_train_test_split = cyber_dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "# cyber_dataset_split = datasets.DatasetDict({\n",
    "#   \"train\": cyber_train_test_split[\"train\"],\n",
    "#   \"test\": cyber_train_test_split[\"test\"]})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # Function to tokenize dataset\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(\n",
    "#         examples[\"text\"],  # Adjust this key based on dataset column name\n",
    "#         padding=\"max_length\",  # Ensures uniform sequence length\n",
    "#         truncation=True,  # Truncates if sequence exceeds max_length\n",
    "#         max_length=1024,  # Adjust max_length based on model capacity\n",
    "#         return_tensors=\"pt\"  # Returns PyTorch tensors\n",
    "#     )\n",
    "\n",
    "# # Tokenize dataset\n",
    "# tokenized_datasets = cyber_dataset_split.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Remove old text column to avoid redundancy\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])  # Ensure column name matches dataset\n",
    "\n",
    "# # Print an example\n",
    "# print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenized_datasets[\"train\"][0][\"input_ids\"])\n",
    "# print(len(tokenized_datasets[\"train\"][0][\"input_ids\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyber_dataset[train]:  Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 4473\n",
      "})\n",
      "cyber_tokenized_dataset:  Dataset({\n",
      "    features: ['tokens'],\n",
      "    num_rows: 79476\n",
      "})\n",
      "cyber_dataset_dict:  DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens'],\n",
      "        num_rows: 63580\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens'],\n",
      "        num_rows: 15896\n",
      "    })\n",
      "})\n",
      "{'tokens': tensor([[50256,   198,    42,  ...,    84,  4496,   198],\n",
      "        [50256,   220,   220,  ...,   281, 15619, 43089],\n",
      "        [50256,    79,  4733,  ...,   284,  4654,   262],\n",
      "        ...,\n",
      "        [50256, 23988,    11,  ..., 12821,    14,  1670],\n",
      "        [50256, 29565,    22,  ...,    60,    77,   320],\n",
      "        [50256,     2,    22,  ...,   397,  8134,    14]])}\n",
      "1\n",
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"cyber_dataset[train]: \", cyber_dataset[\"train\"])\n",
    "cyber_tokenized_dataset = tokenize_and_concatenate(\n",
    "  cyber_dataset[\"train\"],\n",
    "  tokenizer,\n",
    "  max_length=256,\n",
    "  column_name=\"text\",\n",
    "  add_bos_token=True,\n",
    "  num_proc=4,\n",
    ")\n",
    "\n",
    "print(\"cyber_tokenized_dataset: \", cyber_tokenized_dataset)\n",
    "cyber_dataset_dict = cyber_tokenized_dataset.train_test_split(test_size=0.2)\n",
    "print(\"cyber_dataset_dict: \", cyber_dataset_dict)\n",
    "train_cyber_loader = DataLoader(cyber_dataset_dict[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "for i, batch in enumerate(train_cyber_loader):\n",
    "  print(batch)\n",
    "  print(len(batch))\n",
    "  print(batch[\"tokens\"].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(isinstance(cyber_dataset_dict, datasets.DatasetDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Custom collate function to ensure correct shape\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"\n",
    "#     Stacks batch examples along the batch dimension instead of sequence dimension.\n",
    "#     This ensures we get shape [batch_size, sequence_length].\n",
    "#     \"\"\"\n",
    "\n",
    "#     input_ids = t.stack([t.tensor(example[\"input_ids\"]) for example in batch])\n",
    "#     attention_mask = t.stack([t.tensor(example[\"attention_mask\"]) for example in batch])\n",
    "    \n",
    "#     return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "# def testing(dataset):\n",
    "#   train_loader = DataLoader(\n",
    "#       dataset[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn\n",
    "#   )\n",
    "#   # test_loader = DataLoader(\n",
    "#   #     dataset[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    "#   # )\n",
    "#   for i, batch in enumerate(train_loader):\n",
    "#     mybatch = batch[\"input_ids\"]\n",
    "#     print(mybatch)\n",
    "#     print(len(mybatch))\n",
    "\n",
    "#     print(mybatch[0])\n",
    "#     print(len(mybatch[0]))\n",
    "#     return\n",
    "\n",
    "# tokenized_datasets_collated = collate_fn(tokenized_datasets[\"train\"])\n",
    "# print(t.mps.current_allocated_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "embed.W_E torch.Size([50257, 768]) True\n",
      "pos_embed.W_pos torch.Size([1024, 768]) True\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.0.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.0.attn.b_O torch.Size([768]) True\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.0.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.0.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.0.mlp.b_in torch.Size([3072]) True\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.0.mlp.b_out torch.Size([768]) True\n",
      "blocks.1.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.1.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.1.attn.b_O torch.Size([768]) True\n",
      "blocks.1.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.1.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.1.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.1.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.1.mlp.b_in torch.Size([3072]) True\n",
      "blocks.1.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.1.mlp.b_out torch.Size([768]) True\n",
      "blocks.2.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.2.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.2.attn.b_O torch.Size([768]) True\n",
      "blocks.2.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.2.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.2.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.2.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.2.mlp.b_in torch.Size([3072]) True\n",
      "blocks.2.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.2.mlp.b_out torch.Size([768]) True\n",
      "blocks.3.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.3.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.3.attn.b_O torch.Size([768]) True\n",
      "blocks.3.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.3.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.3.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.3.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.3.mlp.b_in torch.Size([3072]) True\n",
      "blocks.3.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.3.mlp.b_out torch.Size([768]) True\n",
      "blocks.4.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.4.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.4.attn.b_O torch.Size([768]) True\n",
      "blocks.4.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.4.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.4.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.4.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.4.mlp.b_in torch.Size([3072]) True\n",
      "blocks.4.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.4.mlp.b_out torch.Size([768]) True\n",
      "blocks.5.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.5.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.5.attn.b_O torch.Size([768]) True\n",
      "blocks.5.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.5.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.5.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.5.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.5.mlp.b_in torch.Size([3072]) True\n",
      "blocks.5.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.5.mlp.b_out torch.Size([768]) True\n",
      "blocks.6.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.6.attn.b_O torch.Size([768]) True\n",
      "blocks.6.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.6.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.6.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.b_out torch.Size([768]) True\n",
      "blocks.7.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.7.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.7.attn.b_O torch.Size([768]) True\n",
      "blocks.7.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.7.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.7.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.7.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.7.mlp.b_in torch.Size([3072]) True\n",
      "blocks.7.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.7.mlp.b_out torch.Size([768]) True\n",
      "blocks.8.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.8.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.8.attn.b_O torch.Size([768]) True\n",
      "blocks.8.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.8.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.8.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.8.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.8.mlp.b_in torch.Size([3072]) True\n",
      "blocks.8.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.8.mlp.b_out torch.Size([768]) True\n",
      "blocks.9.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.9.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.9.attn.b_O torch.Size([768]) True\n",
      "blocks.9.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.9.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.9.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.9.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.9.mlp.b_in torch.Size([3072]) True\n",
      "blocks.9.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.9.mlp.b_out torch.Size([768]) True\n",
      "blocks.10.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.10.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.10.attn.b_O torch.Size([768]) True\n",
      "blocks.10.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.10.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.10.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.10.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.10.mlp.b_in torch.Size([3072]) True\n",
      "blocks.10.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.10.mlp.b_out torch.Size([768]) True\n",
      "blocks.11.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.11.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.11.attn.b_O torch.Size([768]) True\n",
      "blocks.11.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.11.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.11.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.11.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.11.mlp.b_in torch.Size([3072]) True\n",
      "blocks.11.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.11.mlp.b_out torch.Size([768]) True\n",
      "unembed.W_U torch.Size([768, 50257]) True\n",
      "unembed.b_U torch.Size([50257]) True\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.named_parameters())))\n",
    "\n",
    "def print_param_grad(model):\n",
    "  for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)\n",
    "\n",
    "print_param_grad(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freeze_layer(model, layer_index):\n",
    "  for name, param in model.named_parameters():\n",
    "    # The only parameters that are not frozen are the ones `layer_index`\n",
    "    if f\"blocks.{layer_index}\" in name:\n",
    "      continue\n",
    "    param.requires_grad = False\n",
    "  return model\n",
    "\n",
    "freeze_layer(model, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed.W_E torch.Size([50257, 768]) False\n",
      "pos_embed.W_pos torch.Size([1024, 768]) False\n",
      "blocks.0.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.0.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.0.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.0.attn.b_O torch.Size([768]) False\n",
      "blocks.0.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.0.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.0.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.0.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.0.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.0.mlp.b_in torch.Size([3072]) False\n",
      "blocks.0.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.0.mlp.b_out torch.Size([768]) False\n",
      "blocks.1.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.1.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.1.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.1.attn.b_O torch.Size([768]) False\n",
      "blocks.1.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.1.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.1.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.1.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.1.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.1.mlp.b_in torch.Size([3072]) False\n",
      "blocks.1.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.1.mlp.b_out torch.Size([768]) False\n",
      "blocks.2.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.2.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.2.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.2.attn.b_O torch.Size([768]) False\n",
      "blocks.2.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.2.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.2.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.2.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.2.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.2.mlp.b_in torch.Size([3072]) False\n",
      "blocks.2.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.2.mlp.b_out torch.Size([768]) False\n",
      "blocks.3.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.3.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.3.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.3.attn.b_O torch.Size([768]) False\n",
      "blocks.3.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.3.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.3.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.3.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.3.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.3.mlp.b_in torch.Size([3072]) False\n",
      "blocks.3.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.3.mlp.b_out torch.Size([768]) False\n",
      "blocks.4.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.4.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.4.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.4.attn.b_O torch.Size([768]) False\n",
      "blocks.4.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.4.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.4.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.4.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.4.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.4.mlp.b_in torch.Size([3072]) False\n",
      "blocks.4.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.4.mlp.b_out torch.Size([768]) False\n",
      "blocks.5.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.5.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.5.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.5.attn.b_O torch.Size([768]) False\n",
      "blocks.5.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.5.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.5.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.5.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.5.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.5.mlp.b_in torch.Size([3072]) False\n",
      "blocks.5.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.5.mlp.b_out torch.Size([768]) False\n",
      "blocks.6.attn.W_Q torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.W_O torch.Size([12, 64, 768]) True\n",
      "blocks.6.attn.b_Q torch.Size([12, 64]) True\n",
      "blocks.6.attn.b_O torch.Size([768]) True\n",
      "blocks.6.attn.W_K torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.W_V torch.Size([12, 768, 64]) True\n",
      "blocks.6.attn.b_K torch.Size([12, 64]) True\n",
      "blocks.6.attn.b_V torch.Size([12, 64]) True\n",
      "blocks.6.mlp.W_in torch.Size([768, 3072]) True\n",
      "blocks.6.mlp.b_in torch.Size([3072]) True\n",
      "blocks.6.mlp.W_out torch.Size([3072, 768]) True\n",
      "blocks.6.mlp.b_out torch.Size([768]) True\n",
      "blocks.7.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.7.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.7.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.7.attn.b_O torch.Size([768]) False\n",
      "blocks.7.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.7.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.7.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.7.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.7.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.7.mlp.b_in torch.Size([3072]) False\n",
      "blocks.7.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.7.mlp.b_out torch.Size([768]) False\n",
      "blocks.8.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.8.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.8.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.8.attn.b_O torch.Size([768]) False\n",
      "blocks.8.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.8.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.8.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.8.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.8.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.8.mlp.b_in torch.Size([3072]) False\n",
      "blocks.8.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.8.mlp.b_out torch.Size([768]) False\n",
      "blocks.9.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.9.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.9.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.9.attn.b_O torch.Size([768]) False\n",
      "blocks.9.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.9.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.9.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.9.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.9.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.9.mlp.b_in torch.Size([3072]) False\n",
      "blocks.9.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.9.mlp.b_out torch.Size([768]) False\n",
      "blocks.10.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.10.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.10.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.10.attn.b_O torch.Size([768]) False\n",
      "blocks.10.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.10.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.10.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.10.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.10.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.10.mlp.b_in torch.Size([3072]) False\n",
      "blocks.10.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.10.mlp.b_out torch.Size([768]) False\n",
      "blocks.11.attn.W_Q torch.Size([12, 768, 64]) False\n",
      "blocks.11.attn.W_O torch.Size([12, 64, 768]) False\n",
      "blocks.11.attn.b_Q torch.Size([12, 64]) False\n",
      "blocks.11.attn.b_O torch.Size([768]) False\n",
      "blocks.11.attn.W_K torch.Size([12, 768, 64]) False\n",
      "blocks.11.attn.W_V torch.Size([12, 768, 64]) False\n",
      "blocks.11.attn.b_K torch.Size([12, 64]) False\n",
      "blocks.11.attn.b_V torch.Size([12, 64]) False\n",
      "blocks.11.mlp.W_in torch.Size([768, 3072]) False\n",
      "blocks.11.mlp.b_in torch.Size([3072]) False\n",
      "blocks.11.mlp.W_out torch.Size([3072, 768]) False\n",
      "blocks.11.mlp.b_out torch.Size([768]) False\n",
      "unembed.W_U torch.Size([768, 50257]) False\n",
      "unembed.b_U torch.Size([50257]) False\n"
     ]
    }
   ],
   "source": [
    "print_param_grad(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d9585e413e4405b361852aea30f760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m args \u001b[38;5;241m=\u001b[39m TransformerTrainingArgs()\n\u001b[1;32m     91\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TransformerTrainer(args, model, cyber_dataset_dict)\n\u001b[0;32m---> 92\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 76\u001b[0m, in \u001b[0;36mTransformerTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[0;32m---> 76\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     78\u001b[0m         progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 36\u001b[0m, in \u001b[0;36mTransformerTrainer.training_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mCalculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mRemember that `batch` is a dictionary with the single key 'tokens'.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 36\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mget_log_probs(logits, tokens)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:575\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    572\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:181\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    177\u001b[0m resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_mid(resid_pre \u001b[38;5;241m+\u001b[39m attn_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    179\u001b[0m     resid_mid \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    180\u001b[0m )\n\u001b[0;32m--> 181\u001b[0m normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mlp(normalized_resid_mid)\n\u001b[1;32m    183\u001b[0m resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_post(resid_mid \u001b[38;5;241m+\u001b[39m mlp_out)  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/wmdp_replication_v2/lib/python3.11/site-packages/transformer_lens/components/layer_norm_pre.py:52\u001b[0m, in \u001b[0;36mLayerNormPre.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# [batch, pos, length]\u001b[39;00m\n\u001b[1;32m     49\u001b[0m scale: Union[\n\u001b[1;32m     50\u001b[0m     Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos 1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     51\u001b[0m     Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos head_index 1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m---> 52\u001b[0m ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_scale((\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt())\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_normalized(x \u001b[38;5;241m/\u001b[39m scale)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def get_log_probs(\n",
    "    logits: Float[Tensor, \"batch posn d_vocab\"], tokens: Int[Tensor, \"batch posn\"]\n",
    ") -> Float[Tensor, \"batch posn-1\"]:\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    # Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "    log_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    return log_probs_for_tokens\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self, args: TransformerTrainingArgs, model: HookedTransformer, dataset: datasets.DatasetDict):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.optimizer = t.optim.AdamW(self.model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        self.step = 0\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            self.dataset[\"train\"], batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
    "        )\n",
    "        self.test_loader = DataLoader(\n",
    "            self.dataset[\"test\"], batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch: dict[str, Int[Tensor, \"batch seq\"]]) -> Float[Tensor, \"\"]:\n",
    "        \"\"\"\n",
    "        Calculates the loss on the tokens in the batch, performs a gradient update step, and logs the loss.\n",
    "\n",
    "        Remember that `batch` is a dictionary with the single key 'tokens'.\n",
    "        \"\"\"\n",
    "        tokens = batch[\"tokens\"].to(device)\n",
    "        logits = self.model(tokens)\n",
    "        loss = -get_log_probs(logits, tokens).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.step += 1\n",
    "        wandb.log({\"train_loss\": loss}, step=self.step)\n",
    "        return loss\n",
    "\n",
    "    @t.inference_mode()\n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set and return the accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_correct, total_samples = 0, 0\n",
    "\n",
    "        for batch in tqdm(self.test_loader, desc=\"Evaluating\"):\n",
    "            tokens = batch[\"tokens\"].to(device)\n",
    "            logits: Tensor = self.model(tokens)[:, :-1]\n",
    "            predicted_tokens = logits.argmax(dim=-1)\n",
    "            total_correct += (predicted_tokens == tokens[:, 1:]).sum().item()\n",
    "            total_samples += tokens.size(0) * (tokens.size(1) - 1)\n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        wandb.log({\"accuracy\": accuracy}, step=self.step)\n",
    "        return accuracy\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model, for `self.args.epochs` epochs. Also handles wandb initialisation, and early stopping\n",
    "        for each epoch at `self.args.max_steps_per_epoch` steps.\n",
    "        \"\"\"\n",
    "        wandb.init(project=self.args.wandb_project, name=self.args.wandb_name, config=self.args)\n",
    "        accuracy = np.nan\n",
    "\n",
    "        progress_bar = tqdm(total=self.args.max_steps_per_epoch * self.args.epochs)\n",
    "\n",
    "        for epoch in range(self.args.epochs):\n",
    "            for i, batch in enumerate(self.train_loader):\n",
    "                loss = self.training_step(batch)\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(f\"Epoch {epoch+1}, loss: {loss:.3f}, accuracy: {accuracy:.3f}\")\n",
    "                if i >= self.args.max_steps_per_epoch:\n",
    "                    break\n",
    "\n",
    "            accuracy = self.evaluate()\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "\n",
    "# See the full run here: https://api.wandb.ai/links/callum-mcdougall/4xtin05h\n",
    "\n",
    "\n",
    "args = TransformerTrainingArgs()\n",
    "trainer = TransformerTrainer(args, model, cyber_dataset_dict)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wmdp_replication",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
