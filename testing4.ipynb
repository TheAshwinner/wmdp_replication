{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/wmdp_replication/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:03<00:00,  2.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import init_empty_weights\n",
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import JsonlDataset\n",
    "\n",
    "# TODO: I'm unsure about min_len but I can figure this out later maybe.\n",
    "cyber_forget = JsonlDataset(\n",
    "      tokenizer=tokenizer, tokenizer_max_length=1024, batch_size=1,\n",
    "      min_len=30, dataset_name=\"cyber-forget-corpus.jsonl\", dataset_folder=\"data/\", device=device\n",
    "    )\n",
    "\n",
    "cyber_forget._load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[-5.8985, -5.8302, -0.2193,  ..., -4.2019, -3.7009, -3.9271],\n",
      "         [-7.4478, -7.6959, -1.0112,  ..., -6.6283, -3.7932, -4.8868],\n",
      "         [-7.7416, -7.9730,  0.3227,  ..., -6.5928, -6.4520, -7.3018],\n",
      "         ...,\n",
      "         [-8.7295, -7.7559,  6.7179,  ..., -7.1722, -6.6395, -3.9135],\n",
      "         [-4.4445, -4.2029,  1.9865,  ..., -4.3597, -1.1242, -2.7143],\n",
      "         [-9.2293, -8.7453,  9.6163,  ..., -6.0335, -7.8609, -4.8814]]],\n",
      "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x760f6a6d1fd0>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(cyber_forget.data[0][\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "output = model(**inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    1, 15549,   352,  ..., 28781, 15834,  6291], device='cuda:0'), 'attention_mask': tensor([1, 1, 1,  ..., 1, 1, 1], device='cuda:0')}\n",
      "torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "print(cyber_forget[0])\n",
    "print(cyber_forget[0][\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-5.8985, -5.8302, -0.2193,  ..., -4.2019, -3.7009, -3.9271],\n",
       "         [-7.4478, -7.6959, -1.0112,  ..., -6.6283, -3.7932, -4.8868],\n",
       "         [-7.7416, -7.9730,  0.3227,  ..., -6.5928, -6.4520, -7.3018],\n",
       "         ...,\n",
       "         [-8.7295, -7.7559,  6.7179,  ..., -7.1722, -6.6395, -3.9135],\n",
       "         [-4.4445, -4.2029,  1.9865,  ..., -4.3597, -1.1242, -2.7143],\n",
       "         [-9.2293, -8.7453,  9.6163,  ..., -6.0335, -7.8609, -4.8814]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x760f6a6c7bf0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(cyber_forget.data[0][\"text\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
    "model(**tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Model():\n",
    "  def __init__(self, model, tokenizer, device, seed = 42):\n",
    "    self.model = model.to(device)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.device = device\n",
    "    self.seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    self.activations = {}\n",
    "\n",
    "  def hook_fn(self, module, input, output):\n",
    "    self.activations[\"transformer_block_output\"] = output[0].detach()\n",
    "  \n",
    "  def forward(self, inputs, layer_idx: int):\n",
    "    if layer_idx >= len(self.model.model.layers):\n",
    "      raise ValueError(f\"Layer index {layer_idx} is out of bounds for the model. The model has {len(self.model.transformer.h)} layers.\")\n",
    "    try:\n",
    "      hook = self.model.model.layers[layer_idx].register_forward_hook(self.hook_fn)\n",
    "      tokenized_inputs = self.tokenizer(inputs, return_tensors=\"pt\").to(self.device)\n",
    "      # print(inputs[\"input_ids\"].shape)\n",
    "      with torch.no_grad():\n",
    "        _ = self.model(**tokenized_inputs)\n",
    "    finally:\n",
    "      hook.remove()\n",
    "    return self.activations[\"transformer_block_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 768])\n",
      "tensor([[-0.0369,  0.0372, -0.0026,  ..., -0.0416, -0.0009,  0.0819],\n",
      "        [-0.0140,  0.0459, -0.0427,  ..., -0.0310,  0.0112, -0.0398],\n",
      "        [-0.0125,  0.0393,  0.0028,  ..., -0.0679,  0.0845,  0.0271],\n",
      "        ...,\n",
      "        [ 0.0236, -0.0232,  0.0117,  ...,  0.0443, -0.0140,  0.0333],\n",
      "        [ 0.0090,  0.0066,  0.0128,  ..., -0.0061, -0.0057, -0.0025],\n",
      "        [ 0.0112,  0.0058,  0.0366,  ..., -0.0680, -0.0136, -0.0345]])\n"
     ]
    }
   ],
   "source": [
    "test_u = torch.randn(1024, 768)\n",
    "test_u = test_u / torch.linalg.norm(test_u, dim=-1, keepdim=True)\n",
    "print(test_u.shape)\n",
    "print(test_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from src.dataset import JsonlDataset\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "class RMU:\n",
    "  def __init__(self, model, tokenizer, datasets, device, alpha, lr, c, hidden_dimension_size, tokenizer_max_length, min_len, layer_idx, seed = 42):\n",
    "    self.unlearned_model = Model(model, tokenizer, device, seed)\n",
    "    self.frozen_model = copy.deepcopy(self.unlearned_model)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.datasets = datasets\n",
    "    self.device = device\n",
    "    self.alpha = alpha\n",
    "    self.lr = lr\n",
    "    self.c = c\n",
    "    self.tokenizer_max_length = tokenizer_max_length\n",
    "    self.min_len = min_len\n",
    "    self.seed = seed\n",
    "    self.hidden_dimension_size = hidden_dimension_size\n",
    "    self.layer_idx = layer_idx\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    self.freeze_layers_in_unlearned_model([self.layer_idx-2, self.layer_idx-1, self.layer_idx])\n",
    "\n",
    "    # Initialize random unit vector u\n",
    "    some_big_number = 15000\n",
    "    u = torch.randn(some_big_number, self.hidden_dimension_size).to(self.device)\n",
    "    u = u / torch.linalg.norm(u, dim=-1, keepdim=True).to(self.device)\n",
    "    self.u = u\n",
    "\n",
    "  def freeze_layers_in_unlearned_model(self, unfreeze_layers: list[int]):\n",
    "    # Validation\n",
    "    for layer in unfreeze_layers:\n",
    "      assert layer >= 0 and layer < len(self.unlearned_model.model.model.layers)\n",
    "\n",
    "    # Freeze all layers first\n",
    "    for param in self.unlearned_model.model.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the specified layers\n",
    "    for layer in unfreeze_layers:\n",
    "      for param in self.unlearned_model.model.model.layers[layer].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "\n",
    "  def retain_loss(self, act_retain, act_forget):\n",
    "    l2_squared = torch.sum((act_retain - act_forget) ** 2, dim=-1)\n",
    "    final = torch.mean(l2_squared)\n",
    "    return final\n",
    "\n",
    "  def forget_loss(self, act_updated):\n",
    "    print(act_updated.shape)\n",
    "    print(self.u.shape)\n",
    "    print(self.u[:len(act_updated[0]), :].shape)\n",
    "    l2_squared = torch.sum((act_updated - self.c * self.u[:len(act_updated[0]), :]) ** 2, dim=-1)\n",
    "    final = torch.mean(l2_squared)\n",
    "    return final\n",
    "\n",
    "  def rmu_step(self, d_forget, d_retain, layer_idx):\n",
    "    print(\"Beginning RMU step...\")\n",
    "    cyber_forget = JsonlDataset(\n",
    "      tokenizer=self.tokenizer, tokenizer_max_length=self.tokenizer_max_length, batch_size=1,\n",
    "      min_len=self.min_len, dataset_name=\"cyber-forget-corpus.jsonl\", dataset_folder=\"data/\", device=self.device\n",
    "    )\n",
    "    cyber_forget._load_dataset()\n",
    "    cyber_retain = JsonlDataset(\n",
    "      tokenizer=self.tokenizer, tokenizer_max_length=self.tokenizer_max_length, batch_size=1,\n",
    "      min_len=self.min_len, dataset_name=\"cyber-retain-corpus.jsonl\", dataset_folder=\"data/\", device=self.device\n",
    "      )\n",
    "    cyber_retain._load_dataset()\n",
    "\n",
    "    # Retain loss\n",
    "    for i in tqdm.tqdm(range(len(cyber_retain.data))):\n",
    "      print(len(cyber_retain.data[i][\"text\"]))\n",
    "      act_updated = self.unlearned_model.forward(cyber_retain.data[i][\"text\"], layer_idx)\n",
    "      with torch.no_grad():\n",
    "        act_frozen = self.frozen_model.forward(cyber_retain.data[i][\"text\"], layer_idx)\n",
    "      retain_loss = self.retain_loss(act_updated, act_frozen)\n",
    "      print(retain_loss)\n",
    "      break\n",
    "\n",
    "    # Forget loss\n",
    "    for i in tqdm.tqdm(range(len(cyber_forget.data))):\n",
    "      act_updated = self.unlearned_model.forward(cyber_forget.data[i][\"text\"], layer_idx)\n",
    "      forget_loss = self.forget_loss(act_updated)\n",
    "      print(forget_loss)\n",
    "      break\n",
    "\n",
    "    full_loss = forget_loss + self.alpha * retain_loss\n",
    "    optimizer = torch.optim.AdamW(self.unlearned_model.model.parameters(), lr=self.lr)\n",
    "    optimizer.zero_grad()\n",
    "    full_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"Finished RMU step...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 231.06 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 25.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m my_rmu = \u001b[43mRMU\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m my_rmu.rmu_step(d_forget=\u001b[38;5;28;01mNone\u001b[39;00m, d_retain=\u001b[38;5;28;01mNone\u001b[39;00m, layer_idx=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mRMU.__init__\u001b[39m\u001b[34m(self, model, tokenizer, datasets, device, alpha, lr, c, hidden_dimension_size, tokenizer_max_length, min_len, layer_idx, seed)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, tokenizer, datasets, device, alpha, lr, c, hidden_dimension_size, tokenizer_max_length, min_len, layer_idx, seed = \u001b[32m42\u001b[39m):\n\u001b[32m     10\u001b[39m   \u001b[38;5;28mself\u001b[39m.unlearned_model = Model(model, tokenizer, device, seed)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m   \u001b[38;5;28mself\u001b[39m.frozen_model = \u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munlearned_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m   \u001b[38;5;28mself\u001b[39m.tokenizer = tokenizer\n\u001b[32m     13\u001b[39m   \u001b[38;5;28mself\u001b[39m.datasets = datasets\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:162\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    160\u001b[39m                 y = x\n\u001b[32m    161\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:259\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    261\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:162\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    160\u001b[39m                 y = x\n\u001b[32m    161\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:259\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    261\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:162\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    160\u001b[39m                 y = x\n\u001b[32m    161\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:259\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    261\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:136\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    134\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:221\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    219\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/copy.py:143\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    141\u001b[39m copier = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__deepcopy__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     reductor = dispatch_table.get(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/wmdp_replication/lib/python3.12/site-packages/torch/nn/parameter.py:68\u001b[39m, in \u001b[36mParameter.__deepcopy__\u001b[39m\u001b[34m(self, memo)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     67\u001b[39m     result = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.requires_grad\n\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] = result\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 500.00 MiB. GPU 0 has a total capacity of 79.19 GiB of which 231.06 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 78.35 GiB is allocated by PyTorch, and 25.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "my_rmu = RMU(model, tokenizer, [], device, 0.01, 0.001, 1, 4096, 1024, 30, 42)\n",
    "my_rmu.rmu_step(d_forget=None, d_retain=None, layer_idx=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Model(model, tokenizer, device, 42)\n",
    "output = my_model.forward(cyber_forget.data[0][\"text\"], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024])\n",
      "tensor([[[-0.1184, -0.2648, -0.1554,  ..., -0.0136,  0.0858, -0.0389],\n",
      "         [ 0.0420,  0.0390,  0.0470,  ...,  0.0355, -0.0068, -0.0307],\n",
      "         [-0.0149, -0.0296, -0.0170,  ...,  0.0219,  0.0518,  0.0417],\n",
      "         ...,\n",
      "         [ 0.0162, -0.0176,  0.0176,  ...,  0.0365, -0.0607, -0.0196],\n",
      "         [-0.0113, -0.0102,  0.0417,  ...,  0.0305,  0.0347,  0.0183],\n",
      "         [-0.0059,  0.0050, -0.0029,  ...,  0.0445, -0.0131, -0.0024]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 13157, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(cyber_forget[0][\"input_ids\"].shape)\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
